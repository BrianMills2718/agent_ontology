#!/usr/bin/env python3
"""
Claude Code — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Anthropic's official CLI agent — interactive coding assistant with persistent memory, tool use, sub-agent spawning, and human-in-the-loop permission gates
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=8192):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=8192, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "UserInput": {
        "description": """What the user types or triggers""",
        "fields": [{"name": "text", "type": "string"}, {"name": "type", "type": "enum[message, file_event, slash_command]"}, {"name": "files", "type": "list<string>"}],
    },
    "RawInput": {
        "description": """Normalized input ready for context assembly""",
        "fields": [{"name": "content", "type": "string"}, {"name": "role", "type": "enum[user, system]"}],
    },
    "ApiPayload": {
        "description": """The full request sent to Claude API on every call. ~9-11k tokens fixed overhead before any conversation history.""",
        "fields": [{"name": "system", "type": "string"}, {"name": "tools", "type": "list<ToolDefinition>"}, {"name": "messages", "type": "list<Message>"}, {"name": "model", "type": "string"}, {"name": "max_tokens", "type": "integer"}],
    },
    "ApiResponse": {
        "description": """What comes back from the Claude API""",
        "fields": [{"name": "content", "type": "list<ContentBlock>"}, {"name": "stop_reason", "type": "enum[end_turn, tool_use, max_tokens]"}, {"name": "usage", "type": "TokenUsage"}],
    },
    "ContentBlock": {
        "description": """A single block in the API response""",
        "fields": [{"name": "type", "type": "enum[text, tool_use, thinking]"}, {"name": "text", "type": "string"}, {"name": "tool_name", "type": "string"}, {"name": "tool_input", "type": "object"}, {"name": "id", "type": "string"}],
    },
    "ParsedResponse": {
        "description": """API response split into text and tool calls""",
        "fields": [{"name": "text_blocks", "type": "list<string>"}, {"name": "tool_calls", "type": "list<ToolCall>"}],
    },
    "ToolCall": {
        "description": """A single tool invocation""",
        "fields": [{"name": "id", "type": "string"}, {"name": "tool_name", "type": "string"}, {"name": "input", "type": "object"}],
    },
    "ToolCalls": {
        "description": """Set of tool invocations to execute (possibly in parallel)""",
        "fields": [{"name": "calls", "type": "list<ToolCall>"}],
    },
    "ToolResults": {
        "description": """Results from tool execution, possibly with hook injections""",
        "fields": [{"name": "results", "type": "list<ToolResult>"}],
    },
    "ToolResult": {
        "description": """A single tool result""",
        "fields": [{"name": "tool_call_id", "type": "string"}, {"name": "content", "type": "string"}, {"name": "system_reminders", "type": "list<string>"}],
    },
    "ObservedResults": {
        "description": """Tool results after hook processing""",
        "fields": [{"name": "results", "type": "list<ToolResult>"}, {"name": "hook_injections", "type": "list<SystemReminder>"}],
    },
    "SystemReminder": {
        "description": """Context injected by hooks into tool results""",
        "fields": [{"name": "source", "type": "enum[governance, malware_check, edit_constraint, post_edit_quiz, file_modification, task_nudge]"}, {"name": "content", "type": "string"}, {"name": "show_user", "type": "boolean"}],
    },
    "ConversationHistory": {
        "description": """The full conversation state""",
        "fields": [{"name": "messages", "type": "list<Message>"}, {"name": "token_count", "type": "integer"}],
    },
    "Message": {
        "description": """A single message in the conversation""",
        "fields": [{"name": "role", "type": "enum[user, assistant, tool]"}, {"name": "content", "type": "string | list<ContentBlock>"}, {"name": "tool_call_id", "type": "string"}],
    },
    "CompactedHistory": {
        "description": """Conversation after context compression""",
        "fields": [{"name": "summary", "type": "string"}, {"name": "recent_messages", "type": "list<Message>"}, {"name": "transcript_path", "type": "string"}],
    },
    "MemoryEntry": {
        "description": """Content in MEMORY.md""",
        "fields": [{"name": "content", "type": "string"}],
    },
    "TranscriptEntry": {
        "description": """A line in the session .jsonl log""",
        "fields": [{"name": "timestamp", "type": "datetime"}, {"name": "type", "type": "enum[user, assistant, tool_call, tool_result, system]"}, {"name": "content", "type": "object"}],
    },
    "GitStatus": {
        "description": """Git state captured at session start (stale)""",
        "fields": [{"name": "branch", "type": "string"}, {"name": "main_branch", "type": "string"}, {"name": "status", "type": "string"}, {"name": "recent_commits", "type": "list<string>"}],
    },
    "TokenUsage": {
        "description": """Token counts from API response""",
        "fields": [{"name": "input_tokens", "type": "integer"}, {"name": "output_tokens", "type": "integer"}],
    },
    "BashInput": {
        "description": """""",
        "fields": [{"name": "command", "type": "string"}, {"name": "description", "type": "string"}, {"name": "timeout", "type": "integer"}],
    },
    "BashOutput": {
        "description": """""",
        "fields": [{"name": "stdout", "type": "string"}, {"name": "stderr", "type": "string"}, {"name": "exit_code", "type": "integer"}],
    },
    "ReadInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "offset", "type": "integer"}, {"name": "limit", "type": "integer"}],
    },
    "FileContent": {
        "description": """""",
        "fields": [{"name": "content", "type": "string"}, {"name": "line_count", "type": "integer"}],
    },
    "WriteInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "content", "type": "string"}],
    },
    "WriteResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "path", "type": "string"}],
    },
    "EditInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "old_string", "type": "string"}, {"name": "new_string", "type": "string"}, {"name": "replace_all", "type": "boolean"}],
    },
    "EditResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "replacements", "type": "integer"}],
    },
    "GlobInput": {
        "description": """""",
        "fields": [{"name": "pattern", "type": "string"}, {"name": "path", "type": "string"}],
    },
    "FileList": {
        "description": """""",
        "fields": [{"name": "files", "type": "list<string>"}],
    },
    "GrepInput": {
        "description": """""",
        "fields": [{"name": "pattern", "type": "string"}, {"name": "path", "type": "string"}, {"name": "output_mode", "type": "enum[content, files_with_matches, count]"}],
    },
    "SearchResults": {
        "description": """""",
        "fields": [{"name": "matches", "type": "list<SearchMatch>"}],
    },
    "SearchMatch": {
        "description": """""",
        "fields": [{"name": "file", "type": "string"}, {"name": "line", "type": "integer"}, {"name": "content", "type": "string"}],
    },
    "WebFetchInput": {
        "description": """""",
        "fields": [{"name": "url", "type": "string"}, {"name": "prompt", "type": "string"}],
    },
    "WebSearchInput": {
        "description": """""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "WebContent": {
        "description": """""",
        "fields": [{"name": "content", "type": "string"}],
    },
    "TaskInput": {
        "description": """Input to sub-agent spawning""",
        "fields": [{"name": "prompt", "type": "string"}, {"name": "subagent_type", "type": "enum[Bash, general-purpose, Explore, Plan, claude-code-guide]"}, {"name": "model", "type": "string"}, {"name": "max_turns", "type": "integer"}, {"name": "run_in_background", "type": "boolean"}],
    },
    "TaskOutput": {
        "description": """""",
        "fields": [{"name": "result", "type": "string"}, {"name": "agent_id", "type": "string"}],
    },
    "NotebookEditInput": {
        "description": """""",
        "fields": [{"name": "notebook_path", "type": "string"}, {"name": "new_source", "type": "string"}, {"name": "cell_type", "type": "enum[code, markdown]"}, {"name": "edit_mode", "type": "enum[replace, insert, delete]"}],
    },
    "NotebookEditResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}],
    },
    "ToolDefinition": {
        "description": """JSON Schema definition of a tool (sent with every API call)""",
        "fields": [{"name": "name", "type": "string"}, {"name": "description", "type": "string"}, {"name": "parameters", "type": "object"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Claude Code"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    agent_id: str
    approaching: Any
    branch: str
    calls: list
    capacity: Any
    cell_type: str
    command: str
    content: str
    conversation_history: Any
    description: str
    edit_mode: str
    episodic_memory: dict
    exit_code: int
    file: str
    file_path: str
    files: list
    git_snapshot: dict
    hook_injections: list
    id: str
    input: Any
    input_tokens: int
    limit: int
    line: int
    line_count: int
    llm_call_result: Any
    main_branch: str
    matches: list
    max_tokens: int
    max_turns: int
    messages: list
    model: str
    name: str
    new_source: str
    new_string: str
    notebook_path: str
    offset: int
    old_string: str
    output_mode: str
    output_tokens: int
    parameters: Any
    parsed_response: Any
    path: str
    pattern: str
    permission_config: Any
    procedural_memory: dict
    project_instructions: dict
    prompt: str
    query: str
    recent_commits: list
    recent_messages: list
    replace_all: bool
    replacements: int
    result: str
    results: list
    role: str
    rules: Any
    run_in_background: bool
    semantic_memory: dict
    show_user: bool
    source: str
    spawn_sub_agent_result: Any
    status: str
    stderr: str
    stdout: str
    stop_reason: str
    subagent_type: str
    success: bool
    summary: str
    system: str
    system_reminders: list
    text: str
    text_blocks: list
    timeout: int
    timestamp: Any
    token_count: int
    tool_call: Any
    tool_call_id: str
    tool_calls: list
    tool_input: Any
    tool_name: str
    tools: list
    transcript_path: str
    type: str
    url: str
    usage: Any
    working_memory: dict


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_working_memory:
    """Working Memory (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_episodic_memory:
    """Episodic Memory (file)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_semantic_memory:
    """Semantic Memory (file)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_procedural_memory:
    """Procedural Memory (Skills) (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_project_instructions:
    """Project Instructions (file)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_git_snapshot:
    """Git Status Snapshot (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_working_memory = Store_working_memory()
_store_episodic_memory = Store_episodic_memory()
_store_semantic_memory = Store_semantic_memory()
_store_procedural_memory = Store_procedural_memory()
_store_project_instructions = Store_project_instructions()
_store_git_snapshot = Store_git_snapshot()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_claude(user_message, output_schema=None):
    """Claude (LLM)"""
    system = """You are Claude Code, Anthropic's official CLI for Claude..."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="claude-opus-4-6",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=16384,
    )
    trace_call("Claude (LLM)", "claude-opus-4-6", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_sub_agent(user_message, output_schema=None):
    """Sub-Agent"""
    system = """Inherited from parent context + task-specific prompt"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="claude-opus-4-6",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Sub-Agent", "claude-opus-4-6", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Tool Implementations
# ═══════════════════════════════════════════════════════════

def tool_bash(input_text):
    """Bash: """
    import subprocess
    try:
        result = subprocess.run(
            input_text, shell=True, capture_output=True, text=True, timeout=30
        )
        output = result.stdout
        if result.returncode != 0:
            output += f"\nSTDERR: {result.stderr}" if result.stderr else ""
            output += f"\nExit code: {result.returncode}"
        return output or "(no output)"
    except subprocess.TimeoutExpired:
        return "Command timed out (30s)"
    except Exception as e:
        return f"Shell error: {e}"


def tool_read(input_text):
    """Read: """
    return f"[tool Read not implemented for input: {input_text}]"


def tool_write(input_text):
    """Write: """
    return f"[tool Write not implemented for input: {input_text}]"


def tool_edit(input_text):
    """Edit: """
    return f"[tool Edit not implemented for input: {input_text}]"


def tool_glob(input_text):
    """Glob: """
    return f"[tool Glob not implemented for input: {input_text}]"


def tool_grep(input_text):
    """Grep: """
    return f"[tool Grep not implemented for input: {input_text}]"


def tool_web_fetch(input_text):
    """WebFetch: """
    return f"[tool WebFetch not implemented for input: {input_text}]"


def tool_web_search(input_text):
    """WebSearch: """
    import requests
    import re as _re
    _headers = {"User-Agent": "AgentOntology/1.0 (agent research project)"} 
    try:
        resp = requests.get("https://en.wikipedia.org/w/api.php", params={
            "action": "query", "list": "search", "srsearch": input_text,
            "format": "json", "srlimit": 5
        }, headers=_headers, timeout=10)
        data = resp.json()
        results = data.get("query", {}).get("search", [])
        if not results:
            return f"No results found for: {input_text}"
        lines = []
        for r in results:
            snippet = _re.sub(r"<[^>]+>", "", r.get("snippet", ""))
            lines.append(f"{r['title']}: {snippet}")
        return "\n".join(lines)
    except Exception as e:
        return f"Search error: {e}"


def tool_task(input_text):
    """Task (Sub-Agent Spawn): """
    return f"[tool Task (Sub-Agent Spawn) not implemented for input: {input_text}]"


def tool_notebook_edit(input_text):
    """NotebookEdit: """
    return f"[tool NotebookEdit not implemented for input: {input_text}]"


def tool_mcp_tools(input_text):
    """MCP Tools: """
    """Dispatch to MCP server tools."""
    import subprocess
    import json as _json

    # Parse input: expect JSON with {server, tool, args} or plain text
    try:
        req = _json.loads(input_text) if input_text.strip().startswith("{") else {"tool": input_text}
    except _json.JSONDecodeError:
        req = {"tool": input_text}

    server = req.get("server", "puppeteer")
    tool_name = req.get("tool", "")
    tool_args = req.get("args", {})

    # MCP JSON-RPC call via stdio transport
    rpc_request = _json.dumps({
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/call",
        "params": {"name": tool_name, "arguments": tool_args}
    })

    mcp_cmd = os.environ.get(f"MCP_SERVER_{server.upper()}", f"npx -y @modelcontextprotocol/server-{server}")
    try:
        result = subprocess.run(
            mcp_cmd.split(),
            input=rpc_request,
            capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0 and result.stdout.strip():
            resp = _json.loads(result.stdout)
            return _json.dumps(resp.get("result", resp), indent=2)
        return f"MCP call to {server}/{tool_name} returned: {result.stderr or result.stdout}"
    except FileNotFoundError:
        return f"MCP server '{server}' not found. Set MCP_SERVER_{server.upper()} env var or install the server."
    except subprocess.TimeoutExpired:
        return f"MCP call to {server}/{tool_name} timed out (30s)"
    except Exception as e:
        return f"MCP error: {e}"



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_idle(state: AgentState) -> dict:
    """
    IDLE
    Waiting for user input. Session is quiescent.
    """
    print(f"  → IDLE")
    updates = {}

    return updates


def node_receive(state: AgentState) -> dict:
    """
    RECEIVE
    User message, file event, or slash command arrives
    """
    print(f"  → RECEIVE")
    updates = {}

    return updates


def node_build_context(state: AgentState) -> dict:
    """
    BUILD CONTEXT
    Assemble the full API payload: system prompt (~9-11k tokens fixed overhead) + MEMORY.md + CLAUDE.md chain + tool definitions + git snapshot + full conversation history
    """
    print(f"  → BUILD CONTEXT")
    updates = {}

    # Read: Load MEMORY.md (first 200 lines)
    semantic_memory_data = _store_semantic_memory.read()
    updates["semantic_memory"] = semantic_memory_data

    # Read: Load CLAUDE.md chain
    project_instructions_data = _store_project_instructions.read()
    updates["project_instructions"] = project_instructions_data

    # Read: Load git status (stale)
    git_snapshot_data = _store_git_snapshot.read()
    updates["git_snapshot"] = git_snapshot_data

    # Read: Load full conversation history
    working_memory_data = _store_working_memory.read()
    updates["working_memory"] = working_memory_data

    # Read: Load skill definitions
    procedural_memory_data = _store_procedural_memory.read()
    updates["procedural_memory"] = procedural_memory_data

    return updates


def node_llm_call(state: AgentState) -> dict:
    """
    LLM CALL
    Send full context to Claude API. This is the inner loop target — returns here after every tool execution.
    """
    print(f"  → LLM CALL")
    updates = {}

    # Invoke: API call
    _cur = dict(state)
    _cur.update(updates)
    claude_input = build_input(_cur, "ApiPayload")
    claude_msg = json.dumps(claude_input, default=str)
    claude_raw = invoke_claude(claude_msg, output_schema="ApiResponse")
    claude_result = parse_response(claude_raw, "ApiResponse")
    updates["_schema_violations"] = len(validate_output(claude_result, "ApiResponse"))
    updates.update(claude_result)
    updates["api_response"] = claude_result
    updates["llm_call_result"] = claude_result
    print(f"    ← Claude (LLM): {claude_result}")

    return updates


def node_parse_response(state: AgentState) -> dict:
    """
    PARSE RESPONSE
    Extract text blocks and tool_use blocks from API response
    """
    print(f"  → PARSE RESPONSE")
    updates = {}

    return updates


def node_emit_text(state: AgentState) -> dict:
    """
    EMIT TEXT
    Send text response to user terminal
    """
    print(f"  → EMIT TEXT")
    updates = {}

    return updates


def node_prompt_user(state: AgentState) -> dict:
    """
    PERMISSION PROMPT
    """
    print(f"  → PERMISSION PROMPT")
    updates = {}

    # Checkpoint (HITL)
    _canned = state.get("_canned_responses", [])
    if _canned:
        response = _canned[0]
        updates["_canned_responses"] = _canned[1:]
    else:
        response = input("Allow {tool_name} with {args}? [y/n] [approve/deny]: ").strip().lower()
    updates["checkpoint_response"] = response
    return updates


def node_execute(state: AgentState) -> dict:
    """
    EXECUTE
    Run tool call(s). May execute multiple tools in parallel. The Task tool triggers sub-agent spawning (recursive composition).
    """
    print(f"  → EXECUTE")
    updates = {}

    # Tool dispatch
    _cur = dict(state)
    _cur.update(updates)
    _tool_name = _cur.get("tool_name", "").lower().strip()
    _tool_input = str(_cur.get("tool_input", _cur.get("action", "")))
    if _tool_name == "bash":
        _tool_result = tool_bash(_tool_input)
    elif _tool_name == "read":
        _tool_result = tool_read(_tool_input)
    elif _tool_name == "write":
        _tool_result = tool_write(_tool_input)
    elif _tool_name == "edit":
        _tool_result = tool_edit(_tool_input)
    elif _tool_name == "glob":
        _tool_result = tool_glob(_tool_input)
    elif _tool_name == "grep":
        _tool_result = tool_grep(_tool_input)
    elif _tool_name == "webfetch":
        _tool_result = tool_web_fetch(_tool_input)
    elif _tool_name == "websearch":
        _tool_result = tool_web_search(_tool_input)
    elif _tool_name == "task (sub-agent spawn)":
        _tool_result = tool_task(_tool_input)
    elif _tool_name == "notebookedit":
        _tool_result = tool_notebook_edit(_tool_input)
    elif _tool_name == "mcp tools":
        _tool_result = tool_mcp_tools(_tool_input)
    else:
        _tool_result = f"Unknown tool: {_tool_name}. Available: bash, read, write, edit, glob, grep, webfetch, websearch, task (sub-agent spawn), notebookedit, mcp tools"
    updates["observation"] = _tool_result
    print(f"    Observation: {_tool_result[:200]}")

    return updates


def node_observe(state: AgentState) -> dict:
    """
    OBSERVE
    Collect tool results plus any hook-injected system-reminders (governance context, edit constraints, quiz questions, malware warnings). Agent sees hook OUTPUT but not hook scripts.
    """
    print(f"  → OBSERVE")
    updates = {}

    return updates


def node_append(state: AgentState) -> dict:
    """
    APPEND
    Add assistant message + tool results to conversation history
    """
    print(f"  → APPEND")
    updates = {}

    # Write: Append turn to history
    _store_working_memory.write(dict(state))

    return updates


def node_compact(state: AgentState) -> dict:
    """
    COMPACT
    Compress older messages into a ~2000 word summary. Preserves: primary request, key concepts, files discussed, errors, all user messages (summarized), pending tasks, current work state.
    """
    print(f"  → COMPACT")
    updates = {}

    # Read: Read full history
    working_memory_data = _store_working_memory.read()
    updates["working_memory"] = working_memory_data

    # Write: Replace with compacted summary
    _store_working_memory.write(dict(state))

    return updates


def node_persist(state: AgentState) -> dict:
    """
    PERSIST
    Write turn to session transcript (.jsonl)
    """
    print(f"  → PERSIST")
    updates = {}

    # Write: Write to session transcript
    _store_episodic_memory.write(dict(state))

    return updates


def node_spawn_sub_agent(state: AgentState) -> dict:
    """
    Spawn Sub-Agent
    """
    print(f"  → Spawn Sub-Agent")
    updates = {}

    # Spawn: template=self
    print("    [SPAWN] Would create sub-agent from template: self")
    return updates


def node_hook_governance(state: AgentState) -> dict:
    """
    Governance Hooks
    """
    print(f"  → Governance Hooks")
    updates = {}

    # Policy: cross-cutting concern
    return updates


def node_hook_malware(state: AgentState) -> dict:
    """
    Malware Check
    """
    print(f"  → Malware Check")
    updates = {}

    # Policy: cross-cutting concern
    return updates


def node_permission_policy(state: AgentState) -> dict:
    """
    Permission System
    """
    print(f"  → Permission System")
    updates = {}

    # Policy: cross-cutting concern
    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_tools(state: AgentState) -> str:
    """Gate: Tool calls? — parsed_response.tool_calls.length > 0"""
    if len(state.get("parsed_response", {}).get("tool_calls") or []):
        print(f"    → has tool calls")
        return "check_permission"
    else:
        print(f"    → no tool calls")
        return "emit_text"


def route_check_permission(state: AgentState) -> str:
    """Gate: Permission check — tool_call matches permission_config rules"""
    if state.get("_permission_granted", True):
        print(f"    → auto-approved")
        return "execute"
    else:
        print(f"    → needs approval")
        return "prompt_user"


def route_check_context(state: AgentState) -> str:
    """Gate: Context limit? — conversation_history.token_count approaching capacity"""
    if (state.get("_context_token_count", 0) > state.get("_context_capacity", 100000) * 0.8):
        print(f"    → context full")
        return "compact"
    else:
        print(f"    → context ok")
        return "llm_call"


def route_loop_persist(state: AgentState) -> str:
    """Loop: Wait for next input — always"""
    if state.get("_done"):
        return "END"
    if bool(state.get("always")):
        return "idle"
    else:
        return "END"


def route_loop_append(state: AgentState) -> str:
    """Loop: Re-reason after denial — permission_denied"""
    if state.get("_done"):
        return "END"
    if bool(state.get("permission_denied")):
        return "llm_call"
    else:
        return "END"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("idle", node_idle)
    graph.add_node("receive", node_receive)
    graph.add_node("build_context", node_build_context)
    graph.add_node("llm_call", node_llm_call)
    graph.add_node("parse_response", node_parse_response)
    graph.add_node("emit_text", node_emit_text)
    graph.add_node("prompt_user", node_prompt_user)
    graph.add_node("execute", node_execute)
    graph.add_node("observe", node_observe)
    graph.add_node("append", node_append)
    graph.add_node("compact", node_compact)
    graph.add_node("persist", node_persist)
    graph.add_node("spawn_sub_agent", node_spawn_sub_agent)
    graph.add_node("hook_governance", node_hook_governance)
    graph.add_node("hook_malware", node_hook_malware)
    graph.add_node("permission_policy", node_permission_policy)

    graph.set_entry_point("idle")

    graph.add_edge("idle", "receive")
    graph.add_edge("receive", "build_context")
    graph.add_edge("build_context", "llm_call")
    graph.add_edge("llm_call", "parse_response")
    graph.add_conditional_edges(
        "parse_response",
        route_check_tools,
        {
            "emit_text": "emit_text",
            "check_permission": "check_permission",
        }
    )
    graph.add_edge("emit_text", "persist")
    graph.add_edge("prompt_user", "execute")
    graph.add_edge("prompt_user", "append")
    graph.add_edge("execute", "observe")
    graph.add_edge("observe", "append")
    graph.add_conditional_edges(
        "append",
        route_check_context,
        {
            "llm_call": "llm_call",
            "compact": "compact",
        }
    )
    graph.add_edge("compact", "llm_call")
    graph.add_conditional_edges(
        "persist",
        route_loop_persist,
        {
            "idle": "idle",
            "END": END,
        }
    )
    graph.add_edge("spawn_sub_agent", END)
    graph.add_edge("hook_governance", END)
    graph.add_edge("hook_malware", END)
    graph.add_edge("permission_policy", END)

    # Pass-through node for chained gate: check_permission
    graph.add_node("check_permission", lambda state: {})
    graph.add_conditional_edges(
        "check_permission",
        route_check_permission,
        {
            "execute": "execute",
            "prompt_user": "prompt_user",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Claude Code — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Claude Code (LangGraph)")
    print(f"  Anthropic's official CLI agent — interactive coding assistant with persistent memory, tool use, sub-agent spawning, and human-in-the-loop permission gates")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    initial = {}
    initial["text"] = input("Enter text: ")
    initial["type"] = input("Enter type: ")
    initial["files"] = input("Enter files: ")
    run(initial)