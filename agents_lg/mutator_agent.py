#!/usr/bin/env python3
"""
Mutator — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Meta-agent that diagnoses failures in a target agent spec and produces an improved version. Used in meta-evolution to evolve the evolver itself.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=8192):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=8192, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "MutationTask": {
        "description": """The improvement task inputs""",
        "fields": [{"name": "spec_yaml", "type": "string"}, {"name": "failure_summary", "type": "string"}, {"name": "benchmark_description", "type": "string"}],
    },
    "DiagnosisInput": {
        "description": """Input to the diagnostician agent""",
        "fields": [{"name": "spec_yaml", "type": "string"}, {"name": "failure_summary", "type": "string"}, {"name": "benchmark_description", "type": "string"}],
    },
    "DiagnosisOutput": {
        "description": """Output from the diagnostician agent""",
        "fields": [{"name": "diagnosis", "type": "string"}, {"name": "fix_type", "type": "string"}, {"name": "fix_details", "type": "string"}],
    },
    "EditorInput": {
        "description": """Input to the editor agent""",
        "fields": [{"name": "spec_yaml", "type": "string"}, {"name": "diagnosis", "type": "string"}, {"name": "fix_type", "type": "string"}, {"name": "fix_details", "type": "string"}],
    },
    "EditorOutput": {
        "description": """Output from the editor agent""",
        "fields": [{"name": "mutated_spec_yaml", "type": "string"}, {"name": "changes_summary", "type": "string"}],
    },
    "ValidationResult": {
        "description": """Result of validating the mutated spec""",
        "fields": [{"name": "is_valid", "type": "boolean"}, {"name": "validation_error", "type": "string"}],
    },
    "MutatorOutput": {
        "description": """Final output with the improved or original spec""",
        "fields": [{"name": "final_spec_yaml", "type": "string"}, {"name": "mutation_applied", "type": "boolean"}, {"name": "changes_summary", "type": "string"}, {"name": "diagnosis", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Mutator"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    answer: Any
    benchmark_description: str
    changes_summary: str
    diagnose_result: Any
    diagnosis: str
    failure_summary: str
    final_spec_yaml: str
    fix_details: str
    fix_type: str
    is_valid: bool
    mutated_spec_yaml: str
    mutation_applied: bool
    spec_yaml: str
    validation_error: str


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_diagnostician(user_message, output_schema=None):
    """Diagnostician Agent"""
    system = """You are an expert agent architecture diagnostician. You receive:
1. An agent spec (YAML describing the agent's architecture)
2. A failure summary (specific examples where the agent failed)
3. A benchmark description (what the benchmark tests)

Your job: analyze WHY the agent fails on these examples. Consider:
- Is the agent missing a reasoning step?
- Are the prompts too vague or missing format instructions?
- Is data not flowing correctly between steps?
- Is the agent missing a validation or review step?
- Could the agent benefit from a different tool strategy?

Output JSON with:
- "diagnosis": a concise explanation of the root cause (1-3 sentences)
- "fix_type": one of "edit_prompt", "add_step", "modify_logic", "add_review", "restructure"
- "fix_details": specific instructions for what to change (be precise about which entity/process to modify and exactly what the change should be)
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Diagnostician Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_task(state: AgentState) -> dict:
    """
    Receive Task
    Accept the improvement task inputs
    """
    print(f"  → Receive Task")
    updates = {}

    # Logic from spec
    print(f"    Spec to improve: {state.get('spec_yaml', '')[:80]}...")
    print(f"    Failures: {state.get('failure_summary', '')[:80]}...")
    print(f"    Benchmark: {state.get('benchmark_description', '')[:80]}...")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_diagnose(state: AgentState) -> dict:
    """
    Diagnose Failures
    Invoke the diagnostician agent to analyze why the spec fails
    """
    print(f"  → Diagnose Failures")
    updates = {}

    # Logic from spec
    print(f"    Diagnosing failures...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Diagnose failures
    _cur = dict(state)
    _cur.update(updates)
    diagnostician_input = build_input(_cur, "DiagnosisInput")
    diagnostician_msg = json.dumps(diagnostician_input, default=str)
    diagnostician_raw = invoke_diagnostician(diagnostician_msg, output_schema="DiagnosisOutput")
    diagnostician_result = parse_response(diagnostician_raw, "DiagnosisOutput")
    updates["_schema_violations"] = len(validate_output(diagnostician_result, "DiagnosisOutput"))
    updates.update(diagnostician_result)
    updates["diagnosis_output"] = diagnostician_result
    updates["diagnose_result"] = diagnostician_result
    print(f"    ← Diagnostician Agent: {diagnostician_result}")

    return updates


def node_apply_mutation(state: AgentState) -> dict:
    """
    Apply Mutation
    Call editor LLM directly to produce raw YAML (avoids JSON wrapping)
    """
    print(f"  → Apply Mutation")
    updates = {}

    # Logic from spec
    print(f"    Applying mutation: {state.get('fix_type', 'unknown')} — {state.get('fix_details', '')[:80]}...")
    _sys = (
        "You are an agent spec editor. You receive a complete agent spec in YAML format, "
        "a diagnosis explaining what's wrong, and fix details explaining what to change.\n\n"
        "Your job: output the COMPLETE IMPROVED spec as valid YAML.\n\n"
        "Rules:\n"
        "- Output ONLY valid YAML — no JSON wrapping, no markdown fences, no explanation\n"
        "- Include ALL sections: name, version, description, entry_point, entities, processes, edges, schemas\n"
        "- Make targeted changes based on the fix_details\n"
        "- Keep all entity IDs, process IDs, and schema names consistent with edges\n"
        "- Do NOT change model fields\n"
        "- Do NOT add comments — just the YAML content\n"
        "- Start your output with 'name:' on the first line\n\n"
        "Your ENTIRE response must be a valid YAML document."
    )
    _user = (
        f"## Original Spec\n\n{state.get('spec_yaml', '')}\n\n"
        f"## Diagnosis\n\n{state.get('diagnosis', '')}\n\n"
        f"## Fix Type\n\n{state.get('fix_type', '')}\n\n"
        f"## Fix Details\n\n{state.get('fix_details', '')}\n\n"
        "Output the complete improved spec as valid YAML. Start with 'name:' on the first line."
    )
    _resp = call_llm("gemini-3-flash-preview", _sys, _user, temperature=0.3, max_tokens=8192)
    updates["mutated_spec_yaml"] = _resp.strip()
    updates["changes_summary"] = state.get("fix_details", "")
    print(f"    Editor output: {len(_resp)} chars")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_validate_output(state: AgentState) -> dict:
    """
    Validate Output
    Check that the mutated spec is valid YAML with required sections
    """
    print(f"  → Validate Output")
    updates = {}

    # Logic from spec
    import yaml as _yaml
    import re as _re
    spec_text = state.get("mutated_spec_yaml", "")
    # Strip markdown fences if present (editor should output raw YAML)
    if spec_text:
        m = _re.search(r'```(?:yaml)?\s*\n(.*?)```', spec_text, _re.DOTALL)
        if m:
            spec_text = m.group(1).strip()
            updates["mutated_spec_yaml"] = spec_text
    try:
        parsed = _yaml.safe_load(spec_text)
        if not isinstance(parsed, dict):
            updates["is_valid"] = False
            updates["validation_error"] = "Parsed YAML is not a dict"
        elif not all(k in parsed for k in ("entities", "processes", "edges")):
            updates["is_valid"] = False
            updates["validation_error"] = "Missing required sections: entities, processes, or edges"
        else:
            updates["is_valid"] = True
            updates["validation_error"] = ""
            print(f"    Validation passed: {len(parsed.get('entities', []))} entities, {len(parsed.get('processes', []))} processes, {len(parsed.get('edges', []))} edges")
    except Exception as e:
        updates["is_valid"] = False
        updates["validation_error"] = str(e)
    if not state.get("is_valid"):
        print(f"    Validation failed: {state.get('validation_error', '')[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_emit_result(state: AgentState) -> dict:
    """
    Emit Result
    Output the final spec YAML (improved if valid, original if mutation failed)
    """
    print(f"  → Emit Result")
    updates = {}

    # Logic from spec
    if state.get("is_valid"):
        updates["final_spec_yaml"] = state.get("mutated_spec_yaml", "")
        updates["mutation_applied"] = True
        print(f"    Emitting improved spec ({len(state.get('final_spec_yaml', ""))} chars)")
    else:
        updates["final_spec_yaml"] = state.get("spec_yaml", "")
        updates["mutation_applied"] = False
        print(f"    Mutation failed validation, returning original spec")
    updates["answer"] = state.get("final_spec_yaml", "")
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_task", node_receive_task)
    graph.add_node("diagnose", node_diagnose)
    graph.add_node("apply_mutation", node_apply_mutation)
    graph.add_node("validate_output", node_validate_output)
    graph.add_node("emit_result", node_emit_result)

    graph.set_entry_point("receive_task")

    graph.add_edge("receive_task", "diagnose")
    graph.add_edge("diagnose", "apply_mutation")
    graph.add_edge("apply_mutation", "validate_output")
    graph.add_edge("validate_output", "emit_result")
    graph.add_edge("emit_result", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Mutator — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Mutator (LangGraph)")
    print(f"  Meta-agent that diagnoses failures in a target agent spec and produces an improved version. Used in meta-evolution to evolve the evolver itself.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()