#!/usr/bin/env python3
"""
Tree of Thought — Generated by OpenClaw Instantiation Engine (LangGraph backend)
Spec: Tree-of-Thought agent that explores multiple reasoning paths, evaluates them, prunes weak branches, and selects the best leaf as the final answer.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "ProblemInput": {
        "description": """The problem to solve using tree-of-thought""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "max_depth", "type": "integer"}, {"name": "k_candidates", "type": "integer"}, {"name": "top_n", "type": "integer"}],
    },
    "GeneratorInput": {
        "description": """Input to the thought generator agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "parent_thought", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "k_candidates", "type": "integer"}],
    },
    "GeneratedThoughts": {
        "description": """Output from the thought generator""",
        "fields": [{"name": "thoughts", "type": "list<ThoughtNode>"}],
    },
    "EvaluatorInput": {
        "description": """Input to the thought evaluator agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "thought_content", "type": "string"}, {"name": "depth", "type": "integer"}],
    },
    "EvaluationResult": {
        "description": """Evaluation score and rationale for a thought""",
        "fields": [{"name": "score", "type": "integer"}, {"name": "rationale", "type": "string"}],
    },
    "ThoughtNode": {
        "description": """A single node in the thought tree""",
        "fields": [{"name": "thought_id", "type": "string"}, {"name": "content", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "score", "type": "integer"}, {"name": "parent_id", "type": "string"}],
    },
    "FinalAnswer": {
        "description": """The final answer selected from the thought tree""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "answer_score", "type": "integer"}, {"name": "total_thoughts_explored", "type": "integer"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Tree of Thought"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    all_thoughts: Any
    answer: str
    answer_score: int
    best_thought: Any
    content: str
    current_depth: Any
    current_parents: Any
    depth: int
    eval_index: Any
    evaluate_thoughts_result: Any
    generate_thoughts_result: Any
    generated_children: Any
    k_candidates: int
    max_depth: int
    parent_id: str
    parent_index: Any
    parent_thought: str
    problem: str
    rationale: str
    score: int
    thought_content: str
    thought_id: str
    thought_store: list
    thoughts: list
    top_n: int
    total_thoughts_explored: int


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_thought_store:
    """Thought Store (queue)"""
    def __init__(self):
        self.queue = []

    def read(self, key=None):
        return self.queue[0] if self.queue else None

    def write(self, value, key=None):
        self.queue.append(value)


# Store instances
_store_thought_store = Store_thought_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_thought_generator(user_message, output_schema=None):
    """Thought Generator"""
    system = """You are a creative reasoning agent. Given a problem and a parent thought (or the root problem),
generate exactly K candidate next-step thoughts. Each thought should be a distinct reasoning path
that advances toward solving the problem. Output them as a JSON array of objects, each with
"thought_id" (string) and "content" (string describing the reasoning step).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Thought Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_thought_evaluator(user_message, output_schema=None):
    """Thought Evaluator"""
    system = """You are a critical evaluator of reasoning steps. Given a problem and a candidate thought,
score how promising this thought is on a scale of 1-10 (10 = highly promising, likely leads
to correct solution; 1 = dead end or incorrect reasoning). Also provide a brief rationale.
Output JSON with "score" (integer 1-10) and "rationale" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Thought Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_problem(state: AgentState) -> dict:
    """
    Receive Problem
    Accept the problem input and initialize the tree-of-thought state
    """
    print(f"  → Receive Problem")
    updates = {}

    # Logic from spec
    updates["current_depth"] = 0
    updates["max_depth"] = state.get("max_depth", 3)
    updates["k_candidates"] = state.get("k_candidates", 3)
    updates["top_n"] = state.get("top_n", 2)
    updates["all_thoughts"] = []
    # Initialize with root node representing the problem itself
    root = {
        "thought_id": "root",
        "content": state.get("problem", ""),
        "depth": 0,
        "score": 0,
        "parent_id": None
    }
    updates["current_parents"] = [root]
    _list = list(state.get("all_thoughts", []) or []); _list.append(root); updates["all_thoughts"] = _list
    print(f"    Problem: {state.get('problem', '')[:100]}")
    print(f"    Config: depth={state.get('max_depth', "")}, K={state.get('k_candidates', "")}, top-N={state.get('top_n', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_generate_thoughts(state: AgentState) -> dict:
    """
    Generate Thoughts
    For each current parent thought, invoke the generator to produce K candidate children
    """
    print(f"  → Generate Thoughts")
    updates = {}

    # Logic from spec
    updates["current_depth"] = state.get("current_depth", 0) + 1
    parents = state.get("current_parents", [])
    updates["parent_index"] = 0
    updates["generated_children"] = []
    print(f"    Generating thoughts at depth {state.get('current_depth', "")} for {len(parents)} parent(s)")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate K candidate thoughts
    _cur = dict(state)
    _cur.update(updates)
    thought_generator_input = build_input(_cur, "GeneratorInput")
    thought_generator_msg = json.dumps(thought_generator_input, default=str)
    thought_generator_raw = invoke_thought_generator(thought_generator_msg, output_schema="GeneratedThoughts")
    thought_generator_result = parse_response(thought_generator_raw, "GeneratedThoughts")
    updates["_schema_violations"] = len(validate_output(thought_generator_result, "GeneratedThoughts"))
    updates.update(thought_generator_result)
    updates["generated_thoughts"] = thought_generator_result
    updates["generate_thoughts_result"] = thought_generator_result
    print(f"    ← Thought Generator: {thought_generator_result}")

    return updates


def node_evaluate_thoughts(state: AgentState) -> dict:
    """
    Evaluate Thoughts
    Invoke the evaluator agent to score each generated thought
    """
    print(f"  → Evaluate Thoughts")
    updates = {}

    # Logic from spec
    children = state.get("generated_children", [])
    updates["eval_index"] = 0
    print(f"    Evaluating {len(children)} candidate thoughts")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Evaluate each thought
    _cur = dict(state)
    _cur.update(updates)
    thought_evaluator_input = build_input(_cur, "EvaluatorInput")
    thought_evaluator_msg = json.dumps(thought_evaluator_input, default=str)
    thought_evaluator_raw = invoke_thought_evaluator(thought_evaluator_msg, output_schema="EvaluationResult")
    thought_evaluator_result = parse_response(thought_evaluator_raw, "EvaluationResult")
    updates["_schema_violations"] = len(validate_output(thought_evaluator_result, "EvaluationResult"))
    updates.update(thought_evaluator_result)
    updates["evaluation_result"] = thought_evaluator_result
    updates["evaluate_thoughts_result"] = thought_evaluator_result
    print(f"    ← Thought Evaluator: {thought_evaluator_result}")

    return updates


def node_expand_thoughts(state: AgentState) -> dict:
    """
    Expand Thoughts
    Prune to top-N thoughts by score and set them as parents for the next depth level
    """
    print(f"  → Expand Thoughts")
    updates = {}

    # Logic from spec
    children = state.get("generated_children", [])
    top_n = state.get("top_n", 2)
    # Sort by score descending and keep top N
    sorted_children = sorted(children, key=lambda t: t.get("score", 0), reverse=True)
    selected = sorted_children[:top_n]
    updates["current_parents"] = selected
    # Add to all_thoughts
    all_thoughts = state.get("all_thoughts", [])
    all_thoughts.extend(selected)
    updates["all_thoughts"] = all_thoughts
    print(f"    Selected top-{top_n} thoughts (scores: {[t.get('score', 0) for t in selected]})")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Persist selected thoughts
    _cur = dict(state)
    _cur.update(updates)
    thought_store_write = build_input(_cur, "ThoughtNode")
    _store_thought_store.write(thought_store_write)

    return updates


def node_select_best(state: AgentState) -> dict:
    """
    Select Best
    Among all leaf-level thoughts, select the one with the highest score
    """
    print(f"  → Select Best")
    updates = {}

    # Read: Read all thoughts
    thought_store_data = _store_thought_store.read()
    updates["thought_store"] = thought_store_data

    # Logic from spec
    max_depth = state.get("current_depth", state.get("max_depth", 3))
    all_thoughts = state.get("all_thoughts", [])
    leaves = [t for t in all_thoughts if t.get("depth", 0) == max_depth]
    if not leaves:
        leaves = state.get("current_parents", [])
    best = max(leaves, key=lambda t: t.get("score", 0)) if leaves else {"content": "No solution found", "score": 0}
    updates["best_thought"] = best
    print(f"    Best thought (score={best.get('score', 0)}): {best.get('content', '')[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_produce_answer(state: AgentState) -> dict:
    """
    Produce Answer
    Format the best thought into the final answer output
    """
    print(f"  → Produce Answer")
    updates = {}

    # Logic from spec
    best = state.get("best_thought", {})
    updates["answer"] = best.get("content", "No answer found")
    updates["answer_score"] = best.get("score", 0)
    updates["total_thoughts_explored"] = len(state.get("all_thoughts", []))
    print(f"    Final answer (score={state.get('answer_score', "")}): {state.get('answer', "")[:200]}")
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_depth(state: AgentState) -> str:
    """Gate: Depth limit reached? — current_depth >= max_depth"""
    if (state.get("current_depth", 0)) >= (state.get("max_depth", 0)):
        print(f"    → depth >= max_depth")
        return "select_best"
    else:
        print(f"    → depth < max_depth")
        return "generate_thoughts"


def route_loop_expand_thoughts(state: AgentState) -> str:
    """Loop: Next depth level — current_depth < max_depth"""
    if state.get("_done"):
        return "END"
    if (state.get("current_depth", 0)) < (state.get("max_depth", 0)):
        return "check_depth"
    else:
        return "END"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_problem", node_receive_problem)
    graph.add_node("generate_thoughts", node_generate_thoughts)
    graph.add_node("evaluate_thoughts", node_evaluate_thoughts)
    graph.add_node("expand_thoughts", node_expand_thoughts)
    graph.add_node("select_best", node_select_best)
    graph.add_node("produce_answer", node_produce_answer)

    graph.set_entry_point("receive_problem")

    graph.add_conditional_edges(
        "receive_problem",
        route_check_depth,
        {
            "generate_thoughts": "generate_thoughts",
            "select_best": "select_best",
        }
    )
    graph.add_edge("generate_thoughts", "evaluate_thoughts")
    graph.add_edge("evaluate_thoughts", "expand_thoughts")
    graph.add_conditional_edges(
        "expand_thoughts",
        route_check_depth,
        {
            "generate_thoughts": "generate_thoughts",
            "select_best": "select_best",
        }
    )
    graph.add_edge("select_best", "produce_answer")
    graph.add_edge("produce_answer", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Tree of Thought — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Tree of Thought (LangGraph)")
    print(f"  Tree-of-Thought agent that explores multiple reasoning paths, evaluates them, prunes weak branches, and selects the best leaf as the final answer.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()