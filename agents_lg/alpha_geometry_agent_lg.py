#!/usr/bin/env python3
"""
AlphaGeometry-style Agent — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Neural-symbolic geometry solver inspired by AlphaGeometry (Trinh et al. 2024). LLM proposes construction steps; symbolic deduction engine checks validity and derives consequences. The loop continues until goal is proved or max iterations reached.

"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "ConstructionInput": {
        "description": """""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "known_facts", "type": "list<string>"}, {"name": "goal", "type": "string"}],
    },
    "ConstructionOutput": {
        "description": """""",
        "fields": [{"name": "construction", "type": "string"}, {"name": "formal", "type": "string"}],
    },
    "DeductionResult": {
        "description": """""",
        "fields": [{"name": "new_facts", "type": "list<string>"}, {"name": "goal_proved", "type": "boolean"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for AlphaGeometry-style Agent"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    answer: Any
    construction: str
    fact_store: dict
    formal: str
    goal: str
    goal_proved: bool
    iteration: Any
    known_facts: list
    max_iterations: Any
    new_facts: list
    problem: str
    proof_steps: Any
    propose_construction_result: Any
    proved: Any
    query: Any


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_fact_store:
    """Derived Facts (logic_program)"""
    def __init__(self):
        self.data = {}
        self._rules = []

    def read(self, key=None):
        if key is None:
            return self._rules
        return [r for r in self._rules if key in str(r)]

    def write(self, value, key=None):
        self._rules.append(value)


# Store instances
_store_fact_store = Store_fact_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_construction_proposer(user_message, output_schema=None):
    """Construction Proposer (LLM)"""
    system = """You are a geometry construction proposer. Given a geometry problem and the current known facts (derived by symbolic deduction), propose a single auxiliary construction (e.g., "construct midpoint M of segment AB" or "draw line through P parallel to AB"). Output JSON: {"construction": "description", "formal": "formal notation"}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Construction Proposer (LLM)", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_problem(state: AgentState) -> dict:
    """
    Receive Geometry Problem
    """
    print(f"  → Receive Geometry Problem")
    updates = {}

    # Logic from spec
    updates["problem"] = state.get("problem", state.get("query", ""))
    updates["goal"] = state.get("goal", "Prove triangle ABC is isosceles")
    updates["known_facts"] = state.get("known_facts", ["AB = AC", "angle BAC = 60"])
    updates["iteration"] = 0
    updates["max_iterations"] = state.get("max_iterations", 5)
    updates["proof_steps"] = []
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_propose_construction(state: AgentState) -> dict:
    """
    Propose Construction
    """
    print(f"  → Propose Construction")
    updates = {}

    # Invoke: construction_proposer
    _cur = dict(state)
    _cur.update(updates)
    construction_proposer_input = build_input(_cur, "ConstructionInput")
    construction_proposer_msg = json.dumps(construction_proposer_input, default=str)
    construction_proposer_raw = invoke_construction_proposer(construction_proposer_msg, output_schema="ConstructionOutput")
    construction_proposer_result = parse_response(construction_proposer_raw, "ConstructionOutput")
    updates["_schema_violations"] = len(validate_output(construction_proposer_result, "ConstructionOutput"))
    updates.update(construction_proposer_result)
    updates["construction_output"] = construction_proposer_result
    updates["propose_construction_result"] = construction_proposer_result
    print(f"    ← Construction Proposer (LLM): {construction_proposer_result}")

    return updates


def node_translate_to_formal(state: AgentState) -> dict:
    """
    Construction → Formal Rules
    """
    print(f"  → Construction → Formal Rules")
    updates = {}

    return updates


def node_run_deduction(state: AgentState) -> dict:
    """
    Symbolic Deduction Engine
    """
    print(f"  → Symbolic Deduction Engine")
    updates = {}

    return updates


def node_update_facts(state: AgentState) -> dict:
    """
    Update Known Facts
    """
    print(f"  → Update Known Facts")
    updates = {}

    # Logic from spec
    new_facts = state.get("new_facts", [])
    updates["known_facts"] = state.get("known_facts", []) + new_facts
    updates["iteration"] = state.get("iteration", 0) + 1
    construction = state.get("construction", "")
    if construction:
        updates["proof_steps"] = state.get("proof_steps", []) + [construction]
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_emit_proof(state: AgentState) -> dict:
    """
    Emit Proof
    """
    print(f"  → Emit Proof")
    updates = {}

    # Logic from spec
    updates["proved"] = True
    updates["answer"] = "Proof found: " + "; ".join(state.get("proof_steps", []))
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_emit_failure(state: AgentState) -> dict:
    """
    Emit Failure
    """
    print(f"  → Emit Failure")
    updates = {}

    # Logic from spec
    updates["proved"] = False
    updates["answer"] = "Could not prove within iteration limit."
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_handle_timeout(state: AgentState) -> dict:
    """
    Handle Deduction Timeout
    """
    print(f"  → Handle Deduction Timeout")
    updates = {}

    # Logic from spec
    updates["new_facts"] = []
    updates["goal_proved"] = False
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_proved(state: AgentState) -> str:
    """Gate: Goal Proved? — goal_proved"""
    if bool(state.get("goal_proved")):
        print(f"    → goal_proved is true")
        return "emit_proof"
    else:
        print(f"    → goal_proved is false")
        return "check_iterations"


def route_check_iterations(state: AgentState) -> str:
    """Gate: Iterations Remaining? — iteration < max_iterations"""
    if (state.get("iteration", 0)) < (state.get("max_iterations", 0)):
        print(f"    → iteration < max_iterations")
        return "propose_construction"
    else:
        print(f"    → iteration >= max_iterations")
        return "emit_failure"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_problem", node_receive_problem)
    graph.add_node("propose_construction", node_propose_construction)
    graph.add_node("translate_to_formal", node_translate_to_formal)
    graph.add_node("run_deduction", node_run_deduction)
    graph.add_node("update_facts", node_update_facts)
    graph.add_node("emit_proof", node_emit_proof)
    graph.add_node("emit_failure", node_emit_failure)
    graph.add_node("handle_timeout", node_handle_timeout)

    graph.set_entry_point("receive_problem")

    graph.add_edge("receive_problem", "propose_construction")
    graph.add_edge("propose_construction", "translate_to_formal")
    graph.add_edge("translate_to_formal", "run_deduction")
    graph.add_edge("run_deduction", "update_facts")
    graph.add_conditional_edges(
        "update_facts",
        route_check_proved,
        {
            "emit_proof": "emit_proof",
            "check_iterations": "check_iterations",
        }
    )
    graph.add_edge("emit_proof", END)
    graph.add_edge("emit_failure", END)
    graph.add_edge("handle_timeout", "update_facts")

    # Pass-through node for chained gate: check_iterations
    graph.add_node("check_iterations", lambda state: {})
    graph.add_conditional_edges(
        "check_iterations",
        route_check_iterations,
        {
            "propose_construction": "propose_construction",
            "emit_failure": "emit_failure",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """AlphaGeometry-style Agent — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  AlphaGeometry-style Agent (LangGraph)")
    print(f"  Neural-symbolic geometry solver inspired by AlphaGeometry (Trinh et al. 2024). LLM proposes construction steps; symbolic deduction engine checks validity and derives consequences. The loop continues until goal is proved or max iterations reached.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()