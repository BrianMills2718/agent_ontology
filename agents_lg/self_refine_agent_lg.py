#!/usr/bin/env python3
"""
Self Refine — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Self-Refine agent with a generator-critic loop: generates output, critiques it, and iteratively refines based on feedback until quality threshold is met or max rounds reached.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TaskInput": {
        "description": """The task to generate output for""",
        "fields": [{"name": "task", "type": "string"}],
    },
    "GeneratorInput": {
        "description": """Input to the generator agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "previous_output", "type": "string"}, {"name": "specific_feedback", "type": "string"}, {"name": "refinement_round", "type": "integer"}],
    },
    "GeneratorOutput": {
        "description": """Output from the generator agent""",
        "fields": [{"name": "output_text", "type": "string"}, {"name": "changes_made", "type": "string"}],
    },
    "CriticInput": {
        "description": """Input to the critic agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "output_text", "type": "string"}, {"name": "refinement_round", "type": "integer"}],
    },
    "CriticOutput": {
        "description": """Output from the critic agent""",
        "fields": [{"name": "quality_score", "type": "integer"}, {"name": "strengths", "type": "list<string>"}, {"name": "weaknesses", "type": "list<string>"}, {"name": "specific_feedback", "type": "string"}],
    },
    "FinalOutput": {
        "description": """The finalized output with quality metadata""",
        "fields": [{"name": "final_output", "type": "string"}, {"name": "final_score", "type": "integer"}, {"name": "total_rounds", "type": "integer"}, {"name": "feedback_history", "type": "list<FeedbackEntry>"}],
    },
    "FeedbackEntry": {
        "description": """A single round of feedback from the critic""",
        "fields": [{"name": "round_number", "type": "integer"}, {"name": "quality_score", "type": "integer"}, {"name": "specific_feedback", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Self Refine"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    changes_made: str
    critique_result: Any
    current_output: Any
    feedback_history: list
    final_output: str
    final_score: int
    generate_result: Any
    max_rounds: Any
    output_text: str
    previous_output: str
    quality_score: int
    quality_threshold: Any
    refinement_round: int
    round_number: int
    specific_feedback: str
    strengths: list
    task: str
    total_rounds: int
    weaknesses: list


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_generator(user_message, output_schema=None):
    """Generator Agent"""
    system = """You are a skilled content generator. When given a task, produce a high-quality output.
If you also receive feedback from a previous round, incorporate that feedback to improve
your output. Focus on addressing every piece of specific feedback provided.
Output JSON with "output_text" (string with your generated content) and
"changes_made" (string summarizing what you changed from the previous version, or "initial" if first attempt).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Generator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_critic(user_message, output_schema=None):
    """Critic Agent"""
    system = """You are a demanding but fair critic. Given a task description and a generated output,
evaluate the quality of the output on a scale of 1-10 (10 = excellent, publication-ready;
7 = good enough; below 7 = needs improvement). Provide specific, actionable feedback
listing exactly what should be improved. Output JSON with "quality_score" (integer 1-10),
"strengths" (list of strings), "weaknesses" (list of strings), and
"specific_feedback" (string with detailed improvement instructions).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Critic Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_task(state: AgentState) -> dict:
    """
    Receive Task
    Accept the task input and initialize refinement state
    """
    print(f"  → Receive Task")
    updates = {}

    # Logic from spec
    updates["refinement_round"] = 0
    updates["max_rounds"] = 3
    updates["quality_threshold"] = 7
    updates["feedback_history"] = []
    updates["current_output"] = ""
    print(f"    Task: {state.get('task', '')[:100]}")
    print(f"    Config: max_rounds={state.get('max_rounds', "")}, threshold={state.get('quality_threshold', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_generate(state: AgentState) -> dict:
    """
    Generate Output
    Invoke the generator agent to produce or refine output
    """
    print(f"  → Generate Output")
    updates = {}

    # Logic from spec
    round_num = state.get("refinement_round", 0)
    if round_num == 0:
        print(f"    Generating initial output...")
    else:
        print(f"    Refining output (round {round_num})...")
        print(f"    Using feedback: {state.get('specific_feedback', '')[:80]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate or refine output
    _cur = dict(state)
    _cur.update(updates)
    generator_input = build_input(_cur, "GeneratorInput")
    generator_msg = json.dumps(generator_input, default=str)
    generator_raw = invoke_generator(generator_msg, output_schema="GeneratorOutput")
    generator_result = parse_response(generator_raw, "GeneratorOutput")
    updates["_schema_violations"] = len(validate_output(generator_result, "GeneratorOutput"))
    updates.update(generator_result)
    updates["generator_output"] = generator_result
    updates["generate_result"] = generator_result
    print(f"    ← Generator Agent: {generator_result}")

    return updates


def node_critique(state: AgentState) -> dict:
    """
    Critique Output
    Invoke the critic agent to evaluate the generated output
    """
    print(f"  → Critique Output")
    updates = {}

    # Logic from spec
    current = state.get("current_output", "")
    print(f"    Critiquing output ({len(current)} chars)...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Evaluate output quality
    _cur = dict(state)
    _cur.update(updates)
    critic_input = build_input(_cur, "CriticInput")
    critic_msg = json.dumps(critic_input, default=str)
    critic_raw = invoke_critic(critic_msg, output_schema="CriticOutput")
    critic_result = parse_response(critic_raw, "CriticOutput")
    updates["_schema_violations"] = len(validate_output(critic_result, "CriticOutput"))
    updates.update(critic_result)
    updates["critic_output"] = critic_result
    updates["critique_result"] = critic_result
    print(f"    ← Critic Agent: {critic_result}")

    return updates


def node_refine(state: AgentState) -> dict:
    """
    Refine
    Prepare feedback for the generator and increment the round counter
    """
    print(f"  → Refine")
    updates = {}

    # Logic from spec
    updates["refinement_round"] = state.get("refinement_round", 0) + 1
    # Record feedback for history
    feedback_entry = {
        "round": state.get("refinement_round", ""),
        "quality_score": state.get("quality_score", 0),
        "specific_feedback": state.get("specific_feedback", ""),
    }
    history = state.get("feedback_history", [])
    history.append(feedback_entry)
    updates["feedback_history"] = history
    print(f"    Preparing refinement round {state.get('refinement_round', "")}/{state.get('max_rounds', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_finalize(state: AgentState) -> dict:
    """
    Finalize
    Produce the final output with quality metadata
    """
    print(f"  → Finalize")
    updates = {}

    # Logic from spec
    score = state.get("quality_score", 0)
    rounds = state.get("refinement_round", 0)
    threshold = state.get("quality_threshold", 7)
    if score >= threshold:
        print(f"    Quality threshold met (score={score}). Finalizing after {rounds} refinement(s).")
    else:
        print(f"    Max rounds reached (score={score}). Finalizing with best effort after {rounds} round(s).")
    updates["final_output"] = state.get("current_output", "")
    updates["final_score"] = score
    updates["total_rounds"] = rounds
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_quality(state: AgentState) -> str:
    """Gate: Quality >= threshold? — quality_score >= quality_threshold"""
    if (state.get("quality_score", 0)) >= (state.get("quality_threshold", 0)):
        print(f"    → score >= 7")
        return "finalize"
    else:
        print(f"    → score < 7")
        return "check_rounds"


def route_check_rounds(state: AgentState) -> str:
    """Gate: Rounds remaining? — refinement_round < max_rounds"""
    if (state.get("refinement_round", 0)) < (state.get("max_rounds", 0)):
        print(f"    → rounds remaining")
        return "refine"
    else:
        print(f"    → max rounds reached")
        return "finalize"


def route_loop_refine(state: AgentState) -> str:
    """Loop: Refinement loop — refinement_round < max_rounds"""
    if state.get("_done"):
        return "END"
    if (state.get("refinement_round", 0)) < (state.get("max_rounds", 0)):
        return "generate"
    else:
        return "END"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_task", node_receive_task)
    graph.add_node("generate", node_generate)
    graph.add_node("critique", node_critique)
    graph.add_node("refine", node_refine)
    graph.add_node("finalize", node_finalize)

    graph.set_entry_point("receive_task")

    graph.add_edge("receive_task", "generate")
    graph.add_edge("generate", "critique")
    graph.add_conditional_edges(
        "critique",
        route_check_quality,
        {
            "finalize": "finalize",
            "check_rounds": "check_rounds",
        }
    )
    graph.add_edge("refine", "generate")
    graph.add_edge("finalize", END)

    # Pass-through node for chained gate: check_rounds
    graph.add_node("check_rounds", lambda state: {})
    graph.add_conditional_edges(
        "check_rounds",
        route_check_rounds,
        {
            "refine": "refine",
            "finalize": "finalize",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Self Refine — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Self Refine (LangGraph)")
    print(f"  Self-Refine agent with a generator-critic loop: generates output, critiques it, and iteratively refines based on feedback until quality threshold is met or max rounds reached.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()