#!/usr/bin/env python3
"""
Reflexion — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Reflexion agent that attempts tasks, self-evaluates, generates verbal reflections on failure, and retries with accumulated episodic memory until success or max trials.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TaskSpec": {
        "description": """The task to attempt with success criteria""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}],
    },
    "ActorInput": {
        "description": """Input to the actor agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}, {"name": "reflections", "type": "list<string>"}, {"name": "trial", "type": "integer"}],
    },
    "ActorOutput": {
        "description": """Output from the actor agent""",
        "fields": [{"name": "trajectory", "type": "list<string>"}, {"name": "answer", "type": "string"}],
    },
    "EvaluatorInput": {
        "description": """Input to the evaluator agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}, {"name": "answer", "type": "string"}, {"name": "trajectory", "type": "list<string>"}],
    },
    "EvaluatorOutput": {
        "description": """Output from the evaluator agent""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "score", "type": "float"}, {"name": "failure_reason", "type": "string"}],
    },
    "ReflectionInput": {
        "description": """Input to the self-reflection agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "trajectory", "type": "list<string>"}, {"name": "answer", "type": "string"}, {"name": "failure_reason", "type": "string"}, {"name": "prior_reflections", "type": "list<string>"}, {"name": "trial", "type": "integer"}],
    },
    "ReflectionOutput": {
        "description": """Output from the self-reflection agent""",
        "fields": [{"name": "reflection", "type": "string"}, {"name": "suggested_strategy", "type": "string"}],
    },
    "EpisodeEntry": {
        "description": """A single trial episode stored in episodic memory""",
        "fields": [{"name": "trial_number", "type": "integer"}, {"name": "answer", "type": "string"}, {"name": "score", "type": "float"}, {"name": "failure_reason", "type": "string"}, {"name": "reflection", "type": "string"}, {"name": "suggested_strategy", "type": "string"}],
    },
    "ErrorInfo": {
        "description": """Error information passed to the error handler""",
        "fields": [{"name": "error_type", "type": "string"}, {"name": "error_message", "type": "string"}, {"name": "retry_count", "type": "integer"}],
    },
    "ReflexionResult": {
        "description": """Final result of the Reflexion process""",
        "fields": [{"name": "final_answer", "type": "string"}, {"name": "total_trials", "type": "integer"}, {"name": "final_score", "type": "float"}, {"name": "outcome", "type": "enum[success, failure_max_trials]"}, {"name": "reflections", "type": "list<string>"}, {"name": "episodes", "type": "list<EpisodeEntry>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Reflexion"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    answer: str
    attempt_task_result: Any
    episodes: list
    episodic_memory: list
    error_message: str
    error_type: str
    evaluate_attempt_result: Any
    failure_reason: str
    final_answer: str
    final_score: Any
    generate_reflection_result: Any
    max_trials: Any
    outcome: str
    prior_reflections: list
    reflection: str
    reflections: list
    retry_count: int
    score: Any
    succeeded: Any
    success: bool
    success_criteria: str
    suggested_strategy: str
    task: str
    total_trials: int
    trajectory: list
    trial: int
    trial_number: int


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_episodic_memory:
    """Episodic Memory (queue)"""
    def __init__(self):
        self.queue = []

    def read(self, key=None):
        return self.queue[0] if self.queue else None

    def write(self, value, key=None):
        self.queue.append(value)


# Store instances
_store_episodic_memory = Store_episodic_memory()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_actor_agent(user_message, output_schema=None):
    """Actor Agent"""
    system = """You are an Actor agent. You receive a task and a list of prior self-reflections
from previous failed attempts. Use those reflections to avoid repeating mistakes.
Produce an action trajectory: a sequence of reasoning steps and a final answer.
Output JSON with "trajectory" (list of step strings) and "answer" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Actor Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_evaluator_agent(user_message, output_schema=None):
    """Evaluator Agent"""
    system = """You are an Evaluator. Given a task description, the expected outcome criteria,
and the actor's answer, determine whether the attempt succeeded.
Output JSON with "success" (boolean), "score" (float 0-1),
and "failure_reason" (string, empty if success).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Evaluator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_self_reflection_agent(user_message, output_schema=None):
    """Self-Reflection Agent"""
    system = """You are a Self-Reflection agent. Given a failed attempt's trajectory, the task,
the evaluator's failure reason, and all prior reflections, generate a concise
verbal self-reflection. Identify what went wrong, why, and what strategy the
actor should use next time. Be specific and actionable.
Output JSON with "reflection" (string) and "suggested_strategy" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Self-Reflection Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_task(state: AgentState) -> dict:
    """
    Receive Task
    Accept the task and success criteria, initialize trial state
    """
    print(f"  → Receive Task")
    updates = {}

    # Logic from spec
    updates["trial"] = 0
    updates["max_trials"] = 5
    updates["reflections"] = []
    updates["episodes"] = []
    updates["succeeded"] = False
    print(f"    Task: {state.get('task', '')[:100]}")
    print(f"    Max trials: {state.get('max_trials', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_attempt_task(state: AgentState) -> dict:
    """
    Attempt Task
    Invoke the actor agent with the task and accumulated reflections
    """
    print(f"  → Attempt Task")
    updates = {}

    # Read: Read prior episodes
    episodic_memory_data = _store_episodic_memory.read()
    updates["episodic_memory"] = episodic_memory_data

    # Logic from spec
    trial = state.get("trial", 0) + 1
    updates["trial"] = trial
    reflections = state.get("reflections", [])
    print(f"    Trial {trial}/{state.get('max_trials', 5)} with {len(reflections)} prior reflections")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Execute task attempt
    _cur = dict(state)
    _cur.update(updates)
    actor_agent_input = build_input(_cur, "ActorInput")
    actor_agent_msg = json.dumps(actor_agent_input, default=str)
    actor_agent_raw = invoke_actor_agent(actor_agent_msg, output_schema="ActorOutput")
    actor_agent_result = parse_response(actor_agent_raw, "ActorOutput")
    updates["_schema_violations"] = len(validate_output(actor_agent_result, "ActorOutput"))
    updates.update(actor_agent_result)
    updates["actor_output"] = actor_agent_result
    updates["attempt_task_result"] = actor_agent_result
    print(f"    ← Actor Agent: {actor_agent_result}")

    return updates


def node_evaluate_attempt(state: AgentState) -> dict:
    """
    Evaluate Attempt
    Invoke the evaluator agent to judge whether the attempt succeeded
    """
    print(f"  → Evaluate Attempt")
    updates = {}

    # Logic from spec
    answer = state.get("answer", "")
    print(f"    Evaluating answer ({len(answer)} chars)...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Judge attempt
    _cur = dict(state)
    _cur.update(updates)
    evaluator_agent_input = build_input(_cur, "EvaluatorInput")
    evaluator_agent_msg = json.dumps(evaluator_agent_input, default=str)
    evaluator_agent_raw = invoke_evaluator_agent(evaluator_agent_msg, output_schema="EvaluatorOutput")
    evaluator_agent_result = parse_response(evaluator_agent_raw, "EvaluatorOutput")
    updates["_schema_violations"] = len(validate_output(evaluator_agent_result, "EvaluatorOutput"))
    updates.update(evaluator_agent_result)
    updates["evaluator_output"] = evaluator_agent_result
    updates["evaluate_attempt_result"] = evaluator_agent_result
    print(f"    ← Evaluator Agent: {evaluator_agent_result}")

    return updates


def node_generate_reflection(state: AgentState) -> dict:
    """
    Generate Self-Reflection
    Invoke the self-reflection agent to produce verbal reflection on the failure
    """
    print(f"  → Generate Self-Reflection")
    updates = {}

    # Logic from spec
    trial = state.get("trial", 0)
    reason = state.get("failure_reason", "unknown")
    print(f"    Reflecting on trial {trial} failure: {reason[:80]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate self-reflection
    _cur = dict(state)
    _cur.update(updates)
    self_reflection_agent_input = build_input(_cur, "ReflectionInput")
    self_reflection_agent_msg = json.dumps(self_reflection_agent_input, default=str)
    self_reflection_agent_raw = invoke_self_reflection_agent(self_reflection_agent_msg, output_schema="ReflectionOutput")
    self_reflection_agent_result = parse_response(self_reflection_agent_raw, "ReflectionOutput")
    updates["_schema_violations"] = len(validate_output(self_reflection_agent_result, "ReflectionOutput"))
    updates.update(self_reflection_agent_result)
    updates["reflection_output"] = self_reflection_agent_result
    updates["generate_reflection_result"] = self_reflection_agent_result
    print(f"    ← Self-Reflection Agent: {self_reflection_agent_result}")

    return updates


def node_store_reflection(state: AgentState) -> dict:
    """
    Store Reflection
    Append the reflection to episodic memory and prepare for retry
    """
    print(f"  → Store Reflection")
    updates = {}

    # Logic from spec
    reflection = state.get("reflection", "")
    strategy = state.get("suggested_strategy", "")
    trial = state.get("trial", 0)
    # Accumulate reflections
    reflections = state.get("reflections", [])
    reflections.append(reflection)
    updates["reflections"] = reflections
    # Build episode entry
    episode = {
        "trial_number": trial,
        "answer": state.get("answer", ""),
        "score": state.get("score", 0.0),
        "failure_reason": state.get("failure_reason", ""),
        "reflection": reflection,
        "suggested_strategy": strategy,
    }
    episodes = state.get("episodes", [])
    episodes.append(episode)
    updates["episodes"] = episodes
    print(f"    Stored reflection #{len(reflections)}: {reflection[:80]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Persist episode
    _cur = dict(state)
    _cur.update(updates)
    episodic_memory_write = build_input(_cur, "EpisodeEntry")
    _store_episodic_memory.write(episodic_memory_write)

    return updates


def node_actor_error_handler(state: AgentState) -> dict:
    """
    Actor Error Handler
    """
    print(f"  → Actor Error Handler")
    updates = {}

    # Error handler
    print("    [ERROR_HANDLER] handling error")
    return updates


def node_handle_actor_error(state: AgentState) -> dict:
    """
    Handle Actor Error
    Process errors from the actor agent invocation
    """
    print(f"  → Handle Actor Error")
    updates = {}

    # Logic from spec
    error = state.get("error_message", "Unknown error")
    updates["failure_reason"] = f"Actor execution error: {error}"
    updates["success"] = False
    updates["score"] = 0.0
    updates["answer"] = ""
    updates["trajectory"] = []
    print(f"    Actor error caught: {error[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_finalize_success(state: AgentState) -> dict:
    """
    Finalize (Success)
    Return the successful answer with trial metadata
    """
    print(f"  → Finalize (Success)")
    updates = {}

    # Logic from spec
    trial = state.get("trial", 0)
    score = state.get("score", 0.0)
    print(f"    Success on trial {trial} with score {score}")
    updates["final_answer"] = state.get("answer", "")
    updates["total_trials"] = trial
    updates["final_score"] = score
    updates["outcome"] = "success"
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_finalize_failure(state: AgentState) -> dict:
    """
    Finalize (Failure)
    Return the best attempt after exhausting all trials
    """
    print(f"  → Finalize (Failure)")
    updates = {}

    # Logic from spec
    trial = state.get("trial", 0)
    score = state.get("score", 0.0)
    reflections = state.get("reflections", [])
    print(f"    All {trial} trials exhausted. Best score: {score}")
    print(f"    Total reflections generated: {len(reflections)}")
    updates["final_answer"] = state.get("answer", "")
    updates["total_trials"] = trial
    updates["final_score"] = score
    updates["outcome"] = "failure_max_trials"
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_success(state: AgentState) -> str:
    """Gate: Succeeded? — success == True"""
    if state.get("success") == True:
        print(f"    → success is True")
        return "finalize_success"
    else:
        print(f"    → success is False")
        return "check_trials_remaining"


def route_check_trials_remaining(state: AgentState) -> str:
    """Gate: Trials remaining? — trial < max_trials"""
    if (state.get("trial", 0)) < (state.get("max_trials", 0)):
        print(f"    → trials remaining")
        return "generate_reflection"
    else:
        print(f"    → trials exhausted")
        return "finalize_failure"


def route_loop_store_reflection(state: AgentState) -> str:
    """Loop: Reflexion retry loop — trial < max_trials"""
    if state.get("_done"):
        return "END"
    if (state.get("trial", 0)) < (state.get("max_trials", 0)):
        return "attempt_task"
    else:
        return "END"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_task", node_receive_task)
    graph.add_node("attempt_task", node_attempt_task)
    graph.add_node("evaluate_attempt", node_evaluate_attempt)
    graph.add_node("generate_reflection", node_generate_reflection)
    graph.add_node("store_reflection", node_store_reflection)
    graph.add_node("actor_error_handler", node_actor_error_handler)
    graph.add_node("handle_actor_error", node_handle_actor_error)
    graph.add_node("finalize_success", node_finalize_success)
    graph.add_node("finalize_failure", node_finalize_failure)

    graph.set_entry_point("receive_task")

    graph.add_edge("receive_task", "attempt_task")
    graph.add_edge("attempt_task", "evaluate_attempt")
    graph.add_conditional_edges(
        "evaluate_attempt",
        route_check_success,
        {
            "finalize_success": "finalize_success",
            "check_trials_remaining": "check_trials_remaining",
        }
    )
    graph.add_edge("generate_reflection", "store_reflection")
    graph.add_edge("store_reflection", "attempt_task")
    graph.add_edge("actor_error_handler", "handle_actor_error")
    graph.add_conditional_edges(
        "handle_actor_error",
        route_check_trials_remaining,
        {
            "generate_reflection": "generate_reflection",
            "finalize_failure": "finalize_failure",
        }
    )
    graph.add_edge("finalize_success", END)
    graph.add_edge("finalize_failure", END)

    # Pass-through node for chained gate: check_trials_remaining
    graph.add_node("check_trials_remaining", lambda state: {})
    graph.add_conditional_edges(
        "check_trials_remaining",
        route_check_trials_remaining,
        {
            "generate_reflection": "generate_reflection",
            "finalize_failure": "finalize_failure",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Reflexion — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Reflexion (LangGraph)")
    print(f"  Reflexion agent that attempts tasks, self-evaluates, generates verbal reflections on failure, and retries with accumulated episodic memory until success or max trials.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    initial = {}
    initial["task"] = input("Enter task: ")
    initial["success_criteria"] = input("Enter success_criteria: ")
    run(initial)