#!/usr/bin/env python3
"""
Meta-Prompting — Generated by OpenClaw Instantiation Engine (LangGraph backend)
Spec: Meta-Prompting architecture: a meta-agent dynamically generates specialized sub-prompts, delegates to spawned worker agents, and uses a negotiation protocol to reconcile their outputs into a final answer.
"""

import json
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "MetaAgentInput": {
        "description": """Input to the meta-agent for task analysis""",
        "fields": [{"name": "task", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "context", "type": "string"}],
    },
    "TaskDecomposition": {
        "description": """Meta-agent's task decomposition with generated prompts""",
        "fields": [{"name": "analysis", "type": "string"}, {"name": "sub_tasks", "type": "list<SubTaskSpec>"}],
    },
    "SubTaskSpec": {
        "description": """Specification for a single sub-task""",
        "fields": [{"name": "sub_task_id", "type": "string"}, {"name": "description", "type": "string"}, {"name": "generated_prompt", "type": "string"}, {"name": "required_expertise", "type": "string"}, {"name": "is_complex", "type": "boolean"}, {"name": "depends_on", "type": "list<string>"}],
    },
    "WorkerInput": {
        "description": """Input to a worker agent""",
        "fields": [{"name": "sub_task_description", "type": "string"}, {"name": "generated_prompt", "type": "string"}, {"name": "dependency_context", "type": "list<string>"}, {"name": "original_task", "type": "string"}],
    },
    "WorkerOutput": {
        "description": """Output from a worker agent""",
        "fields": [{"name": "result", "type": "string"}, {"name": "confidence", "type": "float"}, {"name": "needs_clarification", "type": "boolean"}],
    },
    "IntegratorInput": {
        "description": """Input to the integrator agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "worker_results", "type": "list<WorkerResultEntry>"}, {"name": "conflicts_detected", "type": "list<string>"}],
    },
    "WorkerResultEntry": {
        "description": """A single worker's result with metadata""",
        "fields": [{"name": "sub_task_id", "type": "string"}, {"name": "result", "type": "string"}, {"name": "confidence", "type": "float"}, {"name": "expertise", "type": "string"}],
    },
    "IntegratorOutput": {
        "description": """Output from the integrator agent""",
        "fields": [{"name": "integrated_answer", "type": "string"}, {"name": "completeness_score", "type": "integer"}, {"name": "conflicts_resolved", "type": "list<string>"}, {"name": "gaps_identified", "type": "list<string>"}],
    },
    "TaskRegistryEntry": {
        "description": """Entry in the task registry tracking sub-task status""",
        "fields": [{"name": "sub_task_id", "type": "string"}, {"name": "status", "type": "enum[pending, in_progress, completed, failed]"}, {"name": "result", "type": "string"}, {"name": "assigned_prompt", "type": "string"}],
    },
    "GeneratedPrompt": {
        "description": """A dynamically generated prompt stored in the prompt library""",
        "fields": [{"name": "sub_task_id", "type": "string"}, {"name": "prompt_text", "type": "string"}, {"name": "required_expertise", "type": "string"}, {"name": "depth", "type": "integer"}],
    },
    "MetaPromptingResult": {
        "description": """Final result from the meta-prompting process""",
        "fields": [{"name": "final_answer", "type": "string"}, {"name": "final_completeness", "type": "integer"}, {"name": "total_workers", "type": "integer"}, {"name": "total_prompts_generated", "type": "integer"}, {"name": "worker_results", "type": "list<WorkerResultEntry>"}, {"name": "generated_prompts", "type": "list<GeneratedPrompt>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Meta-Prompting"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: int
    analysis: str
    assigned_prompt: str
    completed_sub_tasks: Any
    completeness_score: int
    confidence: Any
    conflicts_detected: list
    conflicts_resolved: list
    context: str
    current_sub_task: Any
    current_sub_task_idx: Any
    dependency_context: list
    depends_on: list
    depth: int
    description: str
    expertise: str
    final_answer: str
    final_completeness: int
    gaps_identified: list
    generated_prompt: str
    generated_prompts: list
    integrated_answer: str
    is_complex: bool
    len: Any
    max_depth: Any
    needs_clarification: bool
    original_task: str
    pending_sub_tasks: Any
    prompt_library: dict
    prompt_text: str
    required_expertise: str
    result: str
    status: str
    sub_task_description: str
    sub_task_id: str
    sub_tasks: list
    task: str
    task_registry: dict
    total_prompts_generated: int
    total_workers: int
    worker_description: Any
    worker_expertise: Any
    worker_prompt: Any
    worker_results: list


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_task_registry:
    """Task Registry (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_prompt_library:
    """Prompt Library (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_task_registry = Store_task_registry()
_store_prompt_library = Store_prompt_library()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_meta_agent(user_message, output_schema=None):
    """Meta Agent"""
    system = """You are a Meta-Prompting orchestrator. Given a complex task, your job is to:
1. Analyze the task to identify what specialized expertise is needed.
2. Decompose it into sub-tasks, each requiring a distinct skill.
3. For each sub-task, generate a precise, specialized system prompt
   that will be used to configure a worker agent.
4. Specify whether any sub-task requires recursive decomposition
   (is_complex = true if the sub-task itself needs further breakdown).

Output JSON with "analysis" (string), "sub_tasks" (list of objects, each with
"sub_task_id" (string), "description" (string), "generated_prompt" (string),
"required_expertise" (string), "is_complex" (boolean), and
"depends_on" (list of sub_task_id strings)).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Meta Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_worker_agent(user_message, output_schema=None):
    """Dynamic Worker Agent"""
    system = """You are a specialized worker agent. Your system prompt has been dynamically
generated by a meta-agent to match the specific expertise needed for this
sub-task. Follow the specialized instructions precisely.
Output JSON with "result" (string), "confidence" (float 0-1), and
"needs_clarification" (boolean, true if the sub-task is ambiguous).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Dynamic Worker Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_integrator_agent(user_message, output_schema=None):
    """Integrator Agent"""
    system = """You are an integration agent. You receive results from multiple specialized
worker agents that each handled a sub-task. Your job is to:
1. Verify consistency across worker outputs.
2. Resolve conflicts or contradictions.
3. Integrate all sub-task results into a single coherent answer.
4. Assign a completeness score (1-10) indicating how well all sub-tasks
   were addressed.
Output JSON with "integrated_answer" (string), "completeness_score" (integer 1-10),
"conflicts_resolved" (list of strings), and "gaps_identified" (list of strings).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Integrator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_task(state: AgentState) -> dict:
    """
    Receive Task
    Accept user task and initialize meta-prompting state
    """
    print(f"  → Receive Task")
    updates = {}

    # Logic from spec
    updates["depth"] = 0
    updates["max_depth"] = 3
    updates["completed_sub_tasks"] = {}
    updates["pending_sub_tasks"] = []
    updates["generated_prompts"] = {}
    updates["worker_results"] = []
    print(f"    Task: {state.get('task', '')[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_decompose_task(state: AgentState) -> dict:
    """
    Decompose Task
    Invoke the meta-agent to analyze and decompose the task into sub-tasks with generated prompts
    """
    print(f"  → Decompose Task")
    updates = {}

    # Logic from spec
    depth = state.get("depth", 0)
    print(f"    Decomposing at depth {depth}...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Analyze and decompose task
    _cur = dict(state)
    _cur.update(updates)
    meta_agent_input = build_input(_cur, "MetaAgentInput")
    meta_agent_msg = json.dumps(meta_agent_input, default=str)
    meta_agent_raw = invoke_meta_agent(meta_agent_msg, output_schema="TaskDecomposition")
    meta_agent_result = parse_response(meta_agent_raw, "TaskDecomposition")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(meta_agent_result, "TaskDecomposition"))
    updates.update(meta_agent_result)
    updates["task_decomposition"] = meta_agent_result
    updates["decompose_task_result"] = meta_agent_result
    print(f"    ← Meta Agent: {meta_agent_result}")

    return updates


def node_register_sub_tasks(state: AgentState) -> dict:
    """
    Register Sub-Tasks
    Store generated sub-tasks and their prompts, determine execution order
    """
    print(f"  → Register Sub-Tasks")
    updates = {}

    # Logic from spec
    sub_tasks = state.get("sub_tasks", [])
    prompts = state.get("generated_prompts", {})
    for st in sub_tasks:
        st_id = st.get("sub_task_id", "")
        prompts[st_id] = st.get("generated_prompt", "")
    updates["generated_prompts"] = prompts
    # Build execution queue respecting dependencies
    pending = [st for st in sub_tasks if st.get("sub_task_id") not in state.get("completed_sub_tasks", {})]
    updates["pending_sub_tasks"] = pending
    updates["current_sub_task_idx"] = 0
    print(f"    Registered {len(pending)} sub-tasks with generated prompts")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Store generated prompts
    _cur = dict(state)
    _cur.update(updates)
    prompt_library_write = build_input(_cur, "GeneratedPrompt")
    _store_prompt_library.write(prompt_library_write)

    # Write: Register sub-tasks
    _cur = dict(state)
    _cur.update(updates)
    task_registry_write = build_input(_cur, "TaskRegistryEntry")
    _store_task_registry.write(task_registry_write)

    return updates


def node_prepare_worker(state: AgentState) -> dict:
    """
    Prepare Worker
    Configure the next worker agent with the dynamically generated prompt
    """
    print(f"  → Prepare Worker")
    updates = {}

    # Read: Read dependency results
    task_registry_data = _store_task_registry.read()
    updates["task_registry"] = task_registry_data

    # Logic from spec
    idx = state.get("current_sub_task_idx", 0)
    pending = state.get("pending_sub_tasks", [])
    if idx < len(pending):
        sub_task = pending[idx]
        updates["current_sub_task"] = sub_task
        updates["worker_prompt"] = sub_task.get("generated_prompt", "")
        updates["worker_description"] = sub_task.get("description", "")
        updates["worker_expertise"] = sub_task.get("required_expertise", "")
        updates["is_complex"] = sub_task.get("is_complex", False)
        # Resolve dependencies
        depends = sub_task.get("depends_on", [])
        completed = state.get("completed_sub_tasks", {})
        updates["dependency_context"] = [completed.get(d, "") for d in depends if d in completed]
        print(f"    Preparing worker for: {sub_task.get('sub_task_id', '')} ({sub_task.get('required_expertise', '')})")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_recurse_decomposition(state: AgentState) -> dict:
    """
    Recursive Decomposition
    Recursively decompose a complex sub-task by spawning a new meta-prompting cycle
    """
    print(f"  → Recursive Decomposition")
    updates = {}

    # Spawn: template=meta_agent
    print("    [SPAWN] Would create sub-agent from template: meta_agent")
    return updates


def node_execute_worker(state: AgentState) -> dict:
    """
    Execute Worker
    Invoke the worker agent with the dynamically generated prompt and sub-task
    """
    print(f"  → Execute Worker")
    updates = {}

    # Logic from spec
    expertise = state.get("worker_expertise", "general")
    description = state.get("worker_description", "")
    print(f"    Executing worker [{expertise}]: {description[:80]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Execute with generated prompt
    _cur = dict(state)
    _cur.update(updates)
    worker_agent_input = build_input(_cur, "WorkerInput")
    worker_agent_msg = json.dumps(worker_agent_input, default=str)
    worker_agent_raw = invoke_worker_agent(worker_agent_msg, output_schema="WorkerOutput")
    worker_agent_result = parse_response(worker_agent_raw, "WorkerOutput")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(worker_agent_result, "WorkerOutput"))
    updates.update(worker_agent_result)
    updates["worker_output"] = worker_agent_result
    updates["execute_worker_result"] = worker_agent_result
    print(f"    ← Dynamic Worker Agent: {worker_agent_result}")

    return updates


def node_collect_worker_result(state: AgentState) -> dict:
    """
    Collect Worker Result
    Store worker result and advance to the next sub-task
    """
    print(f"  → Collect Worker Result")
    updates = {}

    # Logic from spec
    idx = state.get("current_sub_task_idx", 0)
    sub_task = state.get("current_sub_task", {})
    st_id = sub_task.get("sub_task_id", f"sub_{idx}")
    result = state.get("result", "")
    confidence = state.get("confidence", 0.0)
    # Store result
    completed = state.get("completed_sub_tasks", {})
    completed[st_id] = result
    updates["completed_sub_tasks"] = completed
    # Track for integration
    worker_results = state.get("worker_results", [])
    worker_results.append({
        "sub_task_id": st_id,
        "result": result,
        "confidence": confidence,
        "expertise": state.get("worker_expertise", ""),
    })
    updates["worker_results"] = worker_results
    updates["current_sub_task_idx"] = idx + 1
    print(f"    Collected result for {st_id} (confidence={confidence})")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_negotiate_conflicts(state: AgentState) -> dict:
    """
    Conflict Negotiation
    Multi-party protocol where workers with conflicting outputs negotiate resolution
    """
    print(f"  → Conflict Negotiation")
    updates = {}

    # Protocol: multi-agent negotiation
    print("    [PROTOCOL] Multi-agent negotiation step")
    return updates


def node_integrate_results(state: AgentState) -> dict:
    """
    Integrate Results
    Invoke the integrator agent to combine all worker results
    """
    print(f"  → Integrate Results")
    updates = {}

    # Logic from spec
    results = state.get("worker_results", [])
    print(f"    Integrating {len(results)} worker results...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Integrate worker results
    _cur = dict(state)
    _cur.update(updates)
    integrator_agent_input = build_input(_cur, "IntegratorInput")
    integrator_agent_msg = json.dumps(integrator_agent_input, default=str)
    integrator_agent_raw = invoke_integrator_agent(integrator_agent_msg, output_schema="IntegratorOutput")
    integrator_agent_result = parse_response(integrator_agent_raw, "IntegratorOutput")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(integrator_agent_result, "IntegratorOutput"))
    updates.update(integrator_agent_result)
    updates["integrator_output"] = integrator_agent_result
    updates["integrate_results_result"] = integrator_agent_result
    print(f"    ← Integrator Agent: {integrator_agent_result}")

    return updates


def node_handle_gaps(state: AgentState) -> dict:
    """
    Handle Gaps
    Generate additional sub-tasks to fill identified gaps
    """
    print(f"  → Handle Gaps")
    updates = {}

    # Logic from spec
    gaps = state.get("gaps_identified", [])
    depth = state.get("depth", 0) + 1
    updates["depth"] = depth
    # Convert gaps into a new task for re-decomposition
    updates["task"] = f"Fill the following gaps in the previous answer: {'; '.join(gaps)}"
    print(f"    Handling {len(gaps)} gaps at depth {depth}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_finalize_output(state: AgentState) -> dict:
    """
    Finalize Output
    Return the final integrated answer with metadata
    """
    print(f"  → Finalize Output")
    updates = {}

    # Logic from spec
    completeness = state.get("completeness_score", 0)
    worker_count = len(state.get("worker_results", []))
    depth = state.get("depth", 0)
    prompts_generated = len(state.get("generated_prompts", {}))
    print(f"    Finalizing: {worker_count} workers used, {prompts_generated} prompts generated, depth={depth}")
    updates["final_answer"] = state.get("integrated_answer", "")
    updates["final_completeness"] = completeness
    updates["total_workers"] = worker_count
    updates["total_prompts_generated"] = prompts_generated
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_pending(state: AgentState) -> str:
    """Gate: Sub-tasks remaining? — current_sub_task_idx < len(pending_sub_tasks)"""
    if (state.get("current_sub_task_idx", 0)) < (state.get("len", 0)):
        print(f"    → sub-tasks remaining")
        return "prepare_worker"
    else:
        print(f"    → all sub-tasks done")
        return "integrate_results"


def route_check_complexity(state: AgentState) -> str:
    """Gate: Needs recursive decomposition? — is_complex == True and depth < max_depth"""
    if (state.get("depth", 0)) < (state.get("max_depth", 0)):
        print(f"    → complex sub-task, recurse")
        return "recurse_decomposition"
    else:
        print(f"    → simple sub-task, execute")
        return "execute_worker"


def route_check_completeness(state: AgentState) -> str:
    """Gate: Integration complete? — completeness_score >= 7"""
    if (state.get("completeness_score", 0)) >= 7:
        print(f"    → complete enough")
        return "finalize_output"
    else:
        print(f"    → gaps identified")
        return "handle_gaps"


def route_gap_limit_check(state: AgentState) -> str:
    """Gate: Gap filling allowed? — depth < max_depth"""
    if (state.get("depth", 0)) < (state.get("max_depth", 0)):
        print(f"    → depth limit reached")
        return "finalize_output"
    else:
        print(f"    → depth under limit")
        return "decompose_task"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_task", node_receive_task)
    graph.add_node("decompose_task", node_decompose_task)
    graph.add_node("register_sub_tasks", node_register_sub_tasks)
    graph.add_node("prepare_worker", node_prepare_worker)
    graph.add_node("recurse_decomposition", node_recurse_decomposition)
    graph.add_node("execute_worker", node_execute_worker)
    graph.add_node("collect_worker_result", node_collect_worker_result)
    graph.add_node("negotiate_conflicts", node_negotiate_conflicts)
    graph.add_node("integrate_results", node_integrate_results)
    graph.add_node("handle_gaps", node_handle_gaps)
    graph.add_node("finalize_output", node_finalize_output)

    graph.set_entry_point("receive_task")

    graph.add_edge("receive_task", "decompose_task")
    graph.add_edge("decompose_task", "register_sub_tasks")
    graph.add_conditional_edges(
        "register_sub_tasks",
        route_check_pending,
        {
            "prepare_worker": "prepare_worker",
            "integrate_results": "integrate_results",
        }
    )
    graph.add_conditional_edges(
        "prepare_worker",
        route_check_complexity,
        {
            "recurse_decomposition": "recurse_decomposition",
            "execute_worker": "execute_worker",
        }
    )
    graph.add_edge("recurse_decomposition", "collect_worker_result")
    graph.add_edge("execute_worker", "collect_worker_result")
    graph.add_conditional_edges(
        "negotiate_conflicts",
        route_check_completeness,
        {
            "finalize_output": "finalize_output",
            "handle_gaps": "handle_gaps",
        }
    )
    graph.add_edge("integrate_results", "negotiate_conflicts")
    graph.add_conditional_edges(
        "handle_gaps",
        route_gap_limit_check,
        {
            "decompose_task": "decompose_task",
            "finalize_output": "finalize_output",
        }
    )
    graph.add_edge("finalize_output", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Meta-Prompting — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Meta-Prompting (LangGraph)")
    print(f"  Meta-Prompting architecture: a meta-agent dynamically generates specialized sub-prompts, delegates to spawned worker agents, and uses a negotiation protocol to reconcile their outputs into a final answer.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    initial = {}
    initial["task"] = input("Enter task: ")
    initial["depth"] = input("Enter depth: ")
    initial["context"] = input("Enter context: ")
    run(initial)