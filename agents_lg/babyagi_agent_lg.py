#!/usr/bin/env python3
"""
BabyAGI — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Task-driven autonomous agent that creates, prioritizes, and executes tasks in a loop
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=8192):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=8192, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "Task": {
        "description": """A single task in the queue""",
        "fields": [{"name": "id", "type": "integer"}, {"name": "description", "type": "string"}, {"name": "status", "type": "enum[pending, complete]"}, {"name": "priority", "type": "integer"}],
    },
    "TaskList": {
        "description": """Ordered list of tasks""",
        "fields": [{"name": "tasks", "type": "list<Task>"}, {"name": "objective", "type": "string"}],
    },
    "ExecutionInput": {
        "description": """Input to the execution agent""",
        "fields": [{"name": "task", "type": "Task"}, {"name": "objective", "type": "string"}, {"name": "context", "type": "list<string>"}],
    },
    "ExecutionOutput": {
        "description": """Result of task execution""",
        "fields": [{"name": "result", "type": "string"}, {"name": "task_id", "type": "integer"}],
    },
    "ContextQuery": {
        "description": """Query to the context agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "top_k", "type": "integer"}, {"name": "stored_results", "type": "list<string>"}],
    },
    "ContextResult": {
        "description": """Retrieved context""",
        "fields": [{"name": "context", "type": "list<string>"}, {"name": "sources", "type": "list<string>"}],
    },
    "EnrichedResult": {
        "description": """Execution result enriched with context""",
        "fields": [{"name": "result", "type": "string"}, {"name": "context", "type": "list<string>"}, {"name": "task_id", "type": "integer"}],
    },
    "EmbeddedResult": {
        "description": """Result stored in vector DB""",
        "fields": [{"name": "text", "type": "string"}, {"name": "embedding", "type": "list<float>"}, {"name": "metadata", "type": "object"}],
    },
    "TaskCreationInput": {
        "description": """Input to the task creation agent""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "result", "type": "string"}, {"name": "existing_tasks", "type": "list<Task>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for BabyAGI"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    completed_count: Any
    context: list
    create_and_reprioritize_result: Any
    description: str
    embedding: list
    enrich_result: Any
    existing_tasks: list
    id: int
    max_tasks: Any
    metadata: Any
    objective: str
    priority: int
    pull_task_result: Any
    query: str
    result: str
    sources: list
    status: str
    stored_results: list
    task: Any
    task_id: int
    tasks: list
    text: str
    top_k: int
    vector_db: list


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    def __init__(self, name, embedding_model=None, embedding_provider=None):
        self._fallback = []
        self._collection = None
        self._embedding_model = embedding_model
        self._embedding_provider = embedding_provider
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
        else:
            print(f"    [VectorStore] using in-memory fallback")

    def _embed(self, texts):
        if not self._embedding_model:
            return None
        try:
            return call_embedding(texts, model=self._embedding_model, provider=self._embedding_provider or "openai")
        except Exception:
            return None

    def read(self, query=None, top_k=5):
        if self._collection is not None:
            if query:
                kwargs = {"n_results": min(top_k, max(self._collection.count(), 1))}
                emb = self._embed([query])
                if emb:
                    kwargs["query_embeddings"] = emb
                else:
                    kwargs["query_texts"] = [query]
                results = self._collection.query(**kwargs)
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            kwargs = {"documents": [text], "ids": [doc_id]}
            if clean_meta:
                kwargs["metadatas"] = [clean_meta]
            emb = self._embed([text])
            if emb:
                kwargs["embeddings"] = emb
            self._collection.add(**kwargs)
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})


class Store_vector_db:
    """Vector DB (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("vector_db")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)


# Store instances
_store_vector_db = Store_vector_db()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_execution_agent(user_message, output_schema=None):
    """Execution Agent"""
    system = """You are an AI who performs one task. You will receive a JSON object with 'task' (the task to perform), 'objective' (the overall goal), and 'context' (relevant prior results). Perform the task and return your result."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Execution Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_context_agent(user_message, output_schema=None):
    """Context Agent"""
    system = """You retrieve relevant context for a given query from stored results. You will receive a query, top_k count, and stored_results (a list of text strings from previous task executions). Select and return the top_k most relevant stored_results as the 'context' array. For 'sources', use short labels like 'task_result_1'. If stored_results is empty, return empty arrays for both context and sources."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Context Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_task_creation_agent(user_message, output_schema=None):
    """Task Creation Agent"""
    system = """You create new tasks based on the result of an execution agent. You will receive the objective, the latest execution result, and a list of existing incomplete tasks. Return a TaskList containing ALL existing incomplete tasks PLUS any new tasks needed to achieve the objective. Assign incrementing integer IDs to new tasks (starting after the highest existing ID). Each task needs: id (integer), description (string), status ('pending'), priority (integer, 1=highest). Do NOT drop any existing tasks from the list."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Task Creation Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_prioritization_agent(user_message, output_schema=None):
    """Prioritization Agent"""
    system = """You reprioritize a task list based on the objective."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Prioritization Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_pull_task(state: AgentState) -> dict:
    """
    Step 1: Pull task
    Dequeue the highest-priority incomplete task from the task list
    """
    print(f"  → Step 1: Pull task")
    updates = {}

    # Read: Read prior results
    vector_db_data = _store_vector_db.read()
    updates["vector_db"] = vector_db_data

    # Logic from spec
    tasks = state.get("tasks", [])
    if not tasks:
        print("    No tasks remaining!")
        updates["_done"] = True
        return state
    completed = state.get("completed_count", 0)
    max_tasks = state.get("max_tasks", 5)
    if completed >= max_tasks:
        print(f"    Completed {completed} tasks (max {max_tasks}). Stopping.")
        updates["_done"] = True
        return state
    task = tasks.pop(0)
    updates["task"] = task
    updates["tasks"] = tasks
    updates["completed_count"] = completed + 1
    updates["context"] = [e.get("text", "") for e in state.get("vector_db", []) if e.get("text")][-5:]
    print(f"    Pulled task {completed + 1}/{max_tasks}: {task.get('description', task)}")
    print(f"    Context from prior results: {len(state.get('context', ""))} items")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Execute task
    _cur = dict(state)
    _cur.update(updates)
    execution_agent_input = build_input(_cur, "ExecutionInput")
    execution_agent_msg = json.dumps(execution_agent_input, default=str)
    execution_agent_raw = invoke_execution_agent(execution_agent_msg, output_schema="ExecutionOutput")
    execution_agent_result = parse_response(execution_agent_raw, "ExecutionOutput")
    updates["_schema_violations"] = len(validate_output(execution_agent_result, "ExecutionOutput"))
    updates.update(execution_agent_result)
    updates["execution_output"] = execution_agent_result
    updates["pull_task_result"] = execution_agent_result
    print(f"    ← Execution Agent: {execution_agent_result}")

    return updates


def node_enrich(state: AgentState) -> dict:
    """
    Step 2: Enrich and store
    Take execution result, retrieve relevant context, enrich the result, and persist to vector store
    """
    print(f"  → Step 2: Enrich and store")
    updates = {}

    # Read: Read stored results
    vector_db_data = _store_vector_db.read()
    updates["vector_db"] = vector_db_data

    # Logic from spec
    updates["query"] = state.get("result", "")
    updates["top_k"] = 5
    updates["stored_results"] = [e.get("text", "") for e in state.get("vector_db", []) if e.get("text")]
    updates["text"] = state.get("result", "")
    updates["embedding"] = []
    updates["metadata"] = {"task_id": state.get("task_id"), "objective": state.get("objective", "")}
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Retrieve context
    _cur = dict(state)
    _cur.update(updates)
    context_agent_input = build_input(_cur, "ContextQuery")
    context_agent_msg = json.dumps(context_agent_input, default=str)
    context_agent_raw = invoke_context_agent(context_agent_msg, output_schema="ContextResult")
    context_agent_result = parse_response(context_agent_raw, "ContextResult")
    updates["_schema_violations"] = len(validate_output(context_agent_result, "ContextResult"))
    updates.update(context_agent_result)
    updates["context_result"] = context_agent_result
    updates["enrich_result"] = context_agent_result
    print(f"    ← Context Agent: {context_agent_result}")

    # Write: Store result in Vector DB
    _cur = dict(state)
    _cur.update(updates)
    vector_db_write = build_input(_cur, "EmbeddedResult")
    _store_vector_db.write(vector_db_write)

    return updates


def node_create_and_reprioritize(state: AgentState) -> dict:
    """
    Step 3: Create and reprioritize
    Based on the enriched result, generate new tasks and reprioritize the full task list
    """
    print(f"  → Step 3: Create and reprioritize")
    updates = {}

    # Logic from spec
    updates["existing_tasks"] = state.get("tasks", [])
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Create new tasks
    _cur = dict(state)
    _cur.update(updates)
    task_creation_agent_input = build_input(_cur, "TaskCreationInput")
    task_creation_agent_msg = json.dumps(task_creation_agent_input, default=str)
    task_creation_agent_raw = invoke_task_creation_agent(task_creation_agent_msg, output_schema="TaskList")
    task_creation_agent_result = parse_response(task_creation_agent_raw, "TaskList")
    updates["_schema_violations"] = len(validate_output(task_creation_agent_result, "TaskList"))
    updates.update(task_creation_agent_result)
    updates["task_list"] = task_creation_agent_result
    updates["create_and_reprioritize_result"] = task_creation_agent_result
    print(f"    ← Task Creation Agent: {task_creation_agent_result}")

    # Invoke: Reprioritize task list
    _cur = dict(state)
    _cur.update(updates)
    prioritization_agent_input = build_input(_cur, "TaskList")
    prioritization_agent_msg = json.dumps(prioritization_agent_input, default=str)
    prioritization_agent_raw = invoke_prioritization_agent(prioritization_agent_msg, output_schema="TaskList")
    prioritization_agent_result = parse_response(prioritization_agent_raw, "TaskList")
    updates["_schema_violations"] = len(validate_output(prioritization_agent_result, "TaskList"))
    updates.update(prioritization_agent_result)
    updates["task_list"] = prioritization_agent_result
    updates["create_and_reprioritize_result"] = prioritization_agent_result
    print(f"    ← Prioritization Agent: {prioritization_agent_result}")

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_loop_create_and_reprioritize(state: AgentState) -> str:
    """Loop: Loop — tasks is not empty"""
    if state.get("_done"):
        return "END"
    if bool(state.get("tasks")):
        return "pull_task"
    else:
        return "END"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("pull_task", node_pull_task)
    graph.add_node("enrich", node_enrich)
    graph.add_node("create_and_reprioritize", node_create_and_reprioritize)

    graph.set_entry_point("pull_task")

    graph.add_edge("pull_task", "enrich")
    graph.add_edge("enrich", "create_and_reprioritize")
    graph.add_conditional_edges(
        "create_and_reprioritize",
        route_loop_create_and_reprioritize,
        {
            "pull_task": "pull_task",
            "END": END,
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """BabyAGI — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  BabyAGI (LangGraph)")
    print(f"  Task-driven autonomous agent that creates, prioritizes, and executes tasks in a loop")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()