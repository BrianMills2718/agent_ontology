#!/usr/bin/env python3
"""
RAP (Reasoning via Planning) — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: RAP agent: LLM as world model + MCTS search for multi-step reasoning. The world model predicts state transitions; MCTS searches over the resulting tree to find optimal reasoning paths. Based on Hao et al. 2023.

"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "WorldModelInput": {
        "description": """""",
        "fields": [{"name": "current_state", "type": "string"}, {"name": "action", "type": "string"}, {"name": "goal", "type": "string"}],
    },
    "WorldModelOutput": {
        "description": """""",
        "fields": [{"name": "next_state", "type": "string"}, {"name": "reward", "type": "float"}, {"name": "is_terminal", "type": "boolean"}],
    },
    "ActionInput": {
        "description": """""",
        "fields": [{"name": "current_state", "type": "string"}, {"name": "goal", "type": "string"}],
    },
    "ActionOutput": {
        "description": """""",
        "fields": [{"name": "actions", "type": "list<string>"}],
    },
    "ReasoningState": {
        "description": """""",
        "fields": [{"name": "state_text", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "value", "type": "float"}, {"name": "visits", "type": "integer"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for RAP (Reasoning via Planning)"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    action: str
    actions: list
    answer: Any
    best_path: Any
    best_reward: Any
    current_path: Any
    current_state: str
    depth: int
    evaluate_state_result: Any
    goal: str
    is_terminal: bool
    iteration: Any
    max_iterations: Any
    next_state: str
    problem: Any
    propose_actions_result: Any
    query: Any
    reasoning_path: Any
    reward: Any
    search_tree: dict
    state_text: str
    value: Any
    visits: int


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_search_tree:
    """MCTS Search Tree (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_search_tree = Store_search_tree()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_world_model_agent(user_message, output_schema=None):
    """World Model (LLM)"""
    system = """You are a world model. Given a current state and a proposed action, predict the next state. Also evaluate how promising this state is for reaching the goal (reward 0-10). Output JSON: {"next_state": "...", "reward": N, "is_terminal": true/false}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("World Model (LLM)", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_action_proposer(user_message, output_schema=None):
    """Action Proposer"""
    system = """Given the current reasoning state and the goal, propose 2-3 possible next actions (reasoning steps). Each action should be a concrete thought or operation that advances toward the goal. Output JSON: {"actions": ["action1", "action2", ...]}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Action Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_problem(state: AgentState) -> dict:
    """
    Receive Problem
    """
    print(f"  → Receive Problem")
    updates = {}

    # Logic from spec
    updates["goal"] = state.get("problem", state.get("query", ""))
    updates["current_state"] = "Initial state: " + state.get("goal", "")
    updates["best_path"] = []
    updates["best_reward"] = 0
    updates["iteration"] = 0
    updates["max_iterations"] = state.get("max_iterations", 5)
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_mcts_search(state: AgentState) -> dict:
    """
    MCTS Search
    """
    print(f"  → MCTS Search")
    updates = {}

    return updates


def node_propose_actions(state: AgentState) -> dict:
    """
    Propose Actions
    """
    print(f"  → Propose Actions")
    updates = {}

    # Invoke: action_proposer
    _cur = dict(state)
    _cur.update(updates)
    action_proposer_input = build_input(_cur, "ActionInput")
    action_proposer_msg = json.dumps(action_proposer_input, default=str)
    action_proposer_raw = invoke_action_proposer(action_proposer_msg, output_schema="ActionOutput")
    action_proposer_result = parse_response(action_proposer_raw, "ActionOutput")
    updates["_schema_violations"] = len(validate_output(action_proposer_result, "ActionOutput"))
    updates.update(action_proposer_result)
    updates["action_output"] = action_proposer_result
    updates["propose_actions_result"] = action_proposer_result
    print(f"    ← Action Proposer: {action_proposer_result}")

    return updates


def node_evaluate_state(state: AgentState) -> dict:
    """
    Evaluate State (World Model)
    """
    print(f"  → Evaluate State (World Model)")
    updates = {}

    # Invoke: world_model_agent
    _cur = dict(state)
    _cur.update(updates)
    world_model_agent_input = build_input(_cur, "WorldModelInput")
    world_model_agent_msg = json.dumps(world_model_agent_input, default=str)
    world_model_agent_raw = invoke_world_model_agent(world_model_agent_msg, output_schema="WorldModelOutput")
    world_model_agent_result = parse_response(world_model_agent_raw, "WorldModelOutput")
    updates["_schema_violations"] = len(validate_output(world_model_agent_result, "WorldModelOutput"))
    updates.update(world_model_agent_result)
    updates["world_model_output"] = world_model_agent_result
    updates["evaluate_state_result"] = world_model_agent_result
    print(f"    ← World Model (LLM): {world_model_agent_result}")

    return updates


def node_update_tree(state: AgentState) -> dict:
    """
    Update Search Tree
    """
    print(f"  → Update Search Tree")
    updates = {}

    # Logic from spec
    updates["iteration"] = state.get("iteration", 0) + 1
    reward = state.get("reward", 0)
    if reward > state.get("best_reward", 0):
        updates["best_reward"] = reward
        updates["best_path"] = state.get("current_path", [])
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: 
    _store_search_tree.write(dict(state))

    return updates


def node_emit_result(state: AgentState) -> dict:
    """
    Emit Best Result
    """
    print(f"  → Emit Best Result")
    updates = {}

    # Logic from spec
    updates["answer"] = state.get("current_state", "")
    updates["reasoning_path"] = state.get("best_path", [])
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_done(state: AgentState) -> str:
    """Gate: Search Complete? — iteration >= max_iterations"""
    if (state.get("iteration", 0)) >= (state.get("max_iterations", 0)):
        print(f"    → iteration >= max_iterations")
        return "emit_result"
    else:
        print(f"    → iteration < max_iterations")
        return "propose_actions"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_problem", node_receive_problem)
    graph.add_node("mcts_search", node_mcts_search)
    graph.add_node("propose_actions", node_propose_actions)
    graph.add_node("evaluate_state", node_evaluate_state)
    graph.add_node("update_tree", node_update_tree)
    graph.add_node("emit_result", node_emit_result)

    graph.set_entry_point("receive_problem")

    graph.add_edge("receive_problem", "propose_actions")
    graph.add_edge("mcts_search", END)
    graph.add_edge("propose_actions", "evaluate_state")
    graph.add_edge("evaluate_state", "update_tree")
    graph.add_conditional_edges(
        "update_tree",
        route_check_done,
        {
            "emit_result": "emit_result",
            "propose_actions": "propose_actions",
        }
    )
    graph.add_edge("emit_result", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """RAP (Reasoning via Planning) — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  RAP (Reasoning via Planning) (LangGraph)")
    print(f"  RAP agent: LLM as world model + MCTS search for multi-step reasoning. The world model predicts state transitions; MCTS searches over the resulting tree to find optimal reasoning paths. Based on Hao et al. 2023.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()