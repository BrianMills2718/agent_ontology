#!/usr/bin/env python3
"""
Plan and Solve — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Plan-and-Solve agent that decomposes a problem into sub-problems, solves each with verification and retry logic, then synthesizes a final answer.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "PlannerInput": {
        "description": """Input to the planner agent""",
        "fields": [{"name": "problem", "type": "string"}],
    },
    "Decomposition": {
        "description": """Output from the planner: a list of sub-problems""",
        "fields": [{"name": "sub_problems", "type": "list<SubProblem>"}],
    },
    "SubProblem": {
        "description": """A single sub-problem in the decomposition""",
        "fields": [{"name": "index", "type": "integer"}, {"name": "description", "type": "string"}, {"name": "depends_on", "type": "list<integer>"}],
    },
    "SolverInput": {
        "description": """Input to the solver agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "sub_description", "type": "string"}, {"name": "prior_solutions", "type": "list<SubSolution>"}],
    },
    "SubSolution": {
        "description": """A solution to a single sub-problem""",
        "fields": [{"name": "solution_text", "type": "string"}, {"name": "confidence", "type": "integer"}],
    },
    "VerifierInput": {
        "description": """Input to the verifier agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "sub_description", "type": "string"}, {"name": "solution_text", "type": "string"}],
    },
    "VerificationResult": {
        "description": """Output from the verifier""",
        "fields": [{"name": "is_valid", "type": "boolean"}, {"name": "issues", "type": "list<string>"}, {"name": "improvement_suggestions", "type": "list<string>"}],
    },
    "SynthesizerInput": {
        "description": """Input to the synthesizer agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "solved_sub_solutions", "type": "list<SubSolution>"}],
    },
    "SynthesizedAnswer": {
        "description": """Final combined answer from the synthesizer""",
        "fields": [{"name": "final_answer", "type": "string"}, {"name": "synthesis_notes", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Plan and Solve"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    confidence: int
    current_sub_description: Any
    current_sub_index: Any
    decompose_result: Any
    depends_on: list
    description: str
    final_answer: str
    improvement_suggestions: list
    index: int
    invoke_verifier_result: Any
    is_valid: bool
    issues: list
    max_retries: Any
    prior_solutions: list
    problem: str
    retry_count: Any
    solution_text: str
    solve_subproblem_result: Any
    solved_sub_solutions: list
    sub_description: str
    sub_problems: list
    synthesis_notes: str
    synthesize_result: Any
    total_sub_problems: Any


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_planner(user_message, output_schema=None):
    """Planner Agent"""
    system = """You are a planning agent. Given a complex problem, decompose it into a numbered list of
smaller, sequential sub-problems. Each sub-problem should be self-contained enough to solve
independently, but they should be ordered so later sub-problems can build on earlier solutions.
Output JSON with "sub_problems": a list of objects, each with "index" (integer starting at 1),
"description" (string), and "depends_on" (list of indices this sub-problem depends on, or empty).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Planner Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_solver(user_message, output_schema=None):
    """Solver Agent"""
    system = """You are a problem-solving agent. Given a sub-problem description, the original problem for context,
and any previously solved sub-problem solutions, produce a solution. Output JSON with
"solution_text" (string with your solution) and "confidence" (integer 1-10).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Solver Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_verifier(user_message, output_schema=None):
    """Verifier Agent"""
    system = """You are a verification agent. Given a sub-problem, its proposed solution, and the original problem,
check whether the solution is correct, complete, and consistent. Output JSON with "is_valid"
(boolean), "issues" (list of strings describing any problems found), and "suggestions" (list of
strings with improvement suggestions).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Verifier Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_synthesizer(user_message, output_schema=None):
    """Synthesizer Agent"""
    system = """You are a synthesis agent. Given the original problem and all verified sub-solutions,
combine them into a single coherent final answer. Ensure consistency across sub-solutions
and present a unified response. Output JSON with "final_answer" (string) and
"synthesis_notes" (string describing how sub-solutions were combined).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Synthesizer Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_problem(state: AgentState) -> dict:
    """
    Receive Problem
    Accept the complex problem and initialize state
    """
    print(f"  → Receive Problem")
    updates = {}

    # Logic from spec
    updates["solved_sub_solutions"] = []
    updates["current_sub_index"] = 0
    updates["retry_count"] = 0
    updates["max_retries"] = 2
    print(f"    Problem: {state.get('problem', '')[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_decompose(state: AgentState) -> dict:
    """
    Decompose Problem
    Invoke the planner agent to break the problem into sub-problems
    """
    print(f"  → Decompose Problem")
    updates = {}

    # Logic from spec
    print(f"    Decomposing problem into sub-problems...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Decompose into sub-problems
    _cur = dict(state)
    _cur.update(updates)
    planner_input = build_input(_cur, "PlannerInput")
    planner_msg = json.dumps(planner_input, default=str)
    planner_raw = invoke_planner(planner_msg, output_schema="Decomposition")
    planner_result = parse_response(planner_raw, "Decomposition")
    updates["_schema_violations"] = len(validate_output(planner_result, "Decomposition"))
    updates.update(planner_result)
    updates["decomposition"] = planner_result
    updates["decompose_result"] = planner_result
    print(f"    ← Planner Agent: {planner_result}")

    return updates


def node_solve_subproblem(state: AgentState) -> dict:
    """
    Solve Sub-problem
    Invoke the solver agent on the current sub-problem
    """
    print(f"  → Solve Sub-problem")
    updates = {}

    # Logic from spec
    sub_problems = state.get("sub_problems", [])
    idx = state.get("current_sub_index", 0)
    if idx < len(sub_problems):
        current = sub_problems[idx]
        updates["current_sub_description"] = current.get("description", "")
        print(f"    Solving sub-problem {idx + 1}/{len(sub_problems)}: {current.get('description', '')[:80]}")
    else:
        print(f"    No more sub-problems to solve")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Solve current sub-problem
    _cur = dict(state)
    _cur.update(updates)
    solver_input = build_input(_cur, "SolverInput")
    solver_msg = json.dumps(solver_input, default=str)
    solver_raw = invoke_solver(solver_msg, output_schema="SubSolution")
    solver_result = parse_response(solver_raw, "SubSolution")
    updates["_schema_violations"] = len(validate_output(solver_result, "SubSolution"))
    updates.update(solver_result)
    updates["sub_solution"] = solver_result
    updates["solve_subproblem_result"] = solver_result
    print(f"    ← Solver Agent: {solver_result}")

    return updates


def node_accept_solution(state: AgentState) -> dict:
    """
    Accept Solution
    Record the verified sub-solution and advance to the next sub-problem
    """
    print(f"  → Accept Solution")
    updates = {}

    # Logic from spec
    solution = {
        "index": state.get("current_sub_index", 0),
        "description": state.get("current_sub_description", ""),
        "solution_text": state.get("solution_text", ""),
        "confidence": state.get("confidence", 0),
    }
    solved = state.get("solved_sub_solutions", [])
    solved.append(solution)
    updates["solved_sub_solutions"] = solved
    updates["current_sub_index"] = state.get("current_sub_index", 0) + 1
    updates["retry_count"] = 0
    print(f"    Accepted solution for sub-problem {solution['index'] + 1}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_invoke_verifier(state: AgentState) -> dict:
    """
    Invoke Verifier
    Send the current sub-solution to the verifier agent for checking
    """
    print(f"  → Invoke Verifier")
    updates = {}

    # Logic from spec
    updates["retry_count"] = state.get("retry_count", 0) + 1
    print(f"    Verifying sub-problem {state.get('current_sub_index', 0) + 1} (attempt {state.get('retry_count', "")})")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Check solution
    _cur = dict(state)
    _cur.update(updates)
    verifier_input = build_input(_cur, "VerifierInput")
    verifier_msg = json.dumps(verifier_input, default=str)
    verifier_raw = invoke_verifier(verifier_msg, output_schema="VerificationResult")
    verifier_result = parse_response(verifier_raw, "VerificationResult")
    updates["_schema_violations"] = len(validate_output(verifier_result, "VerificationResult"))
    updates.update(verifier_result)
    updates["verification_result"] = verifier_result
    updates["invoke_verifier_result"] = verifier_result
    print(f"    ← Verifier Agent: {verifier_result}")

    return updates


def node_synthesize(state: AgentState) -> dict:
    """
    Synthesize
    Invoke the synthesizer agent to combine all sub-solutions into a final answer
    """
    print(f"  → Synthesize")
    updates = {}

    # Logic from spec
    solved = state.get("solved_sub_solutions", [])
    print(f"    Synthesizing {len(solved)} sub-solutions into final answer")
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Combine sub-solutions
    _cur = dict(state)
    _cur.update(updates)
    synthesizer_input = build_input(_cur, "SynthesizerInput")
    synthesizer_msg = json.dumps(synthesizer_input, default=str)
    synthesizer_raw = invoke_synthesizer(synthesizer_msg, output_schema="SynthesizedAnswer")
    synthesizer_result = parse_response(synthesizer_raw, "SynthesizedAnswer")
    updates["_schema_violations"] = len(validate_output(synthesizer_result, "SynthesizedAnswer"))
    updates.update(synthesizer_result)
    updates["synthesized_answer"] = synthesizer_result
    updates["synthesize_result"] = synthesizer_result
    print(f"    ← Synthesizer Agent: {synthesizer_result}")

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_verify_solution(state: AgentState) -> str:
    """Gate: Solution valid? — is_valid == True"""
    if state.get("is_valid") == True:
        print(f"    → valid")
        return "accept_solution"
    else:
        print(f"    → invalid")
        return "retry_or_next"


def route_retry_or_next(state: AgentState) -> str:
    """Gate: Retries left? — retry_count < max_retries"""
    if (state.get("retry_count", 0)) < (state.get("max_retries", 0)):
        print(f"    → retries remaining")
        return "solve_subproblem"
    else:
        print(f"    → retries exhausted")
        return "accept_solution"


def route_check_remaining(state: AgentState) -> str:
    """Gate: More sub-problems? — current_sub_index < total_sub_problems"""
    if (state.get("current_sub_index", 0)) < (state.get("total_sub_problems", 0)):
        print(f"    → more sub-problems")
        return "solve_subproblem"
    else:
        print(f"    → all solved")
        return "synthesize"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_problem", node_receive_problem)
    graph.add_node("decompose", node_decompose)
    graph.add_node("solve_subproblem", node_solve_subproblem)
    graph.add_node("accept_solution", node_accept_solution)
    graph.add_node("invoke_verifier", node_invoke_verifier)
    graph.add_node("synthesize", node_synthesize)

    graph.set_entry_point("receive_problem")

    graph.add_edge("receive_problem", "decompose")
    graph.add_edge("decompose", "solve_subproblem")
    graph.add_edge("solve_subproblem", "invoke_verifier")
    graph.add_conditional_edges(
        "accept_solution",
        route_check_remaining,
        {
            "solve_subproblem": "solve_subproblem",
            "synthesize": "synthesize",
        }
    )
    graph.add_conditional_edges(
        "invoke_verifier",
        route_verify_solution,
        {
            "accept_solution": "accept_solution",
            "retry_or_next": "retry_or_next",
        }
    )
    graph.add_edge("synthesize", END)

    # Pass-through node for chained gate: retry_or_next
    graph.add_node("retry_or_next", lambda state: {})
    graph.add_conditional_edges(
        "retry_or_next",
        route_retry_or_next,
        {
            "solve_subproblem": "solve_subproblem",
            "accept_solution": "accept_solution",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Plan and Solve — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Plan and Solve (LangGraph)")
    print(f"  Plan-and-Solve agent that decomposes a problem into sub-problems, solves each with verification and retry logic, then synthesizes a final answer.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()