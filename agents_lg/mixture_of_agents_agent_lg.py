#!/usr/bin/env python3
"""
Mixture of Agents — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: Mixture of Agents (MoA) architecture: multiple diverse proposer agents generate responses in parallel, then an aggregator synthesizes the best composite response. Supports multi-layer iterative refinement with quality gating.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "MoAInput": {
        "description": """User query input for the Mixture of Agents system""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "ProposerInput": {
        "description": """Input to each proposer agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "prior_responses", "type": "list<string>"}, {"name": "current_layer", "type": "integer"}],
    },
    "ProposerOutput": {
        "description": """Output from a proposer agent""",
        "fields": [{"name": "response", "type": "string"}, {"name": "confidence", "type": "float"}],
    },
    "AggregatorInput": {
        "description": """Input to the aggregator agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "proposals", "type": "list<string>"}, {"name": "current_layer", "type": "integer"}, {"name": "prior_synthesis", "type": "string"}],
    },
    "AggregatorOutput": {
        "description": """Output from the aggregator agent""",
        "fields": [{"name": "synthesized_response", "type": "string"}, {"name": "quality_score", "type": "integer"}, {"name": "sources_used", "type": "list<string>"}, {"name": "reasoning", "type": "string"}],
    },
    "LayerSnapshot": {
        "description": """Snapshot of a single MoA layer's results""",
        "fields": [{"name": "layer", "type": "integer"}, {"name": "proposals", "type": "list<string>"}, {"name": "synthesized_response", "type": "string"}, {"name": "quality_score", "type": "integer"}, {"name": "sources_used", "type": "list<string>"}],
    },
    "MoAResult": {
        "description": """Final result from the Mixture of Agents process""",
        "fields": [{"name": "final_response", "type": "string"}, {"name": "final_quality", "type": "integer"}, {"name": "total_layers", "type": "integer"}, {"name": "layer_history", "type": "list<LayerSnapshot>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Mixture of Agents"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    aggregate_responses_result: Any
    call_analytical_result: Any
    call_creative_result: Any
    call_critical_result: Any
    confidence: Any
    current_layer: int
    final_quality: int
    final_response: str
    layer: int
    layer_history: list
    max_layers: Any
    prior_responses: list
    prior_synthesis: str
    proposal_count: Any
    proposals: list
    quality_score: int
    quality_threshold: Any
    query: str
    reasoning: str
    response: str
    response_store: dict
    sources_used: list
    synthesized_response: str
    total_layers: int


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_response_store:
    """Layer Response Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_response_store = Store_response_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_proposer_analytical(user_message, output_schema=None):
    """Analytical Proposer"""
    system = """You are an analytical reasoning agent. Given a query, approach it with rigorous
logical analysis. Break down the problem systematically, consider edge cases,
and provide a well-structured analytical response.
If prior layer responses are provided, build upon their insights while
correcting any errors you identify.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Analytical Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_proposer_creative(user_message, output_schema=None):
    """Creative Proposer"""
    system = """You are a creative thinking agent. Given a query, approach it with lateral
thinking, analogies, and novel perspectives. Consider unconventional angles
and generate insightful connections.
If prior layer responses are provided, offer fresh perspectives that complement them.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Creative Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_proposer_critical(user_message, output_schema=None):
    """Critical Proposer"""
    system = """You are a critical evaluation agent. Given a query, approach it by questioning
assumptions, identifying potential flaws, and stress-testing reasoning.
If prior layer responses are provided, critique their weaknesses while
offering improved alternatives.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Critical Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_aggregator_agent(user_message, output_schema=None):
    """Aggregator Agent"""
    system = """You are an aggregator agent in a Mixture of Agents architecture.
You receive multiple diverse responses to the same query from different
proposer agents. Your job is to:
1. Identify the strongest elements from each response.
2. Resolve any contradictions by reasoning about which is correct.
3. Synthesize a single, comprehensive response that is better than any individual one.
4. Assign a quality score (1-10) to the synthesized result.
Output JSON with "synthesized_response" (string), "quality_score" (integer 1-10),
"sources_used" (list of strings naming which proposers contributed key elements),
and "reasoning" (string explaining synthesis decisions).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Aggregator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_query(state: AgentState) -> dict:
    """
    Receive Query
    Accept user query and initialize MoA layer state
    """
    print(f"  → Receive Query")
    updates = {}

    # Logic from spec
    updates["current_layer"] = 0
    updates["max_layers"] = 3
    updates["quality_threshold"] = 7
    updates["layer_history"] = []
    updates["prior_responses"] = []
    print(f"    Query: {state.get('query', '')[:100]}")
    print(f"    Config: max_layers={state.get('max_layers', "")}, quality_threshold={state.get('quality_threshold', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_increment_layer(state: AgentState) -> dict:
    """
    Increment Layer
    Advance the current layer counter and prepare proposer context
    """
    print(f"  → Increment Layer")
    updates = {}

    # Logic from spec
    layer = state.get("current_layer", 0) + 1
    updates["current_layer"] = layer
    print(f"    Starting layer {layer}/{state.get('max_layers', 3)}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_call_analytical(state: AgentState) -> dict:
    """
    Call Analytical Proposer
    Invoke the analytical proposer agent
    """
    print(f"  → Call Analytical Proposer")
    updates = {}

    # Logic from spec
    print(f"    Analytical proposer processing (layer {state.get('current_layer', 1)})...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate analytical response
    _cur = dict(state)
    _cur.update(updates)
    proposer_analytical_input = build_input(_cur, "ProposerInput")
    proposer_analytical_msg = json.dumps(proposer_analytical_input, default=str)
    proposer_analytical_raw = invoke_proposer_analytical(proposer_analytical_msg, output_schema="ProposerOutput")
    proposer_analytical_result = parse_response(proposer_analytical_raw, "ProposerOutput")
    updates["_schema_violations"] = len(validate_output(proposer_analytical_result, "ProposerOutput"))
    updates["call_analytical_result"] = proposer_analytical_result
    print(f"    ← Analytical Proposer: {proposer_analytical_result}")

    return updates


def node_call_creative(state: AgentState) -> dict:
    """
    Call Creative Proposer
    Invoke the creative proposer agent
    """
    print(f"  → Call Creative Proposer")
    updates = {}

    # Logic from spec
    print(f"    Creative proposer processing (layer {state.get('current_layer', 1)})...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate creative response
    _cur = dict(state)
    _cur.update(updates)
    proposer_creative_input = build_input(_cur, "ProposerInput")
    proposer_creative_msg = json.dumps(proposer_creative_input, default=str)
    proposer_creative_raw = invoke_proposer_creative(proposer_creative_msg, output_schema="ProposerOutput")
    proposer_creative_result = parse_response(proposer_creative_raw, "ProposerOutput")
    updates["_schema_violations"] = len(validate_output(proposer_creative_result, "ProposerOutput"))
    updates["call_creative_result"] = proposer_creative_result
    print(f"    ← Creative Proposer: {proposer_creative_result}")

    return updates


def node_call_critical(state: AgentState) -> dict:
    """
    Call Critical Proposer
    Invoke the critical proposer agent
    """
    print(f"  → Call Critical Proposer")
    updates = {}

    # Logic from spec
    print(f"    Critical proposer processing (layer {state.get('current_layer', 1)})...")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate critical response
    _cur = dict(state)
    _cur.update(updates)
    proposer_critical_input = build_input(_cur, "ProposerInput")
    proposer_critical_msg = json.dumps(proposer_critical_input, default=str)
    proposer_critical_raw = invoke_proposer_critical(proposer_critical_msg, output_schema="ProposerOutput")
    proposer_critical_result = parse_response(proposer_critical_raw, "ProposerOutput")
    updates["_schema_violations"] = len(validate_output(proposer_critical_result, "ProposerOutput"))
    updates["call_critical_result"] = proposer_critical_result
    print(f"    ← Critical Proposer: {proposer_critical_result}")

    return updates


def node_collect_proposals(state: AgentState) -> dict:
    """
    Collect Proposals
    Gather all proposer responses and prepare input for the aggregator
    """
    print(f"  → Collect Proposals")
    updates = {}

    # Read: Read prior layer responses
    response_store_data = _store_response_store.read()
    updates["response_store"] = response_store_data

    # Logic from spec
    proposals = []
    for key in ["call_analytical_result", "call_creative_result", "call_critical_result"]:
        result = state.get(key, {})
        resp = result.get("response", "") if isinstance(result, dict) else str(result)
        if resp:
            proposals.append(resp)
    updates["proposals"] = proposals
    updates["proposal_count"] = len(proposals)
    print(f"    Collected {len(proposals)} proposals for aggregation")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_aggregate_responses(state: AgentState) -> dict:
    """
    Aggregate Responses
    Invoke the aggregator agent to synthesize proposer responses
    """
    print(f"  → Aggregate Responses")
    updates = {}

    # Logic from spec
    proposals = state.get("proposals", [])
    layer = state.get("current_layer", 1)
    print(f"    Aggregating {len(proposals)} proposals at layer {layer}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Synthesize proposals
    _cur = dict(state)
    _cur.update(updates)
    aggregator_agent_input = build_input(_cur, "AggregatorInput")
    aggregator_agent_msg = json.dumps(aggregator_agent_input, default=str)
    aggregator_agent_raw = invoke_aggregator_agent(aggregator_agent_msg, output_schema="AggregatorOutput")
    aggregator_agent_result = parse_response(aggregator_agent_raw, "AggregatorOutput")
    updates["_schema_violations"] = len(validate_output(aggregator_agent_result, "AggregatorOutput"))
    updates.update(aggregator_agent_result)
    updates["aggregator_output"] = aggregator_agent_result
    updates["aggregate_responses_result"] = aggregator_agent_result
    print(f"    ← Aggregator Agent: {aggregator_agent_result}")

    return updates


def node_record_layer(state: AgentState) -> dict:
    """
    Record Layer
    Store the current layer results in history
    """
    print(f"  → Record Layer")
    updates = {}

    # Logic from spec
    layer = state.get("current_layer", 1)
    synthesized = state.get("synthesized_response", "")
    quality = state.get("quality_score", 0)
    layer_entry = {
        "layer": layer,
        "proposals": state.get("proposals", []),
        "synthesized_response": synthesized,
        "quality_score": quality,
        "sources_used": state.get("sources_used", []),
    }
    history = state.get("layer_history", [])
    history.append(layer_entry)
    updates["layer_history"] = history
    # Feed synthesized response as prior context for next layer
    prior = state.get("prior_responses", [])
    prior.append(synthesized)
    updates["prior_responses"] = prior
    print(f"    Layer {layer} recorded: quality={quality}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Persist layer snapshot
    _cur = dict(state)
    _cur.update(updates)
    response_store_write = build_input(_cur, "LayerSnapshot")
    _store_response_store.write(response_store_write)

    return updates


def node_finalize_output(state: AgentState) -> dict:
    """
    Finalize Output
    Return the final synthesized response with full layer history
    """
    print(f"  → Finalize Output")
    updates = {}

    # Logic from spec
    quality = state.get("quality_score", 0)
    layers = state.get("current_layer", 1)
    threshold = state.get("quality_threshold", 7)
    if quality >= threshold:
        print(f"    Quality threshold met (score={quality}) at layer {layers}. Finalizing.")
    else:
        print(f"    Max layers reached (score={quality}). Finalizing with best result.")
    updates["final_response"] = state.get("synthesized_response", "")
    updates["final_quality"] = quality
    updates["total_layers"] = layers
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_quality_gate(state: AgentState) -> str:
    """Gate: Quality Sufficient? — quality_score >= quality_threshold"""
    if (state.get("quality_score", 0)) >= (state.get("quality_threshold", 0)):
        print(f"    → quality meets threshold")
        return "finalize_output"
    else:
        print(f"    → quality below threshold")
        return "check_layer_limit"


def route_check_layer_limit(state: AgentState) -> str:
    """Gate: More layers allowed? — current_layer < max_layers"""
    if (state.get("current_layer", 0)) < (state.get("max_layers", 0)):
        print(f"    → layers remaining")
        return "increment_layer"
    else:
        print(f"    → max layers reached")
        return "finalize_output"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_query", node_receive_query)
    graph.add_node("increment_layer", node_increment_layer)
    graph.add_node("call_analytical", node_call_analytical)
    graph.add_node("call_creative", node_call_creative)
    graph.add_node("call_critical", node_call_critical)
    graph.add_node("collect_proposals", node_collect_proposals)
    graph.add_node("aggregate_responses", node_aggregate_responses)
    graph.add_node("record_layer", node_record_layer)
    graph.add_node("finalize_output", node_finalize_output)

    graph.set_entry_point("receive_query")

    graph.add_edge("receive_query", "increment_layer")
    graph.add_edge("increment_layer", "call_analytical")
    graph.add_edge("increment_layer", "call_creative")
    graph.add_edge("increment_layer", "call_critical")
    graph.add_edge("call_analytical", "collect_proposals")
    graph.add_edge("call_creative", "collect_proposals")
    graph.add_edge("call_critical", "collect_proposals")
    graph.add_edge("collect_proposals", "aggregate_responses")
    graph.add_edge("aggregate_responses", "record_layer")
    graph.add_conditional_edges(
        "record_layer",
        route_quality_gate,
        {
            "finalize_output": "finalize_output",
            "check_layer_limit": "check_layer_limit",
        }
    )
    graph.add_edge("finalize_output", END)

    # Pass-through node for chained gate: check_layer_limit
    graph.add_node("check_layer_limit", lambda state: {})
    graph.add_conditional_edges(
        "check_layer_limit",
        route_check_layer_limit,
        {
            "increment_layer": "increment_layer",
            "finalize_output": "finalize_output",
        }
    )

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Mixture of Agents — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Mixture of Agents (LangGraph)")
    print(f"  Mixture of Agents (MoA) architecture: multiple diverse proposer agents generate responses in parallel, then an aggregator synthesizes the best composite response. Supports multi-layer iterative refinement with quality gating.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    initial = {}
    initial["query"] = input("Enter query: ")
    run(initial)