#!/usr/bin/env python3
"""
Socratic Tutoring Agent — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: A tutoring system that guides students through topics using probing questions, adaptive hints, and remediation based on understanding levels.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=8192):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=8192, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TutoringInput": {
        "description": """""",
        "fields": [{"name": "topic", "type": "string"}, {"name": "learning_objectives", "type": "list<string>"}, {"name": "difficulty", "type": "enum[beginner, intermediate, advanced]"}],
    },
    "LessonPlanOutput": {
        "description": """""",
        "fields": [{"name": "concepts", "type": "list<Concept>"}, {"name": "total_concepts", "type": "integer"}],
    },
    "Concept": {
        "description": """""",
        "fields": [{"name": "concept_name", "type": "string"}, {"name": "prerequisites", "type": "list<string>"}, {"name": "suggested_question_types", "type": "list<string>"}],
    },
    "QuestionGeneratorInput": {
        "description": """""",
        "fields": [{"name": "concept", "type": "Concept"}, {"name": "history", "type": "list<Exchange>"}, {"name": "attempt_count", "type": "integer"}],
    },
    "TutorQuestionOutput": {
        "description": """""",
        "fields": [{"name": "question_text", "type": "string"}, {"name": "expected_key_points", "type": "list<string>"}, {"name": "difficulty_level", "type": "string"}, {"name": "hints", "type": "list<string>"}],
    },
    "StudentResponse": {
        "description": """""",
        "fields": [{"name": "answer_text", "type": "string"}],
    },
    "EvaluationInput": {
        "description": """""",
        "fields": [{"name": "student_answer", "type": "string"}, {"name": "key_points", "type": "list<string>"}, {"name": "concept", "type": "Concept"}],
    },
    "EvaluationOutput": {
        "description": """""",
        "fields": [{"name": "correctness_score", "type": "float"}, {"name": "misconceptions", "type": "list<string>"}, {"name": "mastered_concepts", "type": "list<string>"}, {"name": "feedback_text", "type": "string"}],
    },
    "RemediationOutput": {
        "description": """""",
        "fields": [{"name": "explanation", "type": "string"}, {"name": "prerequisite_review", "type": "string"}],
    },
    "SummaryInput": {
        "description": """""",
        "fields": [{"name": "mastered", "type": "list<string>"}, {"name": "review", "type": "list<string>"}, {"name": "transcript", "type": "list<Exchange>"}],
    },
    "SessionSummaryOutput": {
        "description": """""",
        "fields": [{"name": "concepts_mastered", "type": "list<string>"}, {"name": "concepts_needing_review", "type": "list<string>"}, {"name": "progress_percentage", "type": "float"}, {"name": "recommended_next_topics", "type": "list<string>"}, {"name": "full_transcript", "type": "list<Exchange>"}],
    },
    "Exchange": {
        "description": """""",
        "fields": [{"name": "role", "type": "string"}, {"name": "content", "type": "string"}],
    },
    "SessionState": {
        "description": """""",
        "fields": [{"name": "transcript", "type": "list<Exchange>"}, {"name": "current_concept_index", "type": "integer"}, {"name": "mastered_concepts", "type": "list<string>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Socratic Tutoring Agent"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    answer_text: str
    attempt_count: int
    concept: Any
    concept_name: str
    concepts: list
    concepts_mastered: list
    concepts_needing_review: list
    content: str
    correctness_score: Any
    current_concept: Any
    current_concept_index: int
    difficulty: str
    difficulty_level: str
    eval_input: Any
    evaluate_response_result: Any
    expected_key_points: list
    explanation: str
    feedback_text: str
    full_transcript: list
    gen_input: Any
    generate_question_result: Any
    hints: list
    history: list
    key_points: list
    learning_objectives: list
    lesson_plan_output: Any
    mastered: list
    mastered_concepts: list
    misconceptions: list
    needs_review: Any
    plan_curriculum_result: Any
    prerequisite_review: str
    prerequisites: list
    progress_percentage: Any
    question_text: str
    recommended_next_topics: list
    remediate_result: Any
    review: list
    role: str
    session_store: dict
    student_answer: str
    suggested_question_types: list
    summarize_session_result: Any
    summary_input: Any
    topic: str
    total_concepts: int
    transcript: list
    tutor_question_output: Any


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_session_store:
    """Session Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_session_store = Store_session_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_curriculum_planner(user_message, output_schema=None):
    """Curriculum Planner"""
    system = """You are an expert educational designer. Given a topic and learning objectives, create a structured lesson plan.
Break the topic into a sequence of concepts. For each concept, specify prerequisite concepts and suggested question types.
Output a LessonPlanOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Curriculum Planner", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_question_generator(user_message, output_schema=None):
    """Question Generator"""
    system = """You are a Socratic tutor. Your goal is to ask probing questions that lead the student to discover answers themselves.
Receive the current concept, student history, and attempt count.
Generate a question, expected key points, and a list of 3 increasingly specific hints.
Output a TutorQuestionOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Question Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_evaluator(user_message, output_schema=None):
    """Response Evaluator"""
    system = """Compare the student's response against the expected key points. 
Provide a correctness score (0.0 to 1.0), identify misconceptions, and provide feedback that encourages further thought without giving away the answer.
Output an EvaluationOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Response Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_remediation_agent(user_message, output_schema=None):
    """Remediation Specialist"""
    system = """The student is struggling significantly with a concept. Provide a simpler explanation or a review of prerequisites to bridge the gap.
Do not solve the original question, but prepare them to try again.
Output a RemediationOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Remediation Specialist", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_summary_agent(user_message, output_schema=None):
    """Session Summarizer"""
    system = """Analyze the full tutoring transcript and progress. Summarize mastered concepts, areas for review, and overall progress.
Output a SessionSummaryOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=8192,
    )
    trace_call("Session Summarizer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_start_tutoring(state: AgentState) -> dict:
    """
    Initialize Session
    Accept tutoring parameters and initialize state
    """
    print(f"  → Initialize Session")
    updates = {}

    # Logic from spec
    updates["transcript"] = []
    updates["current_concept_index"] = 0
    updates["attempt_count"] = 0
    updates["mastered_concepts"] = []
    updates["needs_review"] = []
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_plan_curriculum(state: AgentState) -> dict:
    """
    Plan Curriculum
    Invoke planner to create the lesson sequence
    """
    print(f"  → Plan Curriculum")
    updates = {}

    # Invoke: curriculum_planner
    _cur = dict(state)
    _cur.update(updates)
    curriculum_planner_input = build_input(_cur, "TutoringInput")
    curriculum_planner_msg = json.dumps(curriculum_planner_input, default=str)
    curriculum_planner_raw = invoke_curriculum_planner(curriculum_planner_msg, output_schema="LessonPlanOutput")
    curriculum_planner_result = parse_response(curriculum_planner_raw, "LessonPlanOutput")
    updates["_schema_violations"] = len(validate_output(curriculum_planner_result, "LessonPlanOutput"))
    updates["plan_curriculum_result"] = curriculum_planner_result
    print(f"    ← Curriculum Planner: {curriculum_planner_result}")

    return updates


def node_generate_question(state: AgentState) -> dict:
    """
    Generate Question
    Prepare context for the question generator
    """
    print(f"  → Generate Question")
    updates = {}

    # Logic from spec
    # Retrieve current concept from the plan
    plan = state.get("lesson_plan_output", {})
    concepts = plan.get("concepts", [])
    idx = state.get("current_concept_index", 0)
    
    if idx < len(concepts):
        updates["current_concept"] = concepts[idx]
    
    # Prepare input for agent
    updates["gen_input"] = {
        "concept": state.get("current_concept"),
        "history": state.get("transcript", []),
        "attempt_count": state.get("attempt_count", 0)
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: question_generator
    _cur = dict(state)
    _cur.update(updates)
    question_generator_input = build_input(_cur, "QuestionGeneratorInput")
    question_generator_msg = json.dumps(question_generator_input, default=str)
    question_generator_raw = invoke_question_generator(question_generator_msg, output_schema="TutorQuestionOutput")
    question_generator_result = parse_response(question_generator_raw, "TutorQuestionOutput")
    updates["_schema_violations"] = len(validate_output(question_generator_result, "TutorQuestionOutput"))
    updates.update(question_generator_result)
    updates["tutor_question_output"] = question_generator_result
    updates["generate_question_result"] = question_generator_result
    print(f"    ← Question Generator: {question_generator_result}")

    return updates


def node_get_student_response(state: AgentState) -> dict:
    """
    Wait for Student
    """
    print(f"  → Wait for Student")
    updates = {}

    # Checkpoint (HITL)
    _canned = state.get("_canned_responses", [])
    if _canned:
        response = _canned[0]
        updates["_canned_responses"] = _canned[1:]
    else:
        response = input("Please answer the tutor's question. [Submit Answer/End Session]: ").strip().lower()
    updates["checkpoint_response"] = response
    return updates


def node_evaluate_response(state: AgentState) -> dict:
    """
    Evaluate Response
    Prepare student response and key points for evaluation
    """
    print(f"  → Evaluate Response")
    updates = {}

    # Logic from spec
    # Capture response into transcript
    response_text = state.get("answer_text", "")
    _list = list(state.get("transcript", []) or []); _list.append({"role": "student", "content": response_text}); updates["transcript"] = _list
    
    # Prepare evaluation input
    updates["eval_input"] = {
        "student_answer": response_text,
        "key_points": state.get("tutor_question_output", {}).get("expected_key_points", []),
        "concept": state.get("current_concept")
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: evaluator
    _cur = dict(state)
    _cur.update(updates)
    evaluator_input = build_input(_cur, "EvaluationInput")
    evaluator_msg = json.dumps(evaluator_input, default=str)
    evaluator_raw = invoke_evaluator(evaluator_msg, output_schema="EvaluationOutput")
    evaluator_result = parse_response(evaluator_raw, "EvaluationOutput")
    updates["_schema_violations"] = len(validate_output(evaluator_result, "EvaluationOutput"))
    updates.update(evaluator_result)
    updates["evaluation_output"] = evaluator_result
    updates["evaluate_response_result"] = evaluator_result
    print(f"    ← Response Evaluator: {evaluator_result}")

    # Write: 
    _cur = dict(state)
    _cur.update(updates)
    session_store_write = build_input(_cur, "SessionState")
    _store_session_store.write(session_store_write)

    return updates


def node_advance_concept(state: AgentState) -> dict:
    """
    Advance Concept
    Mark concept as mastered and move to next
    """
    print(f"  → Advance Concept")
    updates = {}

    # Logic from spec
    concept_name = state.get("current_concept", {}).get("concept_name", "Unknown")
    _list = list(state.get("mastered_concepts", []) or []); _list.append(concept_name); updates["mastered_concepts"] = _list
    updates["current_concept_index"] = state.get("current_concept_index", 0) + 1
    updates["attempt_count"] = 0
    print(f"Concept {concept_name} mastered.")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_provide_hint(state: AgentState) -> dict:
    """
    Provide Hint
    Select next hint and increment attempt counter
    """
    print(f"  → Provide Hint")
    updates = {}

    # Logic from spec
    updates["attempt_count"] = state.get("attempt_count", 0) + 1
    hints = state.get("tutor_question_output", {}).get("hints", [])
    # Select hint based on attempt count (clamped to list size)
    hint_idx = min(state.get("attempt_count", "") - 1, len(hints) - 1)
    current_hint = hints[hint_idx] if hints else "Think about the core principle again."
    
    _list = list(state.get("transcript", []) or []); _list.append({"role": "tutor", "content": f"Hint: {current_hint}"}); updates["transcript"] = _list
    
    # Termination check for loop: if attempts > 3, force advance but mark for review
    if state.get("attempt_count", "") >= 3:
        _list = list(state.get("needs_review", []) or []); _list.append(state.get("current_concept", {}).get("concept_name")); updates["needs_review"] = _list
        updates["current_concept_index"] = state.get("current_concept_index", 0) + 1
        updates["attempt_count"] = 0
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_remediate(state: AgentState) -> dict:
    """
    Remediate
    Invoke remediation agent for struggling student
    """
    print(f"  → Remediate")
    updates = {}

    # Logic from spec
    updates["attempt_count"] = state.get("attempt_count", 0) + 1
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: remediation_agent
    _cur = dict(state)
    _cur.update(updates)
    remediation_agent_input = build_input(_cur, "EvaluationOutput")
    remediation_agent_msg = json.dumps(remediation_agent_input, default=str)
    remediation_agent_raw = invoke_remediation_agent(remediation_agent_msg, output_schema="RemediationOutput")
    remediation_agent_result = parse_response(remediation_agent_raw, "RemediationOutput")
    updates["_schema_violations"] = len(validate_output(remediation_agent_result, "RemediationOutput"))
    updates["remediate_result"] = remediation_agent_result
    print(f"    ← Remediation Specialist: {remediation_agent_result}")

    return updates


def node_summarize_session(state: AgentState) -> dict:
    """
    Summarize Session
    Generate final report
    """
    print(f"  → Summarize Session")
    updates = {}

    # Logic from spec
    updates["summary_input"] = {
        "mastered": state.get("mastered_concepts", []),
        "review": state.get("needs_review", []),
        "transcript": state.get("transcript", [])
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: summary_agent
    _cur = dict(state)
    _cur.update(updates)
    summary_agent_input = build_input(_cur, "SummaryInput")
    summary_agent_msg = json.dumps(summary_agent_input, default=str)
    summary_agent_raw = invoke_summary_agent(summary_agent_msg, output_schema="SessionSummaryOutput")
    summary_agent_result = parse_response(summary_agent_raw, "SessionSummaryOutput")
    updates["_schema_violations"] = len(validate_output(summary_agent_result, "SessionSummaryOutput"))
    updates.update(summary_agent_result)
    updates["session_summary_output"] = summary_agent_result
    updates["summarize_session_result"] = summary_agent_result
    print(f"    ← Session Summarizer: {summary_agent_result}")

    return updates


def node_end_session(state: AgentState) -> dict:
    """
    End Session
    """
    print(f"  → End Session")
    updates = {}

    # Logic from spec
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_progress(state: AgentState) -> str:
    """Gate: Check Understanding — correctness_score"""
    if (state.get("correctness_score", 0)) > 0.8:
        return "advance_concept"
    elif (state.get("correctness_score", 0)) >= 0.4:
        return "provide_hint"
    else:
        return "remediate"


def route_check_completion(state: AgentState) -> str:
    """Gate: All Concepts Covered? — current_concept_index >= total_concepts"""
    if (state.get("current_concept_index", 0)) >= (state.get("total_concepts", 0)):
        print(f"    → done")
        return "summarize_session"
    else:
        print(f"    → more")
        return "generate_question"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("start_tutoring", node_start_tutoring)
    graph.add_node("plan_curriculum", node_plan_curriculum)
    graph.add_node("generate_question", node_generate_question)
    graph.add_node("get_student_response", node_get_student_response)
    graph.add_node("evaluate_response", node_evaluate_response)
    graph.add_node("advance_concept", node_advance_concept)
    graph.add_node("provide_hint", node_provide_hint)
    graph.add_node("remediate", node_remediate)
    graph.add_node("summarize_session", node_summarize_session)
    graph.add_node("end_session", node_end_session)

    graph.set_entry_point("start_tutoring")

    graph.add_edge("start_tutoring", "plan_curriculum")
    graph.add_edge("plan_curriculum", "generate_question")
    graph.add_edge("generate_question", "get_student_response")
    graph.add_edge("get_student_response", "evaluate_response")
    graph.add_conditional_edges(
        "evaluate_response",
        route_check_progress,
        {
            "advance_concept": "advance_concept",
            "provide_hint": "provide_hint",
            "remediate": "remediate",
        }
    )
    graph.add_conditional_edges(
        "advance_concept",
        route_check_completion,
        {
            "summarize_session": "summarize_session",
            "generate_question": "generate_question",
        }
    )
    graph.add_edge("provide_hint", "generate_question")
    graph.add_edge("remediate", "generate_question")
    graph.add_edge("summarize_session", "end_session")
    graph.add_edge("end_session", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Socratic Tutoring Agent — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Socratic Tutoring Agent (LangGraph)")
    print(f"  A tutoring system that guides students through topics using probing questions, adaptive hints, and remediation based on understanding levels.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()