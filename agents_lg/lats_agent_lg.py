#!/usr/bin/env python3
"""
Language Agent Tree Search (LATS) — Generated by OpenClaw Instantiation Engine (LangGraph backend)
Spec: Iterative problem solving using Monte Carlo Tree Search (MCTS) combined with LLM-based action generation and state evaluation.
"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "LATSInput": {
        "description": """""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "ExpansionInput": {
        "description": """""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "trajectory", "type": "string"}],
    },
    "ActionGeneratorOutput": {
        "description": """""",
        "fields": [{"name": "actions", "type": "list<string>"}],
    },
    "EvaluationInput": {
        "description": """""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "trajectory", "type": "string"}, {"name": "new_state", "type": "string"}],
    },
    "StateEvaluatorOutput": {
        "description": """""",
        "fields": [{"name": "score", "type": "float"}, {"name": "is_terminal", "type": "boolean"}],
    },
    "FormatterInput": {
        "description": """""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "best_trajectory", "type": "string"}],
    },
    "FinalSolution": {
        "description": """""",
        "fields": [{"name": "answer", "type": "string"}],
    },
    "TreeData": {
        "description": """""",
        "fields": [{"name": "tree", "type": "object"}, {"name": "iteration", "type": "integer"}, {"name": "best_score", "type": "float"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Language Agent Tree Search (LATS)"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: Annotated[int, operator.add]
    action_generator_output: Any
    actions: list
    answer: str
    best_score: Any
    best_trajectory: str
    current_trajectory: Any
    evaluate_node_result: Any
    evaluation_input: Any
    expand_node_result: Any
    expansion_input: Any
    format_output_result: Any
    formatter_input: Any
    is_terminal: bool
    iteration: int
    new_state: str
    problem: str
    query: str
    score: Any
    state_evaluator_output: Any
    trajectory: str
    tree: Any
    tree_store: dict


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_tree_store:
    """MCTS Tree Store (blackboard)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_tree_store = Store_tree_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_action_generator(user_message, output_schema=None):
    """Action Generator"""
    system = """You are the expansion phase of an MCTS agent. Given a problem statement and a trajectory of previous actions/states, generate candidate next actions.
Provide a list of potential actions that could lead toward a solution.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Action Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_state_evaluator(user_message, output_schema=None):
    """State Evaluator"""
    system = """You are the evaluation phase of an MCTS agent. Given a problem and a trajectory ending in a new state, evaluate the state.
Assign a value from 0 to 1 (0 = dead end, 1 = solution). 
Identify if the state is 'terminal' (problem solved) or a 'dead_end'.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("State Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_solution_formatter(user_message, output_schema=None):
    """Solution Formatter"""
    system = """You are the final output phase. Given the best trajectory found during the tree search, format it into a clear, concise final answer for the user.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Solution Formatter", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_initialize_search(state: AgentState) -> dict:
    """
    Initialize Search
    Setup the root node and search parameters
    """
    print(f"  → Initialize Search")
    updates = {}

    # Logic from spec
    updates["iteration"] = 0
    updates["is_terminal"] = False
    updates["best_score"] = 0.0
    updates["tree"] = {"root": {"query": state.get("query"), "children": [], "visits": 0, "value": 0}}
    print(f"Starting LATS for: {state.get('query')}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_select_node(state: AgentState) -> dict:
    """
    Selection
    Select a leaf node to expand using UCT logic
    """
    print(f"  → Selection")
    updates = {}

    # Read: Read Tree State
    tree_store_data = _store_tree_store.read()
    updates["tree_store"] = tree_store_data

    # Logic from spec
    # Logic to traverse the tree in state.get('tree', "") using UCT
    # For this spec, we identify the 'current_trajectory' to expand
    updates["current_trajectory"] = state.get("tree", {}).get("root", {}).get("query", "")
    print(f"Selecting node for iteration {state.get('iteration', "")}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_expand_node(state: AgentState) -> dict:
    """
    Expansion
    Generate candidate actions for the selected node
    """
    print(f"  → Expansion")
    updates = {}

    # Logic from spec
    updates["expansion_input"] = {
      "problem": state.get("query"),
      "trajectory": state.get("current_trajectory")
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: action_generator
    _cur = dict(state)
    _cur.update(updates)
    action_generator_input = build_input(_cur, "ExpansionInput")
    action_generator_msg = json.dumps(action_generator_input, default=str)
    action_generator_raw = invoke_action_generator(action_generator_msg, output_schema="ActionGeneratorOutput")
    action_generator_result = parse_response(action_generator_raw, "ActionGeneratorOutput")
    updates["_schema_violations"] = len(validate_output(action_generator_result, "ActionGeneratorOutput"))
    updates.update(action_generator_result)
    updates["action_generator_output"] = action_generator_result
    updates["expand_node_result"] = action_generator_result
    print(f"    ← Action Generator: {action_generator_result}")

    return updates


def node_evaluate_node(state: AgentState) -> dict:
    """
    Evaluation
    Evaluate the newly generated states
    """
    print(f"  → Evaluation")
    updates = {}

    # Logic from spec
    # Prepare input for the evaluator based on the last action from action_generator
    actions = state.get("action_generator_output", {}).get("actions", [])
    updates["evaluation_input"] = {
      "problem": state.get("query"),
      "trajectory": state.get("current_trajectory"),
      "new_state": actions[0] if actions else "No action"
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: state_evaluator
    _cur = dict(state)
    _cur.update(updates)
    state_evaluator_input = build_input(_cur, "EvaluationInput")
    state_evaluator_msg = json.dumps(state_evaluator_input, default=str)
    state_evaluator_raw = invoke_state_evaluator(state_evaluator_msg, output_schema="StateEvaluatorOutput")
    state_evaluator_result = parse_response(state_evaluator_raw, "StateEvaluatorOutput")
    updates["_schema_violations"] = len(validate_output(state_evaluator_result, "StateEvaluatorOutput"))
    updates.update(state_evaluator_result)
    updates["state_evaluator_output"] = state_evaluator_result
    updates["evaluate_node_result"] = state_evaluator_result
    print(f"    ← State Evaluator: {state_evaluator_result}")

    return updates


def node_backpropagate(state: AgentState) -> dict:
    """
    Backpropagation
    Update tree statistics with the evaluation result
    """
    print(f"  → Backpropagation")
    updates = {}

    # Logic from spec
    eval_result = state.get("state_evaluator_output", {})
    score = eval_result.get("score", 0.0)
    updates["is_terminal"] = eval_result.get("is_terminal", False)
    updates["iteration"] = state.get("iteration", 0) + 1
    if score > state.get("best_score", ""):
        updates["best_score"] = score
        updates["best_trajectory"] = state.get("current_trajectory")
    print(f"Iteration {state.get('iteration', "")} complete. Score: {score}")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Update Tree
    _store_tree_store.write(dict(state))

    return updates


def node_format_output(state: AgentState) -> dict:
    """
    Format Output
    Prepare the final solution
    """
    print(f"  → Format Output")
    updates = {}

    # Logic from spec
    updates["formatter_input"] = {
      "problem": state.get("query"),
      "best_trajectory": state.get("best_trajectory", "No solution found")
    }
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: solution_formatter
    _cur = dict(state)
    _cur.update(updates)
    solution_formatter_input = build_input(_cur, "FormatterInput")
    solution_formatter_msg = json.dumps(solution_formatter_input, default=str)
    solution_formatter_raw = invoke_solution_formatter(solution_formatter_msg, output_schema="FinalSolution")
    solution_formatter_result = parse_response(solution_formatter_raw, "FinalSolution")
    updates["_schema_violations"] = len(validate_output(solution_formatter_result, "FinalSolution"))
    updates.update(solution_formatter_result)
    updates["final_solution"] = solution_formatter_result
    updates["format_output_result"] = solution_formatter_result
    print(f"    ← Solution Formatter: {solution_formatter_result}")

    return updates


def node_finalize(state: AgentState) -> dict:
    """
    Finalize
    """
    print(f"  → Finalize")
    updates = {}

    # Logic from spec
    updates["_done"] = True
    print("LATS process complete.")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_termination(state: AgentState) -> str:
    """Gate: Search Finished? — is_terminal == True or iteration >= 30"""
    if (state.get("iteration", 0)) >= 30:
        print(f"    → continue")
        return "select_node"
    else:
        print(f"    → finish")
        return "format_output"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("initialize_search", node_initialize_search)
    graph.add_node("select_node", node_select_node)
    graph.add_node("expand_node", node_expand_node)
    graph.add_node("evaluate_node", node_evaluate_node)
    graph.add_node("backpropagate", node_backpropagate)
    graph.add_node("format_output", node_format_output)
    graph.add_node("finalize", node_finalize)

    graph.set_entry_point("initialize_search")

    graph.add_edge("initialize_search", "select_node")
    graph.add_edge("select_node", "expand_node")
    graph.add_edge("expand_node", "evaluate_node")
    graph.add_edge("evaluate_node", "backpropagate")
    graph.add_conditional_edges(
        "backpropagate",
        route_check_termination,
        {
            "select_node": "select_node",
            "format_output": "format_output",
        }
    )
    graph.add_edge("format_output", "finalize")
    graph.add_edge("finalize", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Language Agent Tree Search (LATS) — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Language Agent Tree Search (LATS) (LangGraph)")
    print(f"  Iterative problem solving using Monte Carlo Tree Search (MCTS) combined with LLM-based action generation and state evaluation.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()