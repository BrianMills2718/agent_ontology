#!/usr/bin/env python3
"""
RAG — Generated by OpenClaw Instantiation Engine (LangGraph backend)
Spec: Retrieval-Augmented Generation agent that answers questions using a document knowledge base
"""

import json
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "QueryInput": {
        "description": """User query""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "RewrittenQuery": {
        "description": """Improved query for retrieval""",
        "fields": [{"name": "rewritten_query", "type": "string"}, {"name": "original_query", "type": "string"}],
    },
    "RetrievalResult": {
        "description": """Raw retrieval results""",
        "fields": [{"name": "passages", "type": "list<string>"}, {"name": "scores", "type": "list<float>"}, {"name": "search_query", "type": "string"}],
    },
    "JudgeInput": {
        "description": """Input to relevance judge""",
        "fields": [{"name": "query", "type": "string"}, {"name": "passages", "type": "list<string>"}, {"name": "stored_chunks", "type": "list<string>"}],
    },
    "JudgeOutput": {
        "description": """Filtered passages""",
        "fields": [{"name": "filtered_passages", "type": "list<string>"}, {"name": "relevance_scores", "type": "list<float>"}],
    },
    "GeneratorInput": {
        "description": """Input to answer generator""",
        "fields": [{"name": "query", "type": "string"}, {"name": "context_passages", "type": "list<string>"}],
    },
    "GeneratedAnswer": {
        "description": """Generated answer with citations""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "citations", "type": "list<string>"}],
    },
    "Document": {
        "description": """A source document""",
        "fields": [{"name": "document_id", "type": "string"}, {"name": "document_text", "type": "string"}, {"name": "metadata", "type": "object"}],
    },
    "ChunkList": {
        "description": """List of document chunks""",
        "fields": [{"name": "chunks", "type": "list<object>"}],
    },
    "EmbeddedChunk": {
        "description": """A chunk with its embedding""",
        "fields": [{"name": "text", "type": "string"}, {"name": "embedding", "type": "list<float>"}, {"name": "metadata", "type": "object"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for RAG"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _schema_violations: int
    answer: str
    chunk_overlap: Any
    chunk_size: Any
    chunks: list
    citations: list
    context_passages: list
    document_id: str
    document_store: dict
    document_text: str
    embedding: list
    filtered_passages: list
    metadata: Any
    original_query: str
    passages: list
    query: str
    relevance_scores: list
    rewritten_query: str
    scores: list
    search_query: str
    stored_chunks: list
    text: str
    top_k: Any
    vector_store: list


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    def __init__(self, name):
        self._fallback = []
        self._collection = None
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
        else:
            print(f"    [VectorStore] using in-memory fallback")

    def read(self, query=None, top_k=5):
        if self._collection is not None:
            if query:
                results = self._collection.query(query_texts=[query], n_results=min(top_k, max(self._collection.count(), 1)))
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            self._collection.add(documents=[text], metadatas=[clean_meta], ids=[doc_id])
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})


class Store_vector_store:
    """Vector Store (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("vector_store")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)


class Store_document_store:
    """Document Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


# Store instances
_store_vector_store = Store_vector_store()
_store_document_store = Store_document_store()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_generator(user_message, output_schema=None):
    """Generator"""
    system = """You are an answer generator. You receive a user question and retrieved context passages.
Answer the question based ONLY on the provided context. If the context doesn't contain
enough information, say so. Cite which passages you used.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_query_rewriter(user_message, output_schema=None):
    """Query Rewriter"""
    system = """You rewrite user queries to improve retrieval. Given a query, produce a more specific
search query that will match relevant documents. Output only the rewritten query text.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Query Rewriter", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_relevance_judge(user_message, output_schema=None):
    """Relevance Judge"""
    system = """You judge whether retrieved passages are relevant to the query. For each passage,
output a relevance score (0-10) and whether to include it. Return a filtered list.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Relevance Judge", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_query(state: AgentState) -> dict:
    """
    Receive Query
    Accept user query
    """
    print(f"  → Receive Query")
    updates = {}

    # Logic from spec
    print(f"    Query: {state.get('query', '')}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_rewrite_query(state: AgentState) -> dict:
    """
    Rewrite Query
    Improve the query for better retrieval
    """
    print(f"  → Rewrite Query")
    updates = {}

    # Invoke: Rewrite query
    _cur = dict(state)
    _cur.update(updates)
    query_rewriter_input = build_input(_cur, "QueryInput")
    query_rewriter_msg = json.dumps(query_rewriter_input, default=str)
    query_rewriter_raw = invoke_query_rewriter(query_rewriter_msg, output_schema="RewrittenQuery")
    query_rewriter_result = parse_response(query_rewriter_raw, "RewrittenQuery")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(query_rewriter_result, "RewrittenQuery"))
    updates.update(query_rewriter_result)
    updates["rewritten_query_schema"] = query_rewriter_result
    updates["rewrite_query_result"] = query_rewriter_result
    print(f"    ← Query Rewriter: {query_rewriter_result}")

    return updates


def node_retrieve(state: AgentState) -> dict:
    """
    Retrieve
    Search the vector store for relevant passages
    """
    print(f"  → Retrieve")
    updates = {}

    # Read: Similarity search
    vector_store_data = _store_vector_store.read(key=state.get("rewritten_query", ""))
    updates["vector_store"] = vector_store_data

    # Logic from spec
    search_query = state.get("rewritten_query", state.get("query", ""))
    updates["search_query"] = search_query
    top_k = state.get("top_k", 5)
    updates["top_k"] = top_k
    results = state.get("vector_store", [])
    stored = [e.get("text", "") for e in (results or []) if e.get("text")]
    updates["stored_chunks"] = stored
    updates["passages"] = stored
    print(f"    Semantic search returned {len(stored)} chunks for: {search_query[:100]}")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_judge_relevance(state: AgentState) -> dict:
    """
    Judge Relevance
    Filter retrieved passages by relevance
    """
    print(f"  → Judge Relevance")
    updates = {}

    # Invoke: Filter by relevance
    _cur = dict(state)
    _cur.update(updates)
    relevance_judge_input = build_input(_cur, "JudgeInput")
    relevance_judge_msg = json.dumps(relevance_judge_input, default=str)
    relevance_judge_raw = invoke_relevance_judge(relevance_judge_msg, output_schema="JudgeOutput")
    relevance_judge_result = parse_response(relevance_judge_raw, "JudgeOutput")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(relevance_judge_result, "JudgeOutput"))
    updates.update(relevance_judge_result)
    updates["judge_output"] = relevance_judge_result
    updates["judge_relevance_result"] = relevance_judge_result
    print(f"    ← Relevance Judge: {relevance_judge_result}")

    return updates


def node_generate_answer(state: AgentState) -> dict:
    """
    Generate Answer
    Generate an answer from the query and relevant context
    """
    print(f"  → Generate Answer")
    updates = {}

    # Logic from spec
    passages = state.get("filtered_passages", state.get("passages", []))
    updates["context_passages"] = passages
    print(f"    Generating from {len(passages)} passages")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Flatten nested dicts
    _combined = dict(state)
    _combined.update(updates)
    for _nested_val in list(_combined.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in _combined:
                    updates[_nk] = _nv

    # Invoke: Generate answer
    _cur = dict(state)
    _cur.update(updates)
    generator_input = build_input(_cur, "GeneratorInput")
    generator_msg = json.dumps(generator_input, default=str)
    generator_raw = invoke_generator(generator_msg, output_schema="GeneratedAnswer")
    generator_result = parse_response(generator_raw, "GeneratedAnswer")
    updates["_schema_violations"] = state.get("_schema_violations", 0) + len(validate_output(generator_result, "GeneratedAnswer"))
    updates.update(generator_result)
    updates["generated_answer"] = generator_result
    updates["generate_answer_result"] = generator_result
    print(f"    ← Generator: {generator_result}")

    return updates


def node_no_answer(state: AgentState) -> dict:
    """
    No Answer
    Report that no relevant information was found
    """
    print(f"  → No Answer")
    updates = {}

    # Logic from spec
    updates["answer"] = "I could not find relevant information to answer this question."
    updates["citations"] = []
    updates["_done"] = True
    print(f"    No relevant passages found")
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_emit_answer(state: AgentState) -> dict:
    """
    Emit Answer
    Return the generated answer
    """
    print(f"  → Emit Answer")
    updates = {}

    # Logic from spec
    answer = state.get("answer", "")
    citations = state.get("citations", [])
    print(f"    Answer: {answer[:200]}")
    print(f"    Citations: {len(citations)}")
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_ingest_document(state: AgentState) -> dict:
    """
    Ingest Document
    Chunk a document and store embeddings
    """
    print(f"  → Ingest Document")
    updates = {}

    # Logic from spec
    text = state.get("document_text", "")
    chunk_size = state.get("chunk_size", 500)
    overlap = state.get("chunk_overlap", 50)
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk_text = " ".join(words[i:i + chunk_size])
        chunks.append({"text": chunk_text, "index": len(chunks)})
    updates["chunks"] = chunks
    print(f"    Chunked document into {len(chunks)} chunks")
    if updates.get("_done") or state.get("_done"):
        return updates

    # Write: Store raw document
    _cur = dict(state)
    _cur.update(updates)
    document_store_write = build_input(_cur, "Document")
    _store_document_store.write(document_store_write)

    return updates


def node_embed_and_store(state: AgentState) -> dict:
    """
    Embed & Store
    Generate embeddings for chunks and store in vector DB
    """
    print(f"  → Embed & Store")
    updates = {}

    # Logic from spec
    chunks = state.get("chunks", [])
    doc_id = state.get("document_id", "unknown")
    for chunk in chunks:
        entry = {
            "text": chunk["text"],
            "embedding": [],
            "metadata": {"source": doc_id, "index": chunk["index"]}
        }
        _store_vector_store.write(entry, key=f"{doc_id}_chunk_{chunk['index']}")
    updates["text"] = chunks[-1]["text"] if chunks else ""
    updates["embedding"] = []
    updates["metadata"] = {"source": doc_id, "index": len(chunks) - 1}
    print(f"    Stored {len(chunks)} chunks in vector store")
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

def route_check_relevance(state: AgentState) -> str:
    """Gate: Relevant results? — filtered_passages is not empty"""
    if bool(state.get("filtered_passages")):
        print(f"    → has relevant results")
        return "generate_answer"
    else:
        print(f"    → no relevant results")
        return "no_answer"


# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_query", node_receive_query)
    graph.add_node("rewrite_query", node_rewrite_query)
    graph.add_node("retrieve", node_retrieve)
    graph.add_node("judge_relevance", node_judge_relevance)
    graph.add_node("generate_answer", node_generate_answer)
    graph.add_node("no_answer", node_no_answer)
    graph.add_node("emit_answer", node_emit_answer)
    graph.add_node("ingest_document", node_ingest_document)
    graph.add_node("embed_and_store", node_embed_and_store)

    graph.set_entry_point("receive_query")

    graph.add_edge("receive_query", "rewrite_query")
    graph.add_edge("rewrite_query", "retrieve")
    graph.add_edge("retrieve", "judge_relevance")
    graph.add_conditional_edges(
        "judge_relevance",
        route_check_relevance,
        {
            "no_answer": "no_answer",
            "generate_answer": "generate_answer",
        }
    )
    graph.add_edge("generate_answer", "emit_answer")
    graph.add_edge("no_answer", END)
    graph.add_edge("emit_answer", END)
    graph.add_edge("ingest_document", "embed_and_store")
    graph.add_edge("embed_and_store", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """RAG — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  RAG (LangGraph)")
    print(f"  Retrieval-Augmented Generation agent that answers questions using a document knowledge base")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()