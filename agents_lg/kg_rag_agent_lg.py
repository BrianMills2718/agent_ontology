#!/usr/bin/env python3
"""
Knowledge Graph RAG — Generated by Agent Ontology Instantiation Engine (LangGraph backend)
Spec: RAG agent that queries a knowledge graph via SPARQL instead of vector search. Demonstrates knowledge_graph store type with formalism and reasoner properties.

"""

import json
import operator
import os
import sys
import time
from datetime import datetime
from typing import Any, TypedDict, Annotated

from langgraph.graph import StateGraph, END

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model


def _get_chat_model(model, temperature=0.7, max_tokens=4096):
    """Get a LangChain ChatModel instance for the given model name."""
    if _OPENROUTER_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=_openrouter_model_id(model),
            temperature=temperature,
            max_tokens=max_tokens,
            base_url="https://openrouter.ai/api/v1",
            api_key=_OPENROUTER_API_KEY,
        )
    if model.startswith("gemini"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=model, temperature=temperature,
                                      max_output_tokens=max_tokens,
                                      google_api_key=os.environ.get("GEMINI_API_KEY", ""))
    elif model.startswith("claude") or model.startswith("anthropic"):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model=model, temperature=temperature, max_tokens=max_tokens)
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    from langchain_core.messages import SystemMessage, HumanMessage
    for attempt in range(retries):
        try:
            chat = _get_chat_model(model, temperature, max_tokens)
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
            response = chat.invoke(messages)
            content = response.content
            # LangChain may return content as a list of blocks (e.g. Gemini)
            if isinstance(content, list):
                content = " ".join(b.get("text", str(b)) if isinstance(b, dict) else str(b) for b in content)
            return content
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "QueryGenInput": {
        "description": """""",
        "fields": [{"name": "question", "type": "string"}, {"name": "kg_schema", "type": "string"}],
    },
    "KGQuery": {
        "description": """""",
        "fields": [{"name": "entities", "type": "list<string>"}, {"name": "relation", "type": "string"}],
    },
    "AnswerInput": {
        "description": """""",
        "fields": [{"name": "question", "type": "string"}, {"name": "results", "type": "list<string>"}],
    },
    "AnswerOutput": {
        "description": """""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "sources", "type": "list<string>"}],
    },
}


def validate_output(data, schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return dict(state)
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state:
            result[fname] = state[fname]
        else:
            for _k, _v in state.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    import re as _re
    text = response.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}

# ═══════════════════════════════════════════════════════════
# State TypedDict
# ═══════════════════════════════════════════════════════════

class AgentState(TypedDict, total=False):
    """Typed state for Knowledge Graph RAG"""
    _canned_responses: list
    _done: bool
    _iteration: int
    _kg_triples: Any
    _schema_violations: Annotated[int, operator.add]
    answer: str
    entities: list
    invoke_query_gen_result: Any
    kg_schema: str
    knowledge_graph: dict
    query: Any
    question: str
    relation: str
    results: list
    sources: list
    synthesize_answer_result: Any


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_knowledge_graph:
    """Knowledge Graph (knowledge_graph)"""
    def __init__(self):
        self._triples = []
        try:
            import networkx as nx
            self._graph = nx.DiGraph()
            self._has_nx = True
        except ImportError:
            self._graph = None
            self._has_nx = False
        self.data = {}

    def read(self, key=None):
        if key is None:
            return self._triples
        # Search triples where key matches subject, predicate, or object
        matches = [t for t in self._triples if key in str(t[0]) or key in str(t[1]) or key in str(t[2])]
        if self._has_nx and not matches:
            if self._graph.has_node(key):
                matches = [(key, d.get("predicate","related"), n) for n, d in self._graph[key].items()]
        return matches

    def write(self, value, key=None):
        if isinstance(value, tuple) and len(value) == 3:
            self._triples.append(value)
            if self._has_nx:
                self._graph.add_edge(value[0], value[2], predicate=value[1])
        elif isinstance(value, dict):
            t = (value.get("subject",""), value.get("predicate",""), value.get("object",""))
            self._triples.append(t)
            if self._has_nx:
                self._graph.add_edge(t[0], t[2], predicate=t[1])
        else:
            self._triples.append(("_", "_", str(value)))


# Store instances
_store_knowledge_graph = Store_knowledge_graph()


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_query_agent(user_message, output_schema=None):
    """Query Generator"""
    system = """You are a knowledge graph query planner. Given a natural language question and a knowledge graph schema (entities and relations), extract the key entities to look up. Output the entity names that should be searched in the knowledge graph.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Query Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_answer_agent(user_message, output_schema=None):
    """Answer Synthesizer"""
    system = """You synthesize answers from knowledge graph query results. Given the original question and SPARQL query results (as triples), generate a clear, factual answer grounded in the retrieved facts.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Answer Synthesizer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Node Functions
# ═══════════════════════════════════════════════════════════

def node_receive_query(state: AgentState) -> dict:
    """
    Receive Query
    """
    print(f"  → Receive Query")
    updates = {}

    # Logic from spec
    updates["question"] = state.get("query", state.get("question", ""))
    updates["kg_schema"] = "Entities: Person, Organization, Location. Relations: worksAt, locatedIn, foundedBy"
    for triple in state.get("_kg_triples", []):
        _store_knowledge_graph.write(tuple(triple))
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_generate_sparql(state: AgentState) -> dict:
    """
    NL → SPARQL
    """
    print(f"  → NL → SPARQL")
    updates = {}

    return updates


def node_invoke_query_gen(state: AgentState) -> dict:
    """
    Invoke Query Generator
    """
    print(f"  → Invoke Query Generator")
    updates = {}

    # Invoke: query_agent
    _cur = dict(state)
    _cur.update(updates)
    query_agent_input = build_input(_cur, "QueryGenInput")
    query_agent_msg = json.dumps(query_agent_input, default=str)
    query_agent_raw = invoke_query_agent(query_agent_msg, output_schema="KGQuery")
    query_agent_result = parse_response(query_agent_raw, "KGQuery")
    updates["_schema_violations"] = len(validate_output(query_agent_result, "KGQuery"))
    updates.update(query_agent_result)
    updates["kg_query"] = query_agent_result
    updates["invoke_query_gen_result"] = query_agent_result
    print(f"    ← Query Generator: {query_agent_result}")

    return updates


def node_execute_query(state: AgentState) -> dict:
    """
    Execute KG Query
    """
    print(f"  → Execute KG Query")
    updates = {}

    # Read: 
    knowledge_graph_data = _store_knowledge_graph.read()
    updates["knowledge_graph"] = knowledge_graph_data

    # Logic from spec
    entities = state.get("entities", [])
    relation = state.get("relation", "")
    all_results = []
    for entity in entities:
        matches = _store_knowledge_graph.read(entity)
        all_results.extend(matches)
    if relation and not all_results:
        all_results = _store_knowledge_graph.read(relation)
    updates["results"] = [str(r) for r in all_results] if all_results else ["No results found"]
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


def node_synthesize_answer(state: AgentState) -> dict:
    """
    Synthesize Answer
    """
    print(f"  → Synthesize Answer")
    updates = {}

    # Invoke: answer_agent
    _cur = dict(state)
    _cur.update(updates)
    answer_agent_input = build_input(_cur, "AnswerInput")
    answer_agent_msg = json.dumps(answer_agent_input, default=str)
    answer_agent_raw = invoke_answer_agent(answer_agent_msg, output_schema="AnswerOutput")
    answer_agent_result = parse_response(answer_agent_raw, "AnswerOutput")
    updates["_schema_violations"] = len(validate_output(answer_agent_result, "AnswerOutput"))
    updates.update(answer_agent_result)
    updates["answer_output"] = answer_agent_result
    updates["synthesize_answer_result"] = answer_agent_result
    print(f"    ← Answer Synthesizer: {answer_agent_result}")

    return updates


def node_emit_answer(state: AgentState) -> dict:
    """
    Emit Answer
    """
    print(f"  → Emit Answer")
    updates = {}

    # Logic from spec
    updates["_done"] = True
    if updates.get("_done") or state.get("_done"):
        return updates

    return updates


# ═══════════════════════════════════════════════════════════
# Gate Routing Functions
# ═══════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════
# Graph Construction
# ═══════════════════════════════════════════════════════════

def build_graph():
    graph = StateGraph(AgentState)

    graph.add_node("receive_query", node_receive_query)
    graph.add_node("generate_sparql", node_generate_sparql)
    graph.add_node("invoke_query_gen", node_invoke_query_gen)
    graph.add_node("execute_query", node_execute_query)
    graph.add_node("synthesize_answer", node_synthesize_answer)
    graph.add_node("emit_answer", node_emit_answer)

    graph.set_entry_point("receive_query")

    graph.add_edge("receive_query", "generate_sparql")
    graph.add_edge("generate_sparql", "invoke_query_gen")
    graph.add_edge("invoke_query_gen", "execute_query")
    graph.add_edge("execute_query", "synthesize_answer")
    graph.add_edge("synthesize_answer", "emit_answer")
    graph.add_edge("emit_answer", END)

    return graph.compile()


# ═══════════════════════════════════════════════════════════
# Entry Point
# ═══════════════════════════════════════════════════════════

class _StateCompat:
    """Wrapper to make LangGraph state compatible with test harness."""
    def __init__(self, state_dict):
        self.data = {k: v for k, v in state_dict.items() if not k.startswith("_")}
        self.data.update({k: v for k, v in state_dict.items() if k.startswith("_")})
        self.iteration = state_dict.get("_iteration", 0)
        self.schema_violations = state_dict.get("_schema_violations", 0)

    def get(self, key, default=None):
        return self.data.get(key, default)


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Knowledge Graph RAG — LangGraph execution"""
    app = build_graph()

    # Build initial state
    state = {}
    if initial_data:
        state.update(initial_data)
    state["_iteration"] = 0
    state["_schema_violations"] = 0
    state["_done"] = False

    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Knowledge Graph RAG (LangGraph)")
    print(f"  RAG agent that queries a knowledge graph via SPARQL instead of vector search. Demonstrates knowledge_graph store type with formalism and reasoner properties.")
    print(f"════════════════════════════════════════════════════════════\n")

    # LangGraph recursion limit maps roughly to our MAX_ITERATIONS
    try:
        final_state = app.invoke(state, {"recursion_limit": MAX_ITERATIONS * 3})
    except Exception as e:
        print(f"\n  [ERROR] {type(e).__name__}: {e}")
        final_state = state

    _clean_exit = True
    dump_trace(iterations=final_state.get("_iteration", 0), clean_exit=_clean_exit)
    print(f"\nFinal state keys: {list(final_state.keys())}")
    return _StateCompat(final_state)


if __name__ == "__main__":
    run()