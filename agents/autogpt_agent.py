#!/usr/bin/env python3
"""
AutoGPT — Generated by OpenClaw Instantiation Engine
Spec: Goal-driven autonomous agent with planning, self-criticism, and iterative execution
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "PlanInput": {
        "description": """Input to the planner""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "constraints", "type": "list<string>"}],
    },
    "Plan": {
        "description": """A plan of action steps""",
        "fields": [{"name": "plan", "type": "list<string>"}, {"name": "objective", "type": "string"}],
    },
    "ThinkInput": {
        "description": """Input to the thinker""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "current_step", "type": "string"}, {"name": "results", "type": "list<string>"}, {"name": "stored_memories", "type": "list<string>"}, {"name": "feedback", "type": "string"}],
    },
    "Thought": {
        "description": """Reasoning output""",
        "fields": [{"name": "reasoning", "type": "string"}, {"name": "proposed_action", "type": "string"}],
    },
    "CriticInput": {
        "description": """Input to the critic""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "current_step", "type": "string"}, {"name": "reasoning", "type": "string"}, {"name": "proposed_action", "type": "string"}],
    },
    "Criticism": {
        "description": """Critic evaluation""",
        "fields": [{"name": "rating", "type": "integer"}, {"name": "approved", "type": "boolean"}, {"name": "improvements", "type": "string"}],
    },
    "ActionInput": {
        "description": """Input to executor""",
        "fields": [{"name": "proposed_action", "type": "string"}, {"name": "current_step", "type": "string"}, {"name": "objective", "type": "string"}],
    },
    "ActionResult": {
        "description": """Result of action execution""",
        "fields": [{"name": "action_result", "type": "string"}, {"name": "success", "type": "boolean"}],
    },
    "MemoryEntry": {
        "description": """Memory entry for vector store""",
        "fields": [{"name": "text", "type": "string"}, {"name": "embedding", "type": "list<float>"}, {"name": "metadata", "type": "object"}],
    },
    "WorkspaceEntry": {
        "description": """Workspace key-value entry""",
        "fields": [{"name": "key", "type": "string"}, {"name": "value", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

# ── ChromaDB Vector Store (falls back to in-memory list) ──
try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    """Vector store backed by ChromaDB with in-memory fallback."""
    def __init__(self, name):
        self._fallback = []
        self._collection = None
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
            print(f"    [ChromaDB] collection '{name}' ready")
        else:
            print(f"    [VectorStore] using in-memory fallback (pip install chromadb for semantic search)")

    def read(self, query=None, top_k=5):
        """Read all entries, or query for similar ones."""
        if self._collection is not None:
            if query:
                results = self._collection.query(query_texts=[query], n_results=min(top_k, max(self._collection.count(), 1)))
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        """Write an entry. value should have "text" and optionally "metadata"."""
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        # ChromaDB metadata values must be str, int, float, or bool
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            self._collection.add(documents=[text], metadatas=[clean_meta], ids=[doc_id])
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})



class Store_memory_store:
    """Agent Memory (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("memory_store")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)


class Store_workspace:
    """Workspace (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_planner(user_message, output_schema=None):
    """Planner"""
    system = """You are an autonomous AI agent planner. Given an objective, break it down into
a concrete plan of 3-7 actionable steps. Each step should be specific enough to execute.
Consider dependencies between steps. Return the plan as a JSON list of steps.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Planner", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_thinker(user_message, output_schema=None):
    """Thinker"""
    system = """You are the reasoning component of an autonomous agent. Given the current goal,
plan, completed results, and current step, reason about what specific action to take.
Consider what information you have, what you need, and the best approach.
Output your thought process and a proposed action.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Thinker", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_critic(user_message, output_schema=None):
    """Critic"""
    system = """You are a self-critic for an autonomous agent. Review the proposed thought and action.
Check for: logical errors, missing information, better alternatives, potential risks.
Rate the proposal 1-10 and suggest improvements. If rating >= 7, approve. Otherwise, reject
with specific feedback.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Critic", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_executor(user_message, output_schema=None):
    """Executor"""
    system = """You execute a specific action. You receive the action description and any context.
Perform the action to the best of your ability and return the result.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Executor", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for AutoGPT"""
    def __init__(self):
        self.memory_store = Store_memory_store()
        self.workspace = Store_workspace()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_set_goal(state):
    """
    Set Goal
    Accept the user's objective and initialize the agent
    """
    print(f"  → Set Goal")

    # Logic from spec
    state.data["completed_steps"] = []
    state.data["current_step_idx"] = 0
    state.data["criticism_count"] = 0
    state.data["max_criticism_retries"] = 3
    state.data["results"] = []
    print(f"    Goal: {state.data.get('objective', '')}")
    if state.data.get("_done"):
        return state

    return state


def process_decompose(state):
    """
    Decompose
    Break the objective into a plan of concrete steps
    """
    print(f"  → Decompose")

    # Invoke: Create plan
    planner_input = build_input(state, "PlanInput")
    planner_msg = json.dumps(planner_input, default=str)
    planner_raw = invoke_planner(planner_msg, output_schema="Plan")
    planner_result = parse_response(planner_raw, "Plan")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(planner_result, "Plan"))
    state.data.update(planner_result)
    state.data["plan_schema"] = planner_result
    state.data["decompose_result"] = planner_result
    print(f"    ← Planner: {planner_result}")

    return state


def process_think(state):
    """
    Think
    Reason about the current step and propose an action
    """
    print(f"  → Think")

    # Read: Read memory context
    memory_store_data = state.memory_store.read()
    state.data["memory_store"] = memory_store_data

    # Logic from spec
    plan = state.data.get("plan", [])
    state.data["total_steps"] = len(plan)
    idx = state.data.get("current_step_idx", 0)
    if idx < len(plan):
        state.data["current_step"] = plan[idx]
    else:
        state.data["current_step"] = "Review and finalize results"
    state.data["stored_memories"] = [e.get("text", "") for e in state.data.get("memory_store", []) if e.get("text")][-5:]
    print(f"    Step {idx + 1}/{len(plan)}: {state.data.get('current_step', '')[:100]}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Reason about step
    thinker_input = build_input(state, "ThinkInput")
    thinker_msg = json.dumps(thinker_input, default=str)
    thinker_raw = invoke_thinker(thinker_msg, output_schema="Thought")
    thinker_result = parse_response(thinker_raw, "Thought")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(thinker_result, "Thought"))
    state.data.update(thinker_result)
    state.data["thought"] = thinker_result
    state.data["think_result"] = thinker_result
    print(f"    ← Thinker: {thinker_result}")

    return state


def process_criticize(state):
    """
    Criticize
    Self-critique the proposed thought and action
    """
    print(f"  → Criticize")

    # Invoke: Evaluate proposal
    critic_input = build_input(state, "CriticInput")
    critic_msg = json.dumps(critic_input, default=str)
    critic_raw = invoke_critic(critic_msg, output_schema="Criticism")
    critic_result = parse_response(critic_raw, "Criticism")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(critic_result, "Criticism"))
    state.data.update(critic_result)
    state.data["criticism"] = critic_result
    state.data["criticize_result"] = critic_result
    print(f"    ← Critic: {critic_result}")

    return state


def process_critic_gate(state):
    """
    Critic approved?
    """
    print(f"  → Critic approved?")

    # Gate: rating >= 7
    # Branch: rejected (rating < 7) → handle_rejection
    # Branch: approved (rating >= 7) → act

    if (state.data.get("rating", 0)) >= 7:
        print(f"    → approved (rating >= 7)")
        return "act"
    else:
        print(f"    → rejected (rating < 7)")
        return "handle_rejection"


def process_handle_rejection(state):
    """
    Handle Rejection
    Process critic feedback and retry or skip
    """
    print(f"  → Handle Rejection")

    # Logic from spec
    count = state.data.get("criticism_count", 0) + 1
    state.data["criticism_count"] = count
    max_retries = state.data.get("max_criticism_retries", 3)
    if count >= max_retries:
        print(f"    Max retries ({max_retries}) reached, proceeding anyway")
        state.data["criticism_count"] = 0
    else:
        state.data["feedback"] = state.data.get("improvements", "Try a different approach")
        print(f"    Retry {count}/{max_retries}: {state.data.get('improvements', '')[:100]}")
    if state.data.get("_done"):
        return state

    return state


def process_retry_or_proceed(state):
    """
    Retry or proceed?
    """
    print(f"  → Retry or proceed?")

    # Gate: criticism_count >= max_criticism_retries
    # Branch: under retry limit → think
    # Branch: at retry limit → act

    if (state.data.get("criticism_count", 0)) >= (state.data.get("max_criticism_retries", 0)):
        print(f"    → at retry limit")
        return "act"
    else:
        print(f"    → under retry limit")
        return "think"


def process_act(state):
    """
    Act
    Execute the approved action
    """
    print(f"  → Act")

    # Logic from spec
    action = state.data.get("proposed_action", "")
    print(f"    Executing: {action[:100]}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Execute action
    executor_input = build_input(state, "ActionInput")
    executor_msg = json.dumps(executor_input, default=str)
    executor_raw = invoke_executor(executor_msg, output_schema="ActionResult")
    executor_result = parse_response(executor_raw, "ActionResult")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(executor_result, "ActionResult"))
    state.data.update(executor_result)
    state.data["action_result_schema"] = executor_result
    state.data["act_result"] = executor_result
    print(f"    ← Executor: {executor_result}")

    return state


def process_observe(state):
    """
    Observe
    Process the action result and update memory
    """
    print(f"  → Observe")

    # Logic from spec
    result = state.data.get("action_result", "")
    results = state.data.get("results", [])
    results.append(result)
    state.data["results"] = results
    state.data["text"] = result
    state.data["embedding"] = []
    state.data["metadata"] = {"step": state.data.get("current_step_idx", 0), "action": state.data.get("proposed_action", "")}
    state.data["current_step_idx"] = state.data.get("current_step_idx", 0) + 1
    state.data["criticism_count"] = 0
    print(f"    Result recorded. Completed {state.data['current_step_idx']} steps.")
    if state.data.get("_done"):
        return state

    # Write: Store memory
    memory_store_write = build_input(state, "MemoryEntry")
    state.memory_store.write(memory_store_write)

    # Write: Update workspace
    workspace_write = build_input(state, "WorkspaceEntry")
    state.workspace.write(workspace_write)

    return state


def process_check_done(state):
    """
    All steps done?
    """
    print(f"  → All steps done?")

    # Gate: current_step_idx >= total_steps
    # Branch: not done → think
    # Branch: all done → synthesize

    if (state.data.get("current_step_idx", 0)) >= (state.data.get("total_steps", 0)):
        print(f"    → all done")
        return "synthesize"
    else:
        print(f"    → not done")
        return "think"


def process_synthesize(state):
    """
    Synthesize
    Combine all results into a final output
    """
    print(f"  → Synthesize")

    # Logic from spec
    results = state.data.get("results", [])
    print(f"    Synthesizing {len(results)} results")
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "set_goal": process_set_goal,
    "decompose": process_decompose,
    "think": process_think,
    "criticize": process_criticize,
    "critic_gate": process_critic_gate,
    "handle_rejection": process_handle_rejection,
    "retry_or_proceed": process_retry_or_proceed,
    "act": process_act,
    "observe": process_observe,
    "check_done": process_check_done,
    "synthesize": process_synthesize,
}

TRANSITIONS = {
    "set_goal": "decompose",
    "decompose": "think",
    "think": "criticize",
    "criticize": "critic_gate",
    # "critic_gate": determined by gate logic
    "handle_rejection": "retry_or_proceed",
    # "retry_or_proceed": determined by gate logic
    "act": "observe",
    "observe": "check_done",
    # "check_done": determined by gate logic
    "synthesize": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """AutoGPT — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "set_goal"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  AutoGPT")
    print(f"  Goal-driven autonomous agent with planning, self-criticism, and iterative execution")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['critic_gate', 'retry_or_proceed', 'check_done']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()