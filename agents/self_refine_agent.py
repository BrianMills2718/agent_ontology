#!/usr/bin/env python3
"""
Self Refine — Generated by OpenClaw Instantiation Engine
Spec: Self-Refine agent with a generator-critic loop: generates output, critiques it, and iteratively refines based on feedback until quality threshold is met or max rounds reached.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TaskInput": {
        "description": """The task to generate output for""",
        "fields": [{"name": "task", "type": "string"}],
    },
    "GeneratorInput": {
        "description": """Input to the generator agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "previous_output", "type": "string"}, {"name": "specific_feedback", "type": "string"}, {"name": "refinement_round", "type": "integer"}],
    },
    "GeneratorOutput": {
        "description": """Output from the generator agent""",
        "fields": [{"name": "output_text", "type": "string"}, {"name": "changes_made", "type": "string"}],
    },
    "CriticInput": {
        "description": """Input to the critic agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "output_text", "type": "string"}, {"name": "refinement_round", "type": "integer"}],
    },
    "CriticOutput": {
        "description": """Output from the critic agent""",
        "fields": [{"name": "quality_score", "type": "integer"}, {"name": "strengths", "type": "list<string>"}, {"name": "weaknesses", "type": "list<string>"}, {"name": "specific_feedback", "type": "string"}],
    },
    "FinalOutput": {
        "description": """The finalized output with quality metadata""",
        "fields": [{"name": "final_output", "type": "string"}, {"name": "final_score", "type": "integer"}, {"name": "total_rounds", "type": "integer"}, {"name": "feedback_history", "type": "list<FeedbackEntry>"}],
    },
    "FeedbackEntry": {
        "description": """A single round of feedback from the critic""",
        "fields": [{"name": "round_number", "type": "integer"}, {"name": "quality_score", "type": "integer"}, {"name": "specific_feedback", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_generator(user_message, output_schema=None):
    """Generator Agent"""
    system = """You are a skilled content generator. When given a task, produce a high-quality output.
If you also receive feedback from a previous round, incorporate that feedback to improve
your output. Focus on addressing every piece of specific feedback provided.
Output JSON with "output_text" (string with your generated content) and
"changes_made" (string summarizing what you changed from the previous version, or "initial" if first attempt).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Generator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_critic(user_message, output_schema=None):
    """Critic Agent"""
    system = """You are a demanding but fair critic. Given a task description and a generated output,
evaluate the quality of the output on a scale of 1-10 (10 = excellent, publication-ready;
7 = good enough; below 7 = needs improvement). Provide specific, actionable feedback
listing exactly what should be improved. Output JSON with "quality_score" (integer 1-10),
"strengths" (list of strings), "weaknesses" (list of strings), and
"specific_feedback" (string with detailed improvement instructions).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Critic Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Self Refine"""
    def __init__(self):
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_task(state):
    """
    Receive Task
    Accept the task input and initialize refinement state
    """
    print(f"  → Receive Task")

    # Logic from spec
    state.data["refinement_round"] = 0
    state.data["max_rounds"] = 3
    state.data["quality_threshold"] = 7
    state.data["feedback_history"] = []
    state.data["current_output"] = ""
    print(f"    Task: {state.data.get('task', '')[:100]}")
    print(f"    Config: max_rounds={state.data['max_rounds']}, threshold={state.data['quality_threshold']}")
    if state.data.get("_done"):
        return state

    return state


def process_generate(state):
    """
    Generate Output
    Invoke the generator agent to produce or refine output
    """
    print(f"  → Generate Output")

    # Logic from spec
    round_num = state.data.get("refinement_round", 0)
    if round_num == 0:
        print(f"    Generating initial output...")
    else:
        print(f"    Refining output (round {round_num})...")
        print(f"    Using feedback: {state.data.get('specific_feedback', '')[:80]}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate or refine output
    generator_input = build_input(state, "GeneratorInput")
    generator_msg = json.dumps(generator_input, default=str)
    generator_raw = invoke_generator(generator_msg, output_schema="GeneratorOutput")
    generator_result = parse_response(generator_raw, "GeneratorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(generator_result, "GeneratorOutput"))
    state.data.update(generator_result)
    state.data["generator_output"] = generator_result
    state.data["generate_result"] = generator_result
    print(f"    ← Generator Agent: {generator_result}")

    return state


def process_critique(state):
    """
    Critique Output
    Invoke the critic agent to evaluate the generated output
    """
    print(f"  → Critique Output")

    # Logic from spec
    current = state.data.get("current_output", "")
    print(f"    Critiquing output ({len(current)} chars)...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Evaluate output quality
    critic_input = build_input(state, "CriticInput")
    critic_msg = json.dumps(critic_input, default=str)
    critic_raw = invoke_critic(critic_msg, output_schema="CriticOutput")
    critic_result = parse_response(critic_raw, "CriticOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(critic_result, "CriticOutput"))
    state.data.update(critic_result)
    state.data["critic_output"] = critic_result
    state.data["critique_result"] = critic_result
    print(f"    ← Critic Agent: {critic_result}")

    return state


def process_check_quality(state):
    """
    Quality >= threshold?
    """
    print(f"  → Quality >= threshold?")

    # Gate: quality_score >= quality_threshold
    # Branch: score >= 7 → finalize
    # Branch: score < 7 → check_rounds

    if (state.data.get("quality_score", 0)) >= (state.data.get("quality_threshold", 0)):
        print(f"    → score >= 7")
        return "finalize"
    else:
        print(f"    → score < 7")
        return "check_rounds"


def process_check_rounds(state):
    """
    Rounds remaining?
    """
    print(f"  → Rounds remaining?")

    # Gate: refinement_round < max_rounds
    # Branch: rounds remaining → refine
    # Branch: max rounds reached → finalize

    if (state.data.get("refinement_round", 0)) < (state.data.get("max_rounds", 0)):
        print(f"    → rounds remaining")
        return "refine"
    else:
        print(f"    → max rounds reached")
        return "finalize"


def process_refine(state):
    """
    Refine
    Prepare feedback for the generator and increment the round counter
    """
    print(f"  → Refine")

    # Logic from spec
    state.data["refinement_round"] = state.data.get("refinement_round", 0) + 1
    # Record feedback for history
    feedback_entry = {
        "round": state.data["refinement_round"],
        "quality_score": state.data.get("quality_score", 0),
        "specific_feedback": state.data.get("specific_feedback", ""),
    }
    history = state.data.get("feedback_history", [])
    history.append(feedback_entry)
    state.data["feedback_history"] = history
    print(f"    Preparing refinement round {state.data['refinement_round']}/{state.data['max_rounds']}")
    if state.data.get("_done"):
        return state

    return state


def process_finalize(state):
    """
    Finalize
    Produce the final output with quality metadata
    """
    print(f"  → Finalize")

    # Logic from spec
    score = state.data.get("quality_score", 0)
    rounds = state.data.get("refinement_round", 0)
    threshold = state.data.get("quality_threshold", 7)
    if score >= threshold:
        print(f"    Quality threshold met (score={score}). Finalizing after {rounds} refinement(s).")
    else:
        print(f"    Max rounds reached (score={score}). Finalizing with best effort after {rounds} round(s).")
    state.data["final_output"] = state.data.get("current_output", "")
    state.data["final_score"] = score
    state.data["total_rounds"] = rounds
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_task": process_receive_task,
    "generate": process_generate,
    "critique": process_critique,
    "check_quality": process_check_quality,
    "check_rounds": process_check_rounds,
    "refine": process_refine,
    "finalize": process_finalize,
}

TRANSITIONS = {
    "receive_task": "generate",
    "generate": "critique",
    "critique": "check_quality",
    # "check_quality": determined by gate logic
    # "check_rounds": determined by gate logic
    "refine": "generate",  # loop: Refinement loop
    "finalize": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Self Refine — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_task"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Self Refine")
    print(f"  Self-Refine agent with a generator-critic loop: generates output, critiques it, and iteratively refines based on feedback until quality threshold is met or max rounds reached.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_quality', 'check_rounds']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()