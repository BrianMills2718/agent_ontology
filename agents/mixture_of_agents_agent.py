#!/usr/bin/env python3
"""
Mixture of Agents — Generated by OpenClaw Instantiation Engine
Spec: Mixture of Agents (MoA) architecture: multiple diverse proposer agents generate responses in parallel, then an aggregator synthesizes the best composite response. Supports multi-layer iterative refinement with quality gating.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "MoAInput": {
        "description": """User query input for the Mixture of Agents system""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "ProposerInput": {
        "description": """Input to each proposer agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "prior_responses", "type": "list<string>"}, {"name": "current_layer", "type": "integer"}],
    },
    "ProposerOutput": {
        "description": """Output from a proposer agent""",
        "fields": [{"name": "response", "type": "string"}, {"name": "confidence", "type": "float"}],
    },
    "AggregatorInput": {
        "description": """Input to the aggregator agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "proposals", "type": "list<string>"}, {"name": "current_layer", "type": "integer"}, {"name": "prior_synthesis", "type": "string"}],
    },
    "AggregatorOutput": {
        "description": """Output from the aggregator agent""",
        "fields": [{"name": "synthesized_response", "type": "string"}, {"name": "quality_score", "type": "integer"}, {"name": "sources_used", "type": "list<string>"}, {"name": "reasoning", "type": "string"}],
    },
    "LayerSnapshot": {
        "description": """Snapshot of a single MoA layer's results""",
        "fields": [{"name": "layer", "type": "integer"}, {"name": "proposals", "type": "list<string>"}, {"name": "synthesized_response", "type": "string"}, {"name": "quality_score", "type": "integer"}, {"name": "sources_used", "type": "list<string>"}],
    },
    "MoAResult": {
        "description": """Final result from the Mixture of Agents process""",
        "fields": [{"name": "final_response", "type": "string"}, {"name": "final_quality", "type": "integer"}, {"name": "total_layers", "type": "integer"}, {"name": "layer_history", "type": "list<LayerSnapshot>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_response_store:
    """Layer Response Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_proposer_analytical(user_message, output_schema=None):
    """Analytical Proposer"""
    system = """You are an analytical reasoning agent. Given a query, approach it with rigorous
logical analysis. Break down the problem systematically, consider edge cases,
and provide a well-structured analytical response.
If prior layer responses are provided, build upon their insights while
correcting any errors you identify.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Analytical Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_proposer_creative(user_message, output_schema=None):
    """Creative Proposer"""
    system = """You are a creative thinking agent. Given a query, approach it with lateral
thinking, analogies, and novel perspectives. Consider unconventional angles
and generate insightful connections.
If prior layer responses are provided, offer fresh perspectives that complement them.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Creative Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_proposer_critical(user_message, output_schema=None):
    """Critical Proposer"""
    system = """You are a critical evaluation agent. Given a query, approach it by questioning
assumptions, identifying potential flaws, and stress-testing reasoning.
If prior layer responses are provided, critique their weaknesses while
offering improved alternatives.
Output JSON with "response" (string) and "confidence" (float 0-1).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Critical Proposer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_aggregator_agent(user_message, output_schema=None):
    """Aggregator Agent"""
    system = """You are an aggregator agent in a Mixture of Agents architecture.
You receive multiple diverse responses to the same query from different
proposer agents. Your job is to:
1. Identify the strongest elements from each response.
2. Resolve any contradictions by reasoning about which is correct.
3. Synthesize a single, comprehensive response that is better than any individual one.
4. Assign a quality score (1-10) to the synthesized result.
Output JSON with "synthesized_response" (string), "quality_score" (integer 1-10),
"sources_used" (list of strings naming which proposers contributed key elements),
and "reasoning" (string explaining synthesis decisions).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Aggregator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Mixture of Agents"""
    def __init__(self):
        self.response_store = Store_response_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_query(state):
    """
    Receive Query
    Accept user query and initialize MoA layer state
    """
    print(f"  → Receive Query")

    # Logic from spec
    state.data["current_layer"] = 0
    state.data["max_layers"] = 3
    state.data["quality_threshold"] = 7
    state.data["layer_history"] = []
    state.data["prior_responses"] = []
    print(f"    Query: {state.data.get('query', '')[:100]}")
    print(f"    Config: max_layers={state.data['max_layers']}, quality_threshold={state.data['quality_threshold']}")
    if state.data.get("_done"):
        return state

    return state


def process_increment_layer(state):
    """
    Increment Layer
    Advance the current layer counter and prepare proposer context
    """
    print(f"  → Increment Layer")

    # Logic from spec
    layer = state.data.get("current_layer", 0) + 1
    state.data["current_layer"] = layer
    print(f"    Starting layer {layer}/{state.data.get('max_layers', 3)}")
    if state.data.get("_done"):
        return state

    return state


def process_call_analytical(state):
    """
    Call Analytical Proposer
    Invoke the analytical proposer agent
    """
    print(f"  → Call Analytical Proposer")

    # Logic from spec
    print(f"    Analytical proposer processing (layer {state.data.get('current_layer', 1)})...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate analytical response
    proposer_analytical_input = build_input(state, "ProposerInput")
    proposer_analytical_msg = json.dumps(proposer_analytical_input, default=str)
    proposer_analytical_raw = invoke_proposer_analytical(proposer_analytical_msg, output_schema="ProposerOutput")
    proposer_analytical_result = parse_response(proposer_analytical_raw, "ProposerOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(proposer_analytical_result, "ProposerOutput"))
    state.data.update(proposer_analytical_result)
    state.data["proposer_output"] = proposer_analytical_result
    state.data["call_analytical_result"] = proposer_analytical_result
    print(f"    ← Analytical Proposer: {proposer_analytical_result}")

    return state


def process_call_creative(state):
    """
    Call Creative Proposer
    Invoke the creative proposer agent
    """
    print(f"  → Call Creative Proposer")

    # Logic from spec
    print(f"    Creative proposer processing (layer {state.data.get('current_layer', 1)})...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate creative response
    proposer_creative_input = build_input(state, "ProposerInput")
    proposer_creative_msg = json.dumps(proposer_creative_input, default=str)
    proposer_creative_raw = invoke_proposer_creative(proposer_creative_msg, output_schema="ProposerOutput")
    proposer_creative_result = parse_response(proposer_creative_raw, "ProposerOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(proposer_creative_result, "ProposerOutput"))
    state.data.update(proposer_creative_result)
    state.data["proposer_output"] = proposer_creative_result
    state.data["call_creative_result"] = proposer_creative_result
    print(f"    ← Creative Proposer: {proposer_creative_result}")

    return state


def process_call_critical(state):
    """
    Call Critical Proposer
    Invoke the critical proposer agent
    """
    print(f"  → Call Critical Proposer")

    # Logic from spec
    print(f"    Critical proposer processing (layer {state.data.get('current_layer', 1)})...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate critical response
    proposer_critical_input = build_input(state, "ProposerInput")
    proposer_critical_msg = json.dumps(proposer_critical_input, default=str)
    proposer_critical_raw = invoke_proposer_critical(proposer_critical_msg, output_schema="ProposerOutput")
    proposer_critical_result = parse_response(proposer_critical_raw, "ProposerOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(proposer_critical_result, "ProposerOutput"))
    state.data.update(proposer_critical_result)
    state.data["proposer_output"] = proposer_critical_result
    state.data["call_critical_result"] = proposer_critical_result
    print(f"    ← Critical Proposer: {proposer_critical_result}")

    return state


def process_collect_proposals(state):
    """
    Collect Proposals
    Gather all proposer responses and prepare input for the aggregator
    """
    print(f"  → Collect Proposals")

    # Read: Read prior layer responses
    response_store_data = state.response_store.read()
    state.data["response_store"] = response_store_data

    # Logic from spec
    proposals = []
    for key in ["call_analytical_result", "call_creative_result", "call_critical_result"]:
        result = state.data.get(key, {})
        resp = result.get("response", "") if isinstance(result, dict) else str(result)
        if resp:
            proposals.append(resp)
    state.data["proposals"] = proposals
    state.data["proposal_count"] = len(proposals)
    print(f"    Collected {len(proposals)} proposals for aggregation")
    if state.data.get("_done"):
        return state

    return state


def process_aggregate_responses(state):
    """
    Aggregate Responses
    Invoke the aggregator agent to synthesize proposer responses
    """
    print(f"  → Aggregate Responses")

    # Logic from spec
    proposals = state.data.get("proposals", [])
    layer = state.data.get("current_layer", 1)
    print(f"    Aggregating {len(proposals)} proposals at layer {layer}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Synthesize proposals
    aggregator_agent_input = build_input(state, "AggregatorInput")
    aggregator_agent_msg = json.dumps(aggregator_agent_input, default=str)
    aggregator_agent_raw = invoke_aggregator_agent(aggregator_agent_msg, output_schema="AggregatorOutput")
    aggregator_agent_result = parse_response(aggregator_agent_raw, "AggregatorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(aggregator_agent_result, "AggregatorOutput"))
    state.data.update(aggregator_agent_result)
    state.data["aggregator_output"] = aggregator_agent_result
    state.data["aggregate_responses_result"] = aggregator_agent_result
    print(f"    ← Aggregator Agent: {aggregator_agent_result}")

    return state


def process_record_layer(state):
    """
    Record Layer
    Store the current layer results in history
    """
    print(f"  → Record Layer")

    # Logic from spec
    layer = state.data.get("current_layer", 1)
    synthesized = state.data.get("synthesized_response", "")
    quality = state.data.get("quality_score", 0)
    layer_entry = {
        "layer": layer,
        "proposals": state.data.get("proposals", []),
        "synthesized_response": synthesized,
        "quality_score": quality,
        "sources_used": state.data.get("sources_used", []),
    }
    history = state.data.get("layer_history", [])
    history.append(layer_entry)
    state.data["layer_history"] = history
    # Feed synthesized response as prior context for next layer
    prior = state.data.get("prior_responses", [])
    prior.append(synthesized)
    state.data["prior_responses"] = prior
    print(f"    Layer {layer} recorded: quality={quality}")
    if state.data.get("_done"):
        return state

    # Write: Persist layer snapshot
    response_store_write = build_input(state, "LayerSnapshot")
    state.response_store.write(response_store_write)

    return state


def process_quality_gate(state):
    """
    Quality Sufficient?
    """
    print(f"  → Quality Sufficient?")

    # Gate: quality_score >= quality_threshold
    # Branch: quality meets threshold → finalize_output
    # Branch: quality below threshold → check_layer_limit

    if (state.data.get("quality_score", 0)) >= (state.data.get("quality_threshold", 0)):
        print(f"    → quality meets threshold")
        return "finalize_output"
    else:
        print(f"    → quality below threshold")
        return "check_layer_limit"


def process_check_layer_limit(state):
    """
    More layers allowed?
    """
    print(f"  → More layers allowed?")

    # Gate: current_layer < max_layers
    # Branch: layers remaining → increment_layer
    # Branch: max layers reached → finalize_output

    if (state.data.get("current_layer", 0)) < (state.data.get("max_layers", 0)):
        print(f"    → layers remaining")
        return "increment_layer"
    else:
        print(f"    → max layers reached")
        return "finalize_output"


def process_finalize_output(state):
    """
    Finalize Output
    Return the final synthesized response with full layer history
    """
    print(f"  → Finalize Output")

    # Logic from spec
    quality = state.data.get("quality_score", 0)
    layers = state.data.get("current_layer", 1)
    threshold = state.data.get("quality_threshold", 7)
    if quality >= threshold:
        print(f"    Quality threshold met (score={quality}) at layer {layers}. Finalizing.")
    else:
        print(f"    Max layers reached (score={quality}). Finalizing with best result.")
    state.data["final_response"] = state.data.get("synthesized_response", "")
    state.data["final_quality"] = quality
    state.data["total_layers"] = layers
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_query": process_receive_query,
    "increment_layer": process_increment_layer,
    "call_analytical": process_call_analytical,
    "call_creative": process_call_creative,
    "call_critical": process_call_critical,
    "collect_proposals": process_collect_proposals,
    "aggregate_responses": process_aggregate_responses,
    "record_layer": process_record_layer,
    "quality_gate": process_quality_gate,
    "check_layer_limit": process_check_layer_limit,
    "finalize_output": process_finalize_output,
}

TRANSITIONS = {
    "receive_query": "increment_layer",
    "increment_layer": ['call_analytical', 'call_creative', 'call_critical'],  # fan-out
    "call_analytical": "collect_proposals",
    "call_creative": "collect_proposals",
    "call_critical": "collect_proposals",
    "collect_proposals": "aggregate_responses",
    "aggregate_responses": "record_layer",
    "record_layer": "quality_gate",
    # "quality_gate": determined by gate logic
    # "check_layer_limit": determined by gate logic
    "finalize_output": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Mixture of Agents — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_query"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Mixture of Agents")
    print(f"  Mixture of Agents (MoA) architecture: multiple diverse proposer agents generate responses in parallel, then an aggregator synthesizes the best composite response. Supports multi-layer iterative refinement with quality gating.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['quality_gate', 'check_layer_limit']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    initial = {}
    initial["query"] = input("Enter query: ")
    run(initial)