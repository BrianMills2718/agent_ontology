#!/usr/bin/env python3
"""
RAG — Generated by OpenClaw Instantiation Engine
Spec: Retrieval-Augmented Generation agent that answers questions using a document knowledge base
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "QueryInput": {
        "description": """User query""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "RewrittenQuery": {
        "description": """Improved query for retrieval""",
        "fields": [{"name": "rewritten_query", "type": "string"}, {"name": "original_query", "type": "string"}],
    },
    "RetrievalResult": {
        "description": """Raw retrieval results""",
        "fields": [{"name": "passages", "type": "list<string>"}, {"name": "scores", "type": "list<float>"}, {"name": "search_query", "type": "string"}],
    },
    "JudgeInput": {
        "description": """Input to relevance judge""",
        "fields": [{"name": "query", "type": "string"}, {"name": "passages", "type": "list<string>"}, {"name": "stored_chunks", "type": "list<string>"}],
    },
    "JudgeOutput": {
        "description": """Filtered passages""",
        "fields": [{"name": "filtered_passages", "type": "list<string>"}, {"name": "relevance_scores", "type": "list<float>"}],
    },
    "GeneratorInput": {
        "description": """Input to answer generator""",
        "fields": [{"name": "query", "type": "string"}, {"name": "context_passages", "type": "list<string>"}],
    },
    "GeneratedAnswer": {
        "description": """Generated answer with citations""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "citations", "type": "list<string>"}],
    },
    "Document": {
        "description": """A source document""",
        "fields": [{"name": "document_id", "type": "string"}, {"name": "document_text", "type": "string"}, {"name": "metadata", "type": "object"}],
    },
    "ChunkList": {
        "description": """List of document chunks""",
        "fields": [{"name": "chunks", "type": "list<object>"}],
    },
    "EmbeddedChunk": {
        "description": """A chunk with its embedding""",
        "fields": [{"name": "text", "type": "string"}, {"name": "embedding", "type": "list<float>"}, {"name": "metadata", "type": "object"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

# ── ChromaDB Vector Store (falls back to in-memory list) ──
try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    """Vector store backed by ChromaDB with in-memory fallback."""
    def __init__(self, name):
        self._fallback = []
        self._collection = None
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
            print(f"    [ChromaDB] collection '{name}' ready")
        else:
            print(f"    [VectorStore] using in-memory fallback (pip install chromadb for semantic search)")

    def read(self, query=None, top_k=5):
        """Read all entries, or query for similar ones."""
        if self._collection is not None:
            if query:
                results = self._collection.query(query_texts=[query], n_results=min(top_k, max(self._collection.count(), 1)))
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        """Write an entry. value should have "text" and optionally "metadata"."""
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        # ChromaDB metadata values must be str, int, float, or bool
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            self._collection.add(documents=[text], metadatas=[clean_meta], ids=[doc_id])
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})



class Store_vector_store:
    """Vector Store (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("vector_store")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)


class Store_document_store:
    """Document Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_generator(user_message, output_schema=None):
    """Generator"""
    system = """You are an answer generator. You receive a user question and retrieved context passages.
Answer the question based ONLY on the provided context. If the context doesn't contain
enough information, say so. Cite which passages you used.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_query_rewriter(user_message, output_schema=None):
    """Query Rewriter"""
    system = """You rewrite user queries to improve retrieval. Given a query, produce a more specific
search query that will match relevant documents. Output only the rewritten query text.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Query Rewriter", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_relevance_judge(user_message, output_schema=None):
    """Relevance Judge"""
    system = """You judge whether retrieved passages are relevant to the query. For each passage,
output a relevance score (0-10) and whether to include it. Return a filtered list.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Relevance Judge", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for RAG"""
    def __init__(self):
        self.vector_store = Store_vector_store()
        self.document_store = Store_document_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_query(state):
    """
    Receive Query
    Accept user query
    """
    print(f"  → Receive Query")

    # Logic from spec
    print(f"    Query: {state.data.get('query', '')}")
    if state.data.get("_done"):
        return state

    return state


def process_rewrite_query(state):
    """
    Rewrite Query
    Improve the query for better retrieval
    """
    print(f"  → Rewrite Query")

    # Invoke: Rewrite query
    query_rewriter_input = build_input(state, "QueryInput")
    query_rewriter_msg = json.dumps(query_rewriter_input, default=str)
    query_rewriter_raw = invoke_query_rewriter(query_rewriter_msg, output_schema="RewrittenQuery")
    query_rewriter_result = parse_response(query_rewriter_raw, "RewrittenQuery")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(query_rewriter_result, "RewrittenQuery"))
    state.data.update(query_rewriter_result)
    state.data["rewritten_query_schema"] = query_rewriter_result
    state.data["rewrite_query_result"] = query_rewriter_result
    print(f"    ← Query Rewriter: {query_rewriter_result}")

    return state


def process_retrieve(state):
    """
    Retrieve
    Search the vector store for relevant passages
    """
    print(f"  → Retrieve")

    # Read: Similarity search
    vector_store_data = state.vector_store.read(key=state.data.get("rewritten_query", ""))
    state.data["vector_store"] = vector_store_data

    # Logic from spec
    search_query = state.data.get("rewritten_query", state.data.get("query", ""))
    state.data["search_query"] = search_query
    top_k = state.data.get("top_k", 5)
    state.data["top_k"] = top_k
    results = state.data.get("vector_store", [])
    stored = [e.get("text", "") for e in (results or []) if e.get("text")]
    state.data["stored_chunks"] = stored
    state.data["passages"] = stored
    print(f"    Semantic search returned {len(stored)} chunks for: {search_query[:100]}")
    if state.data.get("_done"):
        return state

    return state


def process_judge_relevance(state):
    """
    Judge Relevance
    Filter retrieved passages by relevance
    """
    print(f"  → Judge Relevance")

    # Invoke: Filter by relevance
    relevance_judge_input = build_input(state, "JudgeInput")
    relevance_judge_msg = json.dumps(relevance_judge_input, default=str)
    relevance_judge_raw = invoke_relevance_judge(relevance_judge_msg, output_schema="JudgeOutput")
    relevance_judge_result = parse_response(relevance_judge_raw, "JudgeOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(relevance_judge_result, "JudgeOutput"))
    state.data.update(relevance_judge_result)
    state.data["judge_output"] = relevance_judge_result
    state.data["judge_relevance_result"] = relevance_judge_result
    print(f"    ← Relevance Judge: {relevance_judge_result}")

    return state


def process_check_relevance(state):
    """
    Relevant results?
    """
    print(f"  → Relevant results?")

    # Gate: filtered_passages is not empty
    # Branch: no relevant results → no_answer
    # Branch: has relevant results → generate_answer

    if bool(state.data.get("filtered_passages")):
        print(f"    → has relevant results")
        return "generate_answer"
    else:
        print(f"    → no relevant results")
        return "no_answer"


def process_generate_answer(state):
    """
    Generate Answer
    Generate an answer from the query and relevant context
    """
    print(f"  → Generate Answer")

    # Logic from spec
    passages = state.data.get("filtered_passages", state.data.get("passages", []))
    state.data["context_passages"] = passages
    print(f"    Generating from {len(passages)} passages")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate answer
    generator_input = build_input(state, "GeneratorInput")
    generator_msg = json.dumps(generator_input, default=str)
    generator_raw = invoke_generator(generator_msg, output_schema="GeneratedAnswer")
    generator_result = parse_response(generator_raw, "GeneratedAnswer")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(generator_result, "GeneratedAnswer"))
    state.data.update(generator_result)
    state.data["generated_answer"] = generator_result
    state.data["generate_answer_result"] = generator_result
    print(f"    ← Generator: {generator_result}")

    return state


def process_no_answer(state):
    """
    No Answer
    Report that no relevant information was found
    """
    print(f"  → No Answer")

    # Logic from spec
    state.data["answer"] = "I could not find relevant information to answer this question."
    state.data["citations"] = []
    state.data["_done"] = True
    print(f"    No relevant passages found")
    if state.data.get("_done"):
        return state

    return state


def process_emit_answer(state):
    """
    Emit Answer
    Return the generated answer
    """
    print(f"  → Emit Answer")

    # Logic from spec
    answer = state.data.get("answer", "")
    citations = state.data.get("citations", [])
    print(f"    Answer: {answer[:200]}")
    print(f"    Citations: {len(citations)}")
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


def process_ingest_document(state):
    """
    Ingest Document
    Chunk a document and store embeddings
    """
    print(f"  → Ingest Document")

    # Logic from spec
    text = state.data.get("document_text", "")
    chunk_size = state.data.get("chunk_size", 500)
    overlap = state.data.get("chunk_overlap", 50)
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk_text = " ".join(words[i:i + chunk_size])
        chunks.append({"text": chunk_text, "index": len(chunks)})
    state.data["chunks"] = chunks
    print(f"    Chunked document into {len(chunks)} chunks")
    if state.data.get("_done"):
        return state

    # Write: Store raw document
    document_store_write = build_input(state, "Document")
    state.document_store.write(document_store_write)

    return state


def process_embed_and_store(state):
    """
    Embed & Store
    Generate embeddings for chunks and store in vector DB
    """
    print(f"  → Embed & Store")

    # Logic from spec
    chunks = state.data.get("chunks", [])
    doc_id = state.data.get("document_id", "unknown")
    for chunk in chunks:
        entry = {
            "text": chunk["text"],
            "embedding": [],
            "metadata": {"source": doc_id, "index": chunk["index"]}
        }
        state.vector_store.write(entry, key=f"{doc_id}_chunk_{chunk['index']}")
    state.data["text"] = chunks[-1]["text"] if chunks else ""
    state.data["embedding"] = []
    state.data["metadata"] = {"source": doc_id, "index": len(chunks) - 1}
    print(f"    Stored {len(chunks)} chunks in vector store")
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_query": process_receive_query,
    "rewrite_query": process_rewrite_query,
    "retrieve": process_retrieve,
    "judge_relevance": process_judge_relevance,
    "check_relevance": process_check_relevance,
    "generate_answer": process_generate_answer,
    "no_answer": process_no_answer,
    "emit_answer": process_emit_answer,
    "ingest_document": process_ingest_document,
    "embed_and_store": process_embed_and_store,
}

TRANSITIONS = {
    "receive_query": "rewrite_query",
    "rewrite_query": "retrieve",
    "retrieve": "judge_relevance",
    "judge_relevance": "check_relevance",
    # "check_relevance": determined by gate logic
    "generate_answer": "emit_answer",
    "no_answer": None,  # terminal
    "emit_answer": None,  # terminal
    "ingest_document": "embed_and_store",
    "embed_and_store": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """RAG — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_query"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  RAG")
    print(f"  Retrieval-Augmented Generation agent that answers questions using a document knowledge base")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_relevance']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()