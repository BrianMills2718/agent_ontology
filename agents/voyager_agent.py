#!/usr/bin/env python3
"""
Voyager Exploration Agent — Generated by Agent Ontology Instantiation Engine
Spec: An open-ended exploration agent that autonomously proposes objectives, learns skills, and accumulates them in a persistent library.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set AGENT_ONTOLOGY_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "CurriculumInput": {
        "description": """""",
        "fields": [{"name": "env_description", "type": "string"}, {"name": "skill_summary", "type": "string"}],
    },
    "ObjectiveOutput": {
        "description": """""",
        "fields": [{"name": "objective", "type": "string"}],
    },
    "CoderInput": {
        "description": """""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "retrieved_skills", "type": "list<string>"}, {"name": "reflection_feedback", "type": "string"}],
    },
    "CodeOutput": {
        "description": """""",
        "fields": [{"name": "skill_name", "type": "string"}, {"name": "description", "type": "string"}, {"name": "code", "type": "string"}],
    },
    "ExecutionInput": {
        "description": """""",
        "fields": [{"name": "code", "type": "string"}],
    },
    "ExecutionResult": {
        "description": """""",
        "fields": [{"name": "logs", "type": "string"}, {"name": "success", "type": "boolean"}],
    },
    "ReflectionInput": {
        "description": """""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "code", "type": "string"}, {"name": "logs", "type": "string"}],
    },
    "ReflectionOutput": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "feedback", "type": "string"}],
    },
    "SkillEntry": {
        "description": """""",
        "fields": [{"name": "skill_name", "type": "string"}, {"name": "code", "type": "string"}, {"name": "description", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

# ── ChromaDB Vector Store (falls back to in-memory list) ──
try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    """Vector store backed by ChromaDB with in-memory fallback."""
    def __init__(self, name):
        self._fallback = []
        self._collection = None
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
            print(f"    [ChromaDB] collection '{name}' ready")
        else:
            print(f"    [VectorStore] using in-memory fallback (pip install chromadb for semantic search)")

    def read(self, query=None, top_k=5):
        """Read all entries, or query for similar ones."""
        if self._collection is not None:
            if query:
                results = self._collection.query(query_texts=[query], n_results=min(top_k, max(self._collection.count(), 1)))
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        """Write an entry. value should have "text" and optionally "metadata"."""
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        # ChromaDB metadata values must be str, int, float, or bool
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            self._collection.add(documents=[text], metadatas=[clean_meta], ids=[doc_id])
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})



class Store_skill_library:
    """Skill Library (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("skill_library")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_curriculum_agent(user_message, output_schema=None):
    """Curriculum Agent"""
    system = """You are the Curriculum Agent for Voyager. Your goal is to propose novel and achievable objectives based on the current environment and the skills already discovered.
Examine the skill library summary and the environment description.
Propose an objective that expands the agent's capabilities.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Curriculum Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_coder_agent(user_message, output_schema=None):
    """Code Generation Agent"""
    system = """You are the Code Generation Agent. You write implementation code to accomplish a specific objective.
You have access to a set of retrieved skills that you can call or adapt.
If you receive self-reflection feedback from a previous failed attempt, use it to fix the errors.
Output the skill name, a brief description, and the complete Python code.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Code Generation Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_reflector_agent(user_message, output_schema=None):
    """Self-Reflection Agent"""
    system = """You are the Self-Reflection Agent. You evaluate the execution of a generated skill.
Compare the intended objective with the actual execution logs and errors.
Determine if the objective was met. If not, provide specific feedback on how to fix the code.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Self-Reflection Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Tool Implementations
# ═══════════════════════════════════════════════════════════

def tool_environment(input_text):
    """Minecraft Environment: The game environment where code is executed."""
    return f"[tool Minecraft Environment not implemented for input: {input_text}]"



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Voyager Exploration Agent"""
    def __init__(self):
        self.skill_library = Store_skill_library()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_start_exploration(state):
    """
    Initialize Exploration
    """
    print(f"  → Initialize Exploration")

    # Logic from spec
    state.data["iteration_count"] = 0
    state.data["max_iterations"] = 50
    state.data["env_description"] = "Spawn point in a forest biome near a river."
    state.data["reflection_feedback"] = ""
    state.data["retry_count"] = 0
    if state.data.get("_done"):
        return state

    return state


def process_propose_objective(state):
    """
    Propose Objective
    """
    print(f"  → Propose Objective")

    # Logic from spec
    # Prepare input for curriculum agent
    state.data["curriculum_input"] = {
      "env_description": state.data.get("env_description"),
      "skill_summary": "List of currently known skills..." 
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: curriculum_agent
    curriculum_agent_input = build_input(state, "CurriculumInput")
    curriculum_agent_msg = json.dumps(curriculum_agent_input, default=str)
    curriculum_agent_raw = invoke_curriculum_agent(curriculum_agent_msg, output_schema="ObjectiveOutput")
    curriculum_agent_result = parse_response(curriculum_agent_raw, "ObjectiveOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(curriculum_agent_result, "ObjectiveOutput"))
    state.data.update(curriculum_agent_result)
    state.data["objective_output"] = curriculum_agent_result
    state.data["propose_objective_result"] = curriculum_agent_result
    print(f"    ← Curriculum Agent: {curriculum_agent_result}")

    return state


def process_retrieve_skills(state):
    """
    Retrieve Relevant Skills
    """
    print(f"  → Retrieve Relevant Skills")

    # Read: 
    skill_library_data = state.skill_library.read()
    state.data["skill_library"] = skill_library_data

    # Logic from spec
    # Logic to query the skill_library based on the objective
    objective = state.data.get("objective", "")
    print(f"Retrieving skills for: {objective}")
    if state.data.get("_done"):
        return state

    return state


def process_generate_code(state):
    """
    Generate Skill Code
    """
    print(f"  → Generate Skill Code")

    # Logic from spec
    state.data["coder_input"] = {
      "objective": state.data.get("objective"),
      "retrieved_skills": state.data.get("retrieved_skills", []),
      "reflection_feedback": state.data.get("reflection_feedback", "")
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: coder_agent
    coder_agent_input = build_input(state, "CoderInput")
    coder_agent_msg = json.dumps(coder_agent_input, default=str)
    coder_agent_raw = invoke_coder_agent(coder_agent_msg, output_schema="CodeOutput")
    coder_agent_result = parse_response(coder_agent_raw, "CodeOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(coder_agent_result, "CodeOutput"))
    state.data.update(coder_agent_result)
    state.data["code_output"] = coder_agent_result
    state.data["generate_code_result"] = coder_agent_result
    print(f"    ← Code Generation Agent: {coder_agent_result}")

    return state


def process_execute_skill(state):
    """
    Execute Skill
    """
    print(f"  → Execute Skill")

    # Logic from spec
    print(f"Executing code for skill: {state.data.get('skill_name')}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke tool: Minecraft Environment
    _tool_input = str(state.data.get("tool_input", state.data.get("input", "")))
    _tool_result = tool_environment(_tool_input)
    state.data["observation"] = _tool_result
    print(f"    Observation: {_tool_result[:200]}")

    return state


def process_evaluate_result(state):
    """
    Evaluate Result
    """
    print(f"  → Evaluate Result")

    # Logic from spec
    state.data["reflection_input"] = {
      "objective": state.data.get("objective"),
      "code": state.data.get("code"),
      "logs": state.data.get("logs")
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: reflector_agent
    reflector_agent_input = build_input(state, "ReflectionInput")
    reflector_agent_msg = json.dumps(reflector_agent_input, default=str)
    reflector_agent_raw = invoke_reflector_agent(reflector_agent_msg, output_schema="ReflectionOutput")
    reflector_agent_result = parse_response(reflector_agent_raw, "ReflectionOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(reflector_agent_result, "ReflectionOutput"))
    state.data.update(reflector_agent_result)
    state.data["reflection_output"] = reflector_agent_result
    state.data["evaluate_result_result"] = reflector_agent_result
    print(f"    ← Self-Reflection Agent: {reflector_agent_result}")

    return state


def process_check_success(state):
    """
    Is Skill Successful?
    """
    print(f"  → Is Skill Successful?")

    # Gate: success == True
    # Branch: yes → save_skill
    # Branch: no → check_retry_limit

    if state.data.get("success") == True:
        print(f"    → yes")
        return "save_skill"
    else:
        print(f"    → no")
        return "check_retry_limit"


def process_check_retry_limit(state):
    """
    Retry Limit?
    """
    print(f"  → Retry Limit?")

    # Gate: retry_count < 3
    # Branch: retry → generate_code
    # Branch: abort → increment_iteration

    if (state.data.get("retry_count", 0)) < 3:
        print(f"    → retry")
        return "generate_code"
    else:
        print(f"    → abort")
        return "increment_iteration"


def process_save_skill(state):
    """
    Save to Library
    """
    print(f"  → Save to Library")

    # Logic from spec
    print(f"Saving successful skill: {state.data.get('skill_name')}")
    state.data["retry_count"] = 0
    state.data["reflection_feedback"] = ""
    if state.data.get("_done"):
        return state

    # Write: 
    skill_library_write = build_input(state, "SkillEntry")
    state.skill_library.write(skill_library_write)

    return state


def process_increment_iteration(state):
    """
    Increment Iteration
    """
    print(f"  → Increment Iteration")

    # Logic from spec
    state.data["iteration_count"] += 1
    state.data["retry_count"] = 0
    state.data["reflection_feedback"] = ""
    if state.data.get("_done"):
        return state

    return state


def process_check_exploration_limit(state):
    """
    Continue Exploration?
    """
    print(f"  → Continue Exploration?")

    # Gate: iteration_count < max_iterations
    # Branch: continue → propose_objective
    # Branch: stop → end_exploration

    if (state.data.get("iteration_count", 0)) < (state.data.get("max_iterations", 0)):
        print(f"    → continue")
        return "propose_objective"
    else:
        print(f"    → stop")
        return "end_exploration"


def process_end_exploration(state):
    """
    End Exploration
    """
    print(f"  → End Exploration")

    # Logic from spec
    state.data['_done'] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "start_exploration": process_start_exploration,
    "propose_objective": process_propose_objective,
    "retrieve_skills": process_retrieve_skills,
    "generate_code": process_generate_code,
    "execute_skill": process_execute_skill,
    "evaluate_result": process_evaluate_result,
    "check_success": process_check_success,
    "check_retry_limit": process_check_retry_limit,
    "save_skill": process_save_skill,
    "increment_iteration": process_increment_iteration,
    "check_exploration_limit": process_check_exploration_limit,
    "end_exploration": process_end_exploration,
}

TRANSITIONS = {
    "start_exploration": "propose_objective",
    "propose_objective": "retrieve_skills",
    "retrieve_skills": "generate_code",
    "generate_code": "execute_skill",
    "execute_skill": "evaluate_result",
    "evaluate_result": "check_success",
    # "check_success": determined by gate logic
    # "check_retry_limit": determined by gate logic
    "save_skill": "increment_iteration",
    "increment_iteration": "check_exploration_limit",
    # "check_exploration_limit": determined by gate logic
    "end_exploration": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Voyager Exploration Agent — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "start_exploration"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Voyager Exploration Agent")
    print(f"  An open-ended exploration agent that autonomously proposes objectives, learns skills, and accumulates them in a persistent library.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_success', 'check_retry_limit', 'check_exploration_limit']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()