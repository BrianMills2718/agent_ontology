#!/usr/bin/env python3
"""
MultiDoc Structured — Generated by Agent Ontology Instantiation Engine
Spec: Multi-step document reasoning agent with explicit extraction, cross-checking, and verification stages. Designed to outperform single-shot on questions requiring contradiction detection, arithmetic aggregation, and multi-source reasoning.

"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set AGENT_ONTOLOGY_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text


def call_embedding(texts, model="text-embedding-3-small", provider="openai"):
    """Generate embeddings for a list of texts. Returns list of float vectors."""
    if isinstance(texts, str):
        texts = [texts]
    if provider == "openai" or model.startswith("text-embedding"):
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    elif provider == "google" or model.startswith("models/"):
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        result = client.models.embed_content(model=model or "models/text-embedding-004", contents=texts)
        return result.embeddings
    elif provider == "huggingface" or provider == "local":
        try:
            from sentence_transformers import SentenceTransformer
            _emb_model = SentenceTransformer(model or "all-MiniLM-L6-v2")
            return _emb_model.encode(texts).tolist()
        except ImportError:
            print("    [WARN] sentence-transformers not installed, returning empty embeddings")
            return [[] for _ in texts]
    else:
        from openai import OpenAI
        client = OpenAI()
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "ReasoningInput": {
        "description": """Input with context and query""",
        "fields": [{"name": "context", "type": "string"}, {"name": "query", "type": "string"}],
    },
    "ExtractionOutput": {
        "description": """Extracted facts with source attribution""",
        "fields": [{"name": "extracted_facts", "type": "array"}, {"name": "contradictions", "type": "array"}],
    },
    "ReasoningStepInput": {
        "description": """Input to the reasoning step""",
        "fields": [{"name": "extracted_facts", "type": "array"}, {"name": "contradictions", "type": "array"}, {"name": "query", "type": "string"}],
    },
    "ReasoningOutput": {
        "description": """Final answer with reasoning""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "reasoning", "type": "string"}],
    },
    "VerificationInput": {
        "description": """Input to the verification step""",
        "fields": [{"name": "context", "type": "string"}, {"name": "query", "type": "string"}, {"name": "answer", "type": "string"}, {"name": "reasoning", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_extractor(user_message, output_schema=None):
    """Fact Extractor"""
    system = """You are a fact extraction assistant. Given a set of fact cards and a question, extract ONLY the specific facts relevant to answering the question.
For each relevant fact, note: - The source card ID (A, B, C, etc.) - The specific fact or number - Any caveats (e.g., "self-reported", "according to audit", "before/after event")
If two sources contradict each other, extract BOTH facts and flag the contradiction.
Output as JSON: {"extracted_facts": [{"source": "A", "fact": "...", "caveat": "..."}], "contradictions": [{"fact1_source": "A", "fact2_source": "B", "description": "..."}]}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Fact Extractor", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_reasoner(user_message, output_schema=None):
    """Reasoning Agent"""
    system = """You are a reasoning assistant. Given extracted facts (with source IDs and caveats) and a question, reason step-by-step to produce an answer.
Rules: - When sources contradict, prefer independent audits/studies over self-reported data. - For arithmetic: show your calculation step by step. - For negation questions ("which did NOT..."): check each option systematically. - For temporal questions: establish a timeline before answering.
Output as JSON: {"reasoning": "step-by-step explanation", "answer": "concise answer"}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Reasoning Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_verifier(user_message, output_schema=None):
    """Answer Verifier"""
    system = """You are a verification assistant. Given the original question, fact cards, and a proposed answer with reasoning, verify the answer is correct.
Check: 1. Does the answer actually address the question asked? 2. Is the answer supported by the cited sources? 3. If arithmetic was involved, is the calculation correct? 4. Were any contradictions handled correctly (preferring independent sources)? 5. For negation questions, were all options checked?
IMPORTANT: Always output the actual answer text in the "answer" field, never "same answer". If the answer is correct, output: {"verified": true, "answer": "<the actual answer>", "reasoning": "why it's correct"} If incorrect, output: {"verified": false, "answer": "<corrected answer>", "reasoning": "what was wrong and the correction"}
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Answer Verifier", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for MultiDoc Structured"""
    def __init__(self):
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_query(state):
    """
    Receive Query
    Accept the question and fact card context
    """
    print(f"  → Receive Query")

    return state


def process_extract_facts(state):
    """
    Extract Facts
    Extract relevant facts from each source with attribution
    """
    print(f"  → Extract Facts")

    # Invoke: extractor
    extractor_input = build_input(state, "ReasoningInput")
    extractor_msg = json.dumps(extractor_input, default=str)
    extractor_raw = invoke_extractor(extractor_msg, output_schema="ExtractionOutput")
    extractor_result = parse_response(extractor_raw, "ExtractionOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(extractor_result, "ExtractionOutput"))
    state.data.update(extractor_result)
    state.data["extraction_output"] = extractor_result
    state.data["extract_facts_result"] = extractor_result
    print(f"    ← Fact Extractor: {extractor_result}")

    return state


def process_reason(state):
    """
    Reason
    Reason step-by-step using extracted facts to produce an answer
    """
    print(f"  → Reason")

    # Invoke: reasoner
    reasoner_input = build_input(state, "ReasoningStepInput")
    reasoner_msg = json.dumps(reasoner_input, default=str)
    reasoner_raw = invoke_reasoner(reasoner_msg, output_schema="ReasoningOutput")
    reasoner_result = parse_response(reasoner_raw, "ReasoningOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(reasoner_result, "ReasoningOutput"))
    state.data.update(reasoner_result)
    state.data["reasoning_output"] = reasoner_result
    state.data["reason_result"] = reasoner_result
    print(f"    ← Reasoning Agent: {reasoner_result}")

    return state


def process_verify(state):
    """
    Verify Answer
    Cross-check the answer against original sources
    """
    print(f"  → Verify Answer")

    # Invoke: verifier
    verifier_input = build_input(state, "VerificationInput")
    verifier_msg = json.dumps(verifier_input, default=str)
    verifier_raw = invoke_verifier(verifier_msg, output_schema="ReasoningOutput")
    verifier_result = parse_response(verifier_raw, "ReasoningOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(verifier_result, "ReasoningOutput"))
    state.data.update(verifier_result)
    state.data["reasoning_output"] = verifier_result
    state.data["verify_result"] = verifier_result
    print(f"    ← Answer Verifier: {verifier_result}")

    return state


def process_emit_answer(state):
    """
    Emit Answer
    Return the verified final answer
    """
    print(f"  → Emit Answer")

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_query": process_receive_query,
    "extract_facts": process_extract_facts,
    "reason": process_reason,
    "verify": process_verify,
    "emit_answer": process_emit_answer,
}

TRANSITIONS = {
    "receive_query": "extract_facts",
    "extract_facts": "reason",
    "reason": "verify",
    "verify": "emit_answer",
    "emit_answer": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """MultiDoc Structured — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_query"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  MultiDoc Structured")
    print(f"  Multi-step document reasoning agent with explicit extraction, cross-checking, and verification stages. Designed to outperform single-shot on questions requiring contradiction detection, arithmetic aggregation, and multi-source reasoning.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()