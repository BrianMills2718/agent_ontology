#!/usr/bin/env python3
"""
Tree of Thought — Generated by OpenClaw Instantiation Engine
Spec: Tree-of-Thought agent that explores multiple reasoning paths, evaluates them, prunes weak branches, and selects the best leaf as the final answer.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "ProblemInput": {
        "description": """The problem to solve using tree-of-thought""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "max_depth", "type": "integer"}, {"name": "k_candidates", "type": "integer"}, {"name": "top_n", "type": "integer"}],
    },
    "GeneratorInput": {
        "description": """Input to the thought generator agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "parent_thought", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "k_candidates", "type": "integer"}],
    },
    "GeneratedThoughts": {
        "description": """Output from the thought generator""",
        "fields": [{"name": "thoughts", "type": "list<ThoughtNode>"}],
    },
    "EvaluatorInput": {
        "description": """Input to the thought evaluator agent""",
        "fields": [{"name": "problem", "type": "string"}, {"name": "thought_content", "type": "string"}, {"name": "depth", "type": "integer"}],
    },
    "EvaluationResult": {
        "description": """Evaluation score and rationale for a thought""",
        "fields": [{"name": "score", "type": "integer"}, {"name": "rationale", "type": "string"}],
    },
    "ThoughtNode": {
        "description": """A single node in the thought tree""",
        "fields": [{"name": "thought_id", "type": "string"}, {"name": "content", "type": "string"}, {"name": "depth", "type": "integer"}, {"name": "score", "type": "integer"}, {"name": "parent_id", "type": "string"}],
    },
    "FinalAnswer": {
        "description": """The final answer selected from the thought tree""",
        "fields": [{"name": "answer", "type": "string"}, {"name": "answer_score", "type": "integer"}, {"name": "total_thoughts_explored", "type": "integer"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_thought_store:
    """Thought Store (queue)"""
    def __init__(self):
        self.queue = []

    def read(self, key=None):
        return self.queue[0] if self.queue else None

    def write(self, value, key=None):
        self.queue.append(value)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_thought_generator(user_message, output_schema=None):
    """Thought Generator"""
    system = """You are a creative reasoning agent. Given a problem and a parent thought (or the root problem),
generate exactly K candidate next-step thoughts. Each thought should be a distinct reasoning path
that advances toward solving the problem. Output them as a JSON array of objects, each with
"thought_id" (string) and "content" (string describing the reasoning step).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Thought Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_thought_evaluator(user_message, output_schema=None):
    """Thought Evaluator"""
    system = """You are a critical evaluator of reasoning steps. Given a problem and a candidate thought,
score how promising this thought is on a scale of 1-10 (10 = highly promising, likely leads
to correct solution; 1 = dead end or incorrect reasoning). Also provide a brief rationale.
Output JSON with "score" (integer 1-10) and "rationale" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Thought Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Tree of Thought"""
    def __init__(self):
        self.thought_store = Store_thought_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_problem(state):
    """
    Receive Problem
    Accept the problem input and initialize the tree-of-thought state
    """
    print(f"  → Receive Problem")

    # Logic from spec
    state.data["current_depth"] = 0
    state.data["max_depth"] = state.data.get("max_depth", 3)
    state.data["k_candidates"] = state.data.get("k_candidates", 3)
    state.data["top_n"] = state.data.get("top_n", 2)
    state.data["all_thoughts"] = []
    # Initialize with root node representing the problem itself
    root = {
        "thought_id": "root",
        "content": state.data.get("problem", ""),
        "depth": 0,
        "score": 0,
        "parent_id": None
    }
    state.data["current_parents"] = [root]
    state.data["all_thoughts"].append(root)
    print(f"    Problem: {state.data.get('problem', '')[:100]}")
    print(f"    Config: depth={state.data['max_depth']}, K={state.data['k_candidates']}, top-N={state.data['top_n']}")
    if state.data.get("_done"):
        return state

    return state


def process_check_depth(state):
    """
    Depth limit reached?
    """
    print(f"  → Depth limit reached?")

    # Gate: current_depth >= max_depth
    # Branch: depth < max_depth → generate_thoughts
    # Branch: depth >= max_depth → select_best

    if (state.data.get("current_depth", 0)) >= (state.data.get("max_depth", 0)):
        print(f"    → depth >= max_depth")
        return "select_best"
    else:
        print(f"    → depth < max_depth")
        return "generate_thoughts"


def process_generate_thoughts(state):
    """
    Generate Thoughts
    For each current parent thought, invoke the generator to produce K candidate children
    """
    print(f"  → Generate Thoughts")

    # Logic from spec
    state.data["current_depth"] = state.data.get("current_depth", 0) + 1
    parents = state.data.get("current_parents", [])
    state.data["parent_index"] = 0
    state.data["generated_children"] = []
    print(f"    Generating thoughts at depth {state.data['current_depth']} for {len(parents)} parent(s)")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate K candidate thoughts
    thought_generator_input = build_input(state, "GeneratorInput")
    thought_generator_msg = json.dumps(thought_generator_input, default=str)
    thought_generator_raw = invoke_thought_generator(thought_generator_msg, output_schema="GeneratedThoughts")
    thought_generator_result = parse_response(thought_generator_raw, "GeneratedThoughts")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(thought_generator_result, "GeneratedThoughts"))
    state.data.update(thought_generator_result)
    state.data["generated_thoughts"] = thought_generator_result
    state.data["generate_thoughts_result"] = thought_generator_result
    print(f"    ← Thought Generator: {thought_generator_result}")

    return state


def process_evaluate_thoughts(state):
    """
    Evaluate Thoughts
    Invoke the evaluator agent to score each generated thought
    """
    print(f"  → Evaluate Thoughts")

    # Logic from spec
    children = state.data.get("generated_children", [])
    state.data["eval_index"] = 0
    print(f"    Evaluating {len(children)} candidate thoughts")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Evaluate each thought
    thought_evaluator_input = build_input(state, "EvaluatorInput")
    thought_evaluator_msg = json.dumps(thought_evaluator_input, default=str)
    thought_evaluator_raw = invoke_thought_evaluator(thought_evaluator_msg, output_schema="EvaluationResult")
    thought_evaluator_result = parse_response(thought_evaluator_raw, "EvaluationResult")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(thought_evaluator_result, "EvaluationResult"))
    state.data.update(thought_evaluator_result)
    state.data["evaluation_result"] = thought_evaluator_result
    state.data["evaluate_thoughts_result"] = thought_evaluator_result
    print(f"    ← Thought Evaluator: {thought_evaluator_result}")

    return state


def process_expand_thoughts(state):
    """
    Expand Thoughts
    Prune to top-N thoughts by score and set them as parents for the next depth level
    """
    print(f"  → Expand Thoughts")

    # Logic from spec
    children = state.data.get("generated_children", [])
    top_n = state.data.get("top_n", 2)
    # Sort by score descending and keep top N
    sorted_children = sorted(children, key=lambda t: t.get("score", 0), reverse=True)
    selected = sorted_children[:top_n]
    state.data["current_parents"] = selected
    # Add to all_thoughts
    all_thoughts = state.data.get("all_thoughts", [])
    all_thoughts.extend(selected)
    state.data["all_thoughts"] = all_thoughts
    print(f"    Selected top-{top_n} thoughts (scores: {[t.get('score', 0) for t in selected]})")
    if state.data.get("_done"):
        return state

    # Write: Persist selected thoughts
    thought_store_write = build_input(state, "ThoughtNode")
    state.thought_store.write(thought_store_write)

    return state


def process_select_best(state):
    """
    Select Best
    Among all leaf-level thoughts, select the one with the highest score
    """
    print(f"  → Select Best")

    # Read: Read all thoughts
    thought_store_data = state.thought_store.read()
    state.data["thought_store"] = thought_store_data

    # Logic from spec
    max_depth = state.data.get("current_depth", state.data.get("max_depth", 3))
    all_thoughts = state.data.get("all_thoughts", [])
    leaves = [t for t in all_thoughts if t.get("depth", 0) == max_depth]
    if not leaves:
        leaves = state.data.get("current_parents", [])
    best = max(leaves, key=lambda t: t.get("score", 0)) if leaves else {"content": "No solution found", "score": 0}
    state.data["best_thought"] = best
    print(f"    Best thought (score={best.get('score', 0)}): {best.get('content', '')[:100]}")
    if state.data.get("_done"):
        return state

    return state


def process_produce_answer(state):
    """
    Produce Answer
    Format the best thought into the final answer output
    """
    print(f"  → Produce Answer")

    # Logic from spec
    best = state.data.get("best_thought", {})
    state.data["answer"] = best.get("content", "No answer found")
    state.data["answer_score"] = best.get("score", 0)
    state.data["total_thoughts_explored"] = len(state.data.get("all_thoughts", []))
    print(f"    Final answer (score={state.data['answer_score']}): {state.data['answer'][:200]}")
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_problem": process_receive_problem,
    "check_depth": process_check_depth,
    "generate_thoughts": process_generate_thoughts,
    "evaluate_thoughts": process_evaluate_thoughts,
    "expand_thoughts": process_expand_thoughts,
    "select_best": process_select_best,
    "produce_answer": process_produce_answer,
}

TRANSITIONS = {
    "receive_problem": "check_depth",
    # "check_depth": determined by gate logic
    "generate_thoughts": "evaluate_thoughts",
    "evaluate_thoughts": "expand_thoughts",
    "expand_thoughts": "check_depth",  # loop: Next depth level
    "select_best": "produce_answer",
    "produce_answer": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Tree of Thought — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_problem"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Tree of Thought")
    print(f"  Tree-of-Thought agent that explores multiple reasoning paths, evaluates them, prunes weak branches, and selects the best leaf as the final answer.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_depth']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()