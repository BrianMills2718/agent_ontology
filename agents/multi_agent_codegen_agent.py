#!/usr/bin/env python3
"""
Multi-Agent Code Generation Pipeline — Generated by OpenClaw Instantiation Engine
Spec: Automated pipeline for generating, testing, and reviewing code from natural language specs using specialized agents with parallel artifact generation.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "CodegenRequest": {
        "description": """Initial user requirements""",
        "fields": [{"name": "prompt", "type": "string"}],
    },
    "TechnicalSpecOutput": {
        "description": """Structured technical specification""",
        "fields": [{"name": "spec_content", "type": "string"}, {"name": "architecture_notes", "type": "string"}, {"name": "dependencies", "type": "list<string>"}],
    },
    "GeneratorInput": {
        "description": """Input for the code generator""",
        "fields": [{"name": "spec", "type": "string"}],
    },
    "CodeArtifactOutput": {
        "description": """Generated code implementation""",
        "fields": [{"name": "code", "type": "string"}, {"name": "dependencies", "type": "list<string>"}],
    },
    "TestWriterInput": {
        "description": """Input for the test writer""",
        "fields": [{"name": "spec", "type": "string"}],
    },
    "TestArtifactOutput": {
        "description": """Generated test suite""",
        "fields": [{"name": "tests", "type": "string"}],
    },
    "FinalArtifacts": {
        "description": """Combined code and test package""",
        "fields": [{"name": "code", "type": "string"}, {"name": "tests", "type": "string"}, {"name": "dependencies", "type": "list<string>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_artifact_store:
    """Artifact Store (file)"""
    def __init__(self):
        self.path = os.path.expanduser("/tmp/artifact_store.jsonl")
        self.data = []

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        self.data.append(value)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_spec_analyst_agent(user_message, output_schema=None):
    """Specification Analyst"""
    system = """You are a technical architect. Analyze natural language requirements and produce a structured technical specification.
Identify all functions, classes, data models, external dependencies, and edge cases.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Specification Analyst", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_code_generator_agent(user_message, output_schema=None):
    """Code Generator"""
    system = """You are an expert software engineer. Generate implementation code based on a technical specification.
Include inline comments and a dependency manifest.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Code Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_test_writer_agent(user_message, output_schema=None):
    """Test Writer"""
    system = """You are a QA engineer. Write comprehensive tests based ONLY on a technical specification.
Cover happy paths, edge cases, and error handling.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Test Writer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Multi-Agent Code Generation Pipeline"""
    def __init__(self):
        self.artifact_store = Store_artifact_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_intake_request(state):
    """
    Intake Request
    Receive natural language prompt from user
    """
    print(f"  → Intake Request")

    # Logic from spec
    state.data["prompt"] = state.data.get("prompt", "")
    print(f"Received request: {state.data['prompt'][:100]}...")
    if state.data.get("_done"):
        return state

    return state


def process_analyze_spec(state):
    """
    Analyze Specification
    Invoke the Analyst to create a technical spec
    """
    print(f"  → Analyze Specification")

    # Logic from spec
    # Prepare input for the analyst
    state.data["analyst_input"] = {"prompt": state.data.get("prompt")}
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Get Technical Spec
    spec_analyst_agent_input = build_input(state, "CodegenRequest")
    spec_analyst_agent_msg = json.dumps(spec_analyst_agent_input, default=str)
    spec_analyst_agent_raw = invoke_spec_analyst_agent(spec_analyst_agent_msg, output_schema="TechnicalSpecOutput")
    spec_analyst_agent_result = parse_response(spec_analyst_agent_raw, "TechnicalSpecOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(spec_analyst_agent_result, "TechnicalSpecOutput"))
    state.data.update(spec_analyst_agent_result)
    state.data["technical_spec_output"] = spec_analyst_agent_result
    state.data["analyze_spec_result"] = spec_analyst_agent_result
    print(f"    ← Specification Analyst: {spec_analyst_agent_result}")

    return state


def process_generate_artifacts(state):
    """
    Generate Artifacts (Fan-Out)
    Invoke Code Generator and Test Writer in parallel
    """
    print(f"  → Generate Artifacts (Fan-Out)")

    # Logic from spec
    # Prepare inputs for parallel agents from the analyst's output
    spec = state.data.get("technical_spec_output", {})
    state.data["generator_input"] = {"spec": spec.get("spec_content")}
    state.data["test_writer_input"] = {"spec": spec.get("spec_content")}
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate Code
    code_generator_agent_input = build_input(state, "GeneratorInput")
    code_generator_agent_msg = json.dumps(code_generator_agent_input, default=str)
    code_generator_agent_raw = invoke_code_generator_agent(code_generator_agent_msg, output_schema="CodeArtifactOutput")
    code_generator_agent_result = parse_response(code_generator_agent_raw, "CodeArtifactOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(code_generator_agent_result, "CodeArtifactOutput"))
    state.data.update(code_generator_agent_result)
    state.data["code_artifact_output"] = code_generator_agent_result
    state.data["generate_artifacts_result"] = code_generator_agent_result
    print(f"    ← Code Generator: {code_generator_agent_result}")

    # Invoke: Generate Tests
    test_writer_agent_input = build_input(state, "TestWriterInput")
    test_writer_agent_msg = json.dumps(test_writer_agent_input, default=str)
    test_writer_agent_raw = invoke_test_writer_agent(test_writer_agent_msg, output_schema="TestArtifactOutput")
    test_writer_agent_result = parse_response(test_writer_agent_raw, "TestArtifactOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(test_writer_agent_result, "TestArtifactOutput"))
    state.data.update(test_writer_agent_result)
    state.data["test_artifact_output"] = test_writer_agent_result
    state.data["generate_artifacts_result"] = test_writer_agent_result
    print(f"    ← Test Writer: {test_writer_agent_result}")

    return state


def process_synthesis(state):
    """
    Synthesize Results
    Combine code and tests into a final package
    """
    print(f"  → Synthesize Results")

    # Logic from spec
    # Merge namespaced outputs
    code_out = state.data.get("code_artifact_output", {})
    test_out = state.data.get("test_artifact_output", {})
    state.data["final_package"] = {
      "code": code_out.get("code"),
      "tests": test_out.get("tests"),
      "dependencies": code_out.get("dependencies")
    }
    if state.data.get("_done"):
        return state

    # Write: Save artifacts
    artifact_store_write = build_input(state, "FinalArtifacts")
    state.artifact_store.write(artifact_store_write)

    return state


def process_final_output(state):
    """
    Final Output
    Mark process as complete
    """
    print(f"  → Final Output")

    # Logic from spec
    print("Generation complete. Artifacts stored.")
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "intake_request": process_intake_request,
    "analyze_spec": process_analyze_spec,
    "generate_artifacts": process_generate_artifacts,
    "synthesis": process_synthesis,
    "final_output": process_final_output,
}

TRANSITIONS = {
    "intake_request": "analyze_spec",
    "analyze_spec": "generate_artifacts",
    "generate_artifacts": "synthesis",
    "synthesis": "final_output",
    "final_output": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Multi-Agent Code Generation Pipeline — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "intake_request"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Multi-Agent Code Generation Pipeline")
    print(f"  Automated pipeline for generating, testing, and reviewing code from natural language specs using specialized agents with parallel artifact generation.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()