#!/usr/bin/env python3
"""
Claude Code — Generated by OpenClaw Instantiation Engine
Spec: Anthropic's official CLI agent — interactive coding assistant with persistent memory, tool use, sub-agent spawning, and human-in-the-loop permission gates
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096):
    if model.startswith("claude") or model.startswith("anthropic"):
        return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
    elif model.startswith("gemini"):
        return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
    else:
        return _call_openai(model, system_prompt, user_message, temperature, max_tokens)


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    try:
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"[STUB] Would call OpenAI {model} — {user_message[:80]}")
        return json.dumps({"stub": True, "model": model})


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    try:
        import anthropic
        client = anthropic.Anthropic()
        response = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}],
        )
        return response.content[0].text
    except Exception as e:
        print(f"[STUB] Would call Anthropic {model} — {user_message[:80]}")
        return json.dumps({"stub": True, "model": model})


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    try:
        from google import genai
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
        response = client.models.generate_content(
            model=model,
            contents=f"{system_prompt}\n\n{user_message}",
            config={"temperature": temperature, "max_output_tokens": max_tokens},
        )
        return response.text
    except Exception as e:
        print(f"[STUB] Would call Gemini {model} — {user_message[:80]}")
        return json.dumps({"stub": True, "model": model})

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "UserInput": {
        "description": """What the user types or triggers""",
        "fields": [{"name": "text", "type": "string"}, {"name": "type", "type": "enum[message, file_event, slash_command]"}, {"name": "files", "type": "list<string>"}],
    },
    "RawInput": {
        "description": """Normalized input ready for context assembly""",
        "fields": [{"name": "content", "type": "string"}, {"name": "role", "type": "enum[user, system]"}],
    },
    "ApiPayload": {
        "description": """The full request sent to Claude API on every call. ~9-11k tokens fixed overhead before any conversation history.""",
        "fields": [{"name": "system", "type": "string"}, {"name": "tools", "type": "list<ToolDefinition>"}, {"name": "messages", "type": "list<Message>"}, {"name": "model", "type": "string"}, {"name": "max_tokens", "type": "integer"}],
    },
    "ApiResponse": {
        "description": """What comes back from the Claude API""",
        "fields": [{"name": "content", "type": "list<ContentBlock>"}, {"name": "stop_reason", "type": "enum[end_turn, tool_use, max_tokens]"}, {"name": "usage", "type": "TokenUsage"}],
    },
    "ContentBlock": {
        "description": """A single block in the API response""",
        "fields": [{"name": "type", "type": "enum[text, tool_use, thinking]"}, {"name": "text", "type": "string"}, {"name": "tool_name", "type": "string"}, {"name": "tool_input", "type": "object"}, {"name": "id", "type": "string"}],
    },
    "ParsedResponse": {
        "description": """API response split into text and tool calls""",
        "fields": [{"name": "text_blocks", "type": "list<string>"}, {"name": "tool_calls", "type": "list<ToolCall>"}],
    },
    "ToolCall": {
        "description": """A single tool invocation""",
        "fields": [{"name": "id", "type": "string"}, {"name": "tool_name", "type": "string"}, {"name": "input", "type": "object"}],
    },
    "ToolCalls": {
        "description": """Set of tool invocations to execute (possibly in parallel)""",
        "fields": [{"name": "calls", "type": "list<ToolCall>"}],
    },
    "ToolResults": {
        "description": """Results from tool execution, possibly with hook injections""",
        "fields": [{"name": "results", "type": "list<ToolResult>"}],
    },
    "ToolResult": {
        "description": """A single tool result""",
        "fields": [{"name": "tool_call_id", "type": "string"}, {"name": "content", "type": "string"}, {"name": "system_reminders", "type": "list<string>"}],
    },
    "ObservedResults": {
        "description": """Tool results after hook processing""",
        "fields": [{"name": "results", "type": "list<ToolResult>"}, {"name": "hook_injections", "type": "list<SystemReminder>"}],
    },
    "SystemReminder": {
        "description": """Context injected by hooks into tool results""",
        "fields": [{"name": "source", "type": "enum[governance, malware_check, edit_constraint, post_edit_quiz, file_modification, task_nudge]"}, {"name": "content", "type": "string"}, {"name": "show_user", "type": "boolean"}],
    },
    "ConversationHistory": {
        "description": """The full conversation state""",
        "fields": [{"name": "messages", "type": "list<Message>"}, {"name": "token_count", "type": "integer"}],
    },
    "Message": {
        "description": """A single message in the conversation""",
        "fields": [{"name": "role", "type": "enum[user, assistant, tool]"}, {"name": "content", "type": "string | list<ContentBlock>"}, {"name": "tool_call_id", "type": "string"}],
    },
    "CompactedHistory": {
        "description": """Conversation after context compression""",
        "fields": [{"name": "summary", "type": "string"}, {"name": "recent_messages", "type": "list<Message>"}, {"name": "transcript_path", "type": "string"}],
    },
    "MemoryEntry": {
        "description": """Content in MEMORY.md""",
        "fields": [{"name": "content", "type": "string"}],
    },
    "TranscriptEntry": {
        "description": """A line in the session .jsonl log""",
        "fields": [{"name": "timestamp", "type": "datetime"}, {"name": "type", "type": "enum[user, assistant, tool_call, tool_result, system]"}, {"name": "content", "type": "object"}],
    },
    "GitStatus": {
        "description": """Git state captured at session start (stale)""",
        "fields": [{"name": "branch", "type": "string"}, {"name": "main_branch", "type": "string"}, {"name": "status", "type": "string"}, {"name": "recent_commits", "type": "list<string>"}],
    },
    "TokenUsage": {
        "description": """Token counts from API response""",
        "fields": [{"name": "input_tokens", "type": "integer"}, {"name": "output_tokens", "type": "integer"}],
    },
    "BashInput": {
        "description": """""",
        "fields": [{"name": "command", "type": "string"}, {"name": "description", "type": "string"}, {"name": "timeout", "type": "integer"}],
    },
    "BashOutput": {
        "description": """""",
        "fields": [{"name": "stdout", "type": "string"}, {"name": "stderr", "type": "string"}, {"name": "exit_code", "type": "integer"}],
    },
    "ReadInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "offset", "type": "integer"}, {"name": "limit", "type": "integer"}],
    },
    "FileContent": {
        "description": """""",
        "fields": [{"name": "content", "type": "string"}, {"name": "line_count", "type": "integer"}],
    },
    "WriteInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "content", "type": "string"}],
    },
    "WriteResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "path", "type": "string"}],
    },
    "EditInput": {
        "description": """""",
        "fields": [{"name": "file_path", "type": "string"}, {"name": "old_string", "type": "string"}, {"name": "new_string", "type": "string"}, {"name": "replace_all", "type": "boolean"}],
    },
    "EditResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "replacements", "type": "integer"}],
    },
    "GlobInput": {
        "description": """""",
        "fields": [{"name": "pattern", "type": "string"}, {"name": "path", "type": "string"}],
    },
    "FileList": {
        "description": """""",
        "fields": [{"name": "files", "type": "list<string>"}],
    },
    "GrepInput": {
        "description": """""",
        "fields": [{"name": "pattern", "type": "string"}, {"name": "path", "type": "string"}, {"name": "output_mode", "type": "enum[content, files_with_matches, count]"}],
    },
    "SearchResults": {
        "description": """""",
        "fields": [{"name": "matches", "type": "list<SearchMatch>"}],
    },
    "SearchMatch": {
        "description": """""",
        "fields": [{"name": "file", "type": "string"}, {"name": "line", "type": "integer"}, {"name": "content", "type": "string"}],
    },
    "WebFetchInput": {
        "description": """""",
        "fields": [{"name": "url", "type": "string"}, {"name": "prompt", "type": "string"}],
    },
    "WebSearchInput": {
        "description": """""",
        "fields": [{"name": "query", "type": "string"}],
    },
    "WebContent": {
        "description": """""",
        "fields": [{"name": "content", "type": "string"}],
    },
    "TaskInput": {
        "description": """Input to sub-agent spawning""",
        "fields": [{"name": "prompt", "type": "string"}, {"name": "subagent_type", "type": "enum[Bash, general-purpose, Explore, Plan, claude-code-guide]"}, {"name": "model", "type": "string"}, {"name": "max_turns", "type": "integer"}, {"name": "run_in_background", "type": "boolean"}],
    },
    "TaskOutput": {
        "description": """""",
        "fields": [{"name": "result", "type": "string"}, {"name": "agent_id", "type": "string"}],
    },
    "NotebookEditInput": {
        "description": """""",
        "fields": [{"name": "notebook_path", "type": "string"}, {"name": "new_source", "type": "string"}, {"name": "cell_type", "type": "enum[code, markdown]"}, {"name": "edit_mode", "type": "enum[replace, insert, delete]"}],
    },
    "NotebookEditResult": {
        "description": """""",
        "fields": [{"name": "success", "type": "boolean"}],
    },
    "ToolDefinition": {
        "description": """JSON Schema definition of a tool (sent with every API call)""",
        "fields": [{"name": "name", "type": "string"}, {"name": "description", "type": "string"}, {"name": "parameters", "type": "object"}],
    },
}


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    # Try to extract JSON from the response
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        # Try to find JSON object in the response
        start = text.find("{")
        end = text.rfind("}") + 1
        if start >= 0 and end > start:
            try:
                return json.loads(text[start:end])
            except json.JSONDecodeError:
                pass
        return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_working_memory:
    """Working Memory (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_episodic_memory:
    """Episodic Memory (file)"""
    def __init__(self):
        self.path = os.path.expanduser("~/.claude/projects/{project}/{session}.jsonl")
        self.data = []

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        self.data.append(value)


class Store_semantic_memory:
    """Semantic Memory (file)"""
    def __init__(self):
        self.path = os.path.expanduser("~/.claude/projects/{project}/memory/MEMORY.md")
        self.data = []

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        self.data.append(value)


class Store_procedural_memory:
    """Procedural Memory (Skills) (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value


class Store_project_instructions:
    """Project Instructions (file)"""
    def __init__(self):
        self.path = os.path.expanduser("/tmp/project_instructions.jsonl")
        self.data = []

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        self.data.append(value)


class Store_git_snapshot:
    """Git Status Snapshot (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_claude(user_message, output_schema=None):
    """Claude (LLM)"""
    system = """You are Claude Code, Anthropic's official CLI for Claude..."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="claude-opus-4-6",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=16384,
    )
    trace_call("Claude (LLM)", "claude-opus-4-6", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_sub_agent(user_message, output_schema=None):
    """Sub-Agent"""
    system = """Inherited from parent context + task-specific prompt"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="claude-opus-4-6",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Sub-Agent", "claude-opus-4-6", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Tool Implementations
# ═══════════════════════════════════════════════════════════

def tool_bash(input_text):
    """Bash: """
    return f"[tool Bash not implemented for input: {input_text}]"


def tool_read(input_text):
    """Read: """
    return f"[tool Read not implemented for input: {input_text}]"


def tool_write(input_text):
    """Write: """
    return f"[tool Write not implemented for input: {input_text}]"


def tool_edit(input_text):
    """Edit: """
    return f"[tool Edit not implemented for input: {input_text}]"


def tool_glob(input_text):
    """Glob: """
    return f"[tool Glob not implemented for input: {input_text}]"


def tool_grep(input_text):
    """Grep: """
    return f"[tool Grep not implemented for input: {input_text}]"


def tool_web_fetch(input_text):
    """WebFetch: """
    return f"[tool WebFetch not implemented for input: {input_text}]"


def tool_web_search(input_text):
    """WebSearch: """
    import requests
    import re as _re
    _headers = {"User-Agent": "OpenClaw/1.0 (agent research project)"} 
    try:
        resp = requests.get("https://en.wikipedia.org/w/api.php", params={
            "action": "query", "list": "search", "srsearch": input_text,
            "format": "json", "srlimit": 5
        }, headers=_headers, timeout=10)
        data = resp.json()
        results = data.get("query", {}).get("search", [])
        if not results:
            return f"No results found for: {input_text}"
        lines = []
        for r in results:
            snippet = _re.sub(r"<[^>]+>", "", r.get("snippet", ""))
            lines.append(f"{r['title']}: {snippet}")
        return "\n".join(lines)
    except Exception as e:
        return f"Search error: {e}"


def tool_task(input_text):
    """Task (Sub-Agent Spawn): """
    return f"[tool Task (Sub-Agent Spawn) not implemented for input: {input_text}]"


def tool_notebook_edit(input_text):
    """NotebookEdit: """
    return f"[tool NotebookEdit not implemented for input: {input_text}]"


def tool_mcp_tools(input_text):
    """MCP Tools: """
    return f"[tool MCP Tools not implemented for input: {input_text}]"



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Claude Code"""
    def __init__(self):
        self.working_memory = Store_working_memory()
        self.episodic_memory = Store_episodic_memory()
        self.semantic_memory = Store_semantic_memory()
        self.procedural_memory = Store_procedural_memory()
        self.project_instructions = Store_project_instructions()
        self.git_snapshot = Store_git_snapshot()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_idle(state):
    """
    IDLE
    Waiting for user input. Session is quiescent.
    """
    print(f"  → IDLE")

    return state


def process_receive(state):
    """
    RECEIVE
    User message, file event, or slash command arrives
    """
    print(f"  → RECEIVE")

    return state


def process_build_context(state):
    """
    BUILD CONTEXT
    Assemble the full API payload: system prompt (~9-11k tokens fixed overhead) + MEMORY.md + CLAUDE.md chain + tool definitions + git snapshot + full conversation history
    """
    print(f"  → BUILD CONTEXT")

    # Read: Load MEMORY.md (first 200 lines)
    semantic_memory_data = state.semantic_memory.read()
    state.data["semantic_memory"] = semantic_memory_data

    # Read: Load CLAUDE.md chain
    project_instructions_data = state.project_instructions.read()
    state.data["project_instructions"] = project_instructions_data

    # Read: Load git status (stale)
    git_snapshot_data = state.git_snapshot.read()
    state.data["git_snapshot"] = git_snapshot_data

    # Read: Load full conversation history
    working_memory_data = state.working_memory.read()
    state.data["working_memory"] = working_memory_data

    # Read: Load skill definitions
    procedural_memory_data = state.procedural_memory.read()
    state.data["procedural_memory"] = procedural_memory_data

    return state


def process_llm_call(state):
    """
    LLM CALL
    Send full context to Claude API. This is the inner loop target — returns here after every tool execution.
    """
    print(f"  → LLM CALL")

    # Invoke: API call
    claude_input = build_input(state, "ApiPayload")
    claude_msg = json.dumps(claude_input, default=str)
    claude_raw = invoke_claude(claude_msg, output_schema="ApiResponse")
    claude_result = parse_response(claude_raw, "ApiResponse")
    # Merge output fields into state.data
    state.data.update(claude_result)
    state.data["api_response"] = claude_result
    state.data["llm_call_result"] = claude_result
    print(f"    ← Claude (LLM): {claude_result}")

    return state


def process_parse_response(state):
    """
    PARSE RESPONSE
    Extract text blocks and tool_use blocks from API response
    """
    print(f"  → PARSE RESPONSE")

    return state


def process_check_tools(state):
    """
    Tool calls?
    """
    print(f"  → Tool calls?")

    # Gate: parsed_response.tool_calls.length > 0
    # Branch: no tool calls → emit_text
    # Branch: has tool calls → check_permission

    if len(state.data.get("parsed_response", {}).get("tool_calls") or []):
        print(f"    → has tool calls")
        return "check_permission"
    else:
        print(f"    → no tool calls")
        return "emit_text"


def process_emit_text(state):
    """
    EMIT TEXT
    Send text response to user terminal
    """
    print(f"  → EMIT TEXT")

    return state


def process_check_permission(state):
    """
    Permission check
    """
    print(f"  → Permission check")

    # Gate: tool_call matches permission_config rules
    # Branch: auto-approved → execute
    # Branch: needs approval → prompt_user

    if state.data.get("_permission_granted", True):
        print(f"    → auto-approved")
        return "execute"
    else:
        print(f"    → needs approval")
        return "prompt_user"


def process_prompt_user(state):
    """
    PERMISSION PROMPT
    """
    print(f"  → PERMISSION PROMPT")

    response = input("Allow {tool_name} with {args}? [y/n] [approve/deny]: ").strip().lower()
    state.data["checkpoint_response"] = response
    return state


def process_execute(state):
    """
    EXECUTE
    Run tool call(s). May execute multiple tools in parallel. The Task tool triggers sub-agent spawning (recursive composition).
    """
    print(f"  → EXECUTE")

    # Tool dispatch
    _tool_name = state.data.get("tool_name", "").lower().strip()
    _tool_input = str(state.data.get("tool_input", state.data.get("action", "")))
    if _tool_name == "bash":
        _tool_result = tool_bash(_tool_input)
    elif _tool_name == "read":
        _tool_result = tool_read(_tool_input)
    elif _tool_name == "write":
        _tool_result = tool_write(_tool_input)
    elif _tool_name == "edit":
        _tool_result = tool_edit(_tool_input)
    elif _tool_name == "glob":
        _tool_result = tool_glob(_tool_input)
    elif _tool_name == "grep":
        _tool_result = tool_grep(_tool_input)
    elif _tool_name == "webfetch":
        _tool_result = tool_web_fetch(_tool_input)
    elif _tool_name == "websearch":
        _tool_result = tool_web_search(_tool_input)
    elif _tool_name == "task (sub-agent spawn)":
        _tool_result = tool_task(_tool_input)
    elif _tool_name == "notebookedit":
        _tool_result = tool_notebook_edit(_tool_input)
    elif _tool_name == "mcp tools":
        _tool_result = tool_mcp_tools(_tool_input)
    else:
        _tool_result = f"Unknown tool: {_tool_name}. Available: bash, read, write, edit, glob, grep, webfetch, websearch, task (sub-agent spawn), notebookedit, mcp tools"
    state.data["observation"] = _tool_result
    print(f"    Observation: {_tool_result[:200]}")

    return state


def process_observe(state):
    """
    OBSERVE
    Collect tool results plus any hook-injected system-reminders (governance context, edit constraints, quiz questions, malware warnings). Agent sees hook OUTPUT but not hook scripts.
    """
    print(f"  → OBSERVE")

    return state


def process_append(state):
    """
    APPEND
    Add assistant message + tool results to conversation history
    """
    print(f"  → APPEND")

    # Write: Append turn to history
    state.working_memory.write(state.data.copy())

    return state


def process_check_context(state):
    """
    Context limit?
    """
    print(f"  → Context limit?")

    # Gate: conversation_history.token_count approaching capacity
    # Branch: context ok → llm_call
    # Branch: context full → compact

    if (state.data.get("_context_token_count", 0) > state.data.get("_context_capacity", 100000) * 0.8):
        print(f"    → context full")
        return "compact"
    else:
        print(f"    → context ok")
        return "llm_call"


def process_compact(state):
    """
    COMPACT
    Compress older messages into a ~2000 word summary. Preserves: primary request, key concepts, files discussed, errors, all user messages (summarized), pending tasks, current work state.
    """
    print(f"  → COMPACT")

    # Read: Read full history
    working_memory_data = state.working_memory.read()
    state.data["working_memory"] = working_memory_data

    # Write: Replace with compacted summary
    state.working_memory.write(state.data.copy())

    return state


def process_persist(state):
    """
    PERSIST
    Write turn to session transcript (.jsonl)
    """
    print(f"  → PERSIST")

    # Write: Write to session transcript
    state.episodic_memory.write(state.data.copy())

    return state


def process_spawn_sub_agent(state):
    """
    Spawn Sub-Agent
    """
    print(f"  → Spawn Sub-Agent")

    # Spawn: template=self, cardinality=dynamic, recursive=True
    print("    [SPAWN] Would create sub-agent from template: self")
    return state


def process_hook_governance(state):
    """
    Governance Hooks
    """
    print(f"  → Governance Hooks")

    # Policy: cross-cutting concern
    pass
    return state


def process_hook_malware(state):
    """
    Malware Check
    """
    print(f"  → Malware Check")

    # Policy: cross-cutting concern
    pass
    return state


def process_permission_policy(state):
    """
    Permission System
    """
    print(f"  → Permission System")

    # Policy: cross-cutting concern
    pass
    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "idle": process_idle,
    "receive": process_receive,
    "build_context": process_build_context,
    "llm_call": process_llm_call,
    "parse_response": process_parse_response,
    "check_tools": process_check_tools,
    "emit_text": process_emit_text,
    "check_permission": process_check_permission,
    "prompt_user": process_prompt_user,
    "execute": process_execute,
    "observe": process_observe,
    "append": process_append,
    "check_context": process_check_context,
    "compact": process_compact,
    "persist": process_persist,
    "spawn_sub_agent": process_spawn_sub_agent,
    "hook_governance": process_hook_governance,
    "hook_malware": process_hook_malware,
    "permission_policy": process_permission_policy,
}

TRANSITIONS = {
    "idle": "receive",
    "receive": "build_context",
    "build_context": "llm_call",
    "llm_call": "parse_response",
    "parse_response": "check_tools",
    # "check_tools": determined by gate logic
    "emit_text": "persist",
    # "check_permission": determined by gate logic
    "prompt_user": ['execute', 'append'],  # fan-out
    "execute": "observe",
    "observe": "append",
    "append": "llm_call",  # loop: Re-reason after denial
    # "check_context": determined by gate logic
    "compact": "llm_call",
    "persist": "idle",  # loop: Wait for next input
    "spawn_sub_agent": None,  # terminal
    "hook_governance": None,  # terminal
    "hook_malware": None,  # terminal
    "permission_policy": None,  # terminal
}


MAX_ITERATIONS = 100


def run(initial_data=None):
    """Claude Code — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "idle"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Claude Code")
    print(f"  Anthropic's official CLI agent — interactive coding assistant with persistent memory, tool use, sub-agent spawning, and human-in-the-loop permission gates")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_tools', 'check_permission', 'check_context']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    initial = {}
    initial["text"] = input("Enter text: ")
    initial["type"] = input("Enter type: ")
    initial["files"] = input("Enter files: ")
    run(initial)