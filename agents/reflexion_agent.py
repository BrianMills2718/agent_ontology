#!/usr/bin/env python3
"""
Reflexion — Generated by OpenClaw Instantiation Engine
Spec: Reflexion agent that attempts tasks, self-evaluates, generates verbal reflections on failure, and retries with accumulated episodic memory until success or max trials.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TaskSpec": {
        "description": """The task to attempt with success criteria""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}],
    },
    "ActorInput": {
        "description": """Input to the actor agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}, {"name": "reflections", "type": "list<string>"}, {"name": "trial", "type": "integer"}],
    },
    "ActorOutput": {
        "description": """Output from the actor agent""",
        "fields": [{"name": "trajectory", "type": "list<string>"}, {"name": "answer", "type": "string"}],
    },
    "EvaluatorInput": {
        "description": """Input to the evaluator agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "success_criteria", "type": "string"}, {"name": "answer", "type": "string"}, {"name": "trajectory", "type": "list<string>"}],
    },
    "EvaluatorOutput": {
        "description": """Output from the evaluator agent""",
        "fields": [{"name": "success", "type": "boolean"}, {"name": "score", "type": "float"}, {"name": "failure_reason", "type": "string"}],
    },
    "ReflectionInput": {
        "description": """Input to the self-reflection agent""",
        "fields": [{"name": "task", "type": "string"}, {"name": "trajectory", "type": "list<string>"}, {"name": "answer", "type": "string"}, {"name": "failure_reason", "type": "string"}, {"name": "prior_reflections", "type": "list<string>"}, {"name": "trial", "type": "integer"}],
    },
    "ReflectionOutput": {
        "description": """Output from the self-reflection agent""",
        "fields": [{"name": "reflection", "type": "string"}, {"name": "suggested_strategy", "type": "string"}],
    },
    "EpisodeEntry": {
        "description": """A single trial episode stored in episodic memory""",
        "fields": [{"name": "trial_number", "type": "integer"}, {"name": "answer", "type": "string"}, {"name": "score", "type": "float"}, {"name": "failure_reason", "type": "string"}, {"name": "reflection", "type": "string"}, {"name": "suggested_strategy", "type": "string"}],
    },
    "ErrorInfo": {
        "description": """Error information passed to the error handler""",
        "fields": [{"name": "error_type", "type": "string"}, {"name": "error_message", "type": "string"}, {"name": "retry_count", "type": "integer"}],
    },
    "ReflexionResult": {
        "description": """Final result of the Reflexion process""",
        "fields": [{"name": "final_answer", "type": "string"}, {"name": "total_trials", "type": "integer"}, {"name": "final_score", "type": "float"}, {"name": "outcome", "type": "enum[success, failure_max_trials]"}, {"name": "reflections", "type": "list<string>"}, {"name": "episodes", "type": "list<EpisodeEntry>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_episodic_memory:
    """Episodic Memory (queue)"""
    def __init__(self):
        self.queue = []

    def read(self, key=None):
        return self.queue[0] if self.queue else None

    def write(self, value, key=None):
        self.queue.append(value)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_actor_agent(user_message, output_schema=None):
    """Actor Agent"""
    system = """You are an Actor agent. You receive a task and a list of prior self-reflections
from previous failed attempts. Use those reflections to avoid repeating mistakes.
Produce an action trajectory: a sequence of reasoning steps and a final answer.
Output JSON with "trajectory" (list of step strings) and "answer" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Actor Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_evaluator_agent(user_message, output_schema=None):
    """Evaluator Agent"""
    system = """You are an Evaluator. Given a task description, the expected outcome criteria,
and the actor's answer, determine whether the attempt succeeded.
Output JSON with "success" (boolean), "score" (float 0-1),
and "failure_reason" (string, empty if success).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Evaluator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_self_reflection_agent(user_message, output_schema=None):
    """Self-Reflection Agent"""
    system = """You are a Self-Reflection agent. Given a failed attempt's trajectory, the task,
the evaluator's failure reason, and all prior reflections, generate a concise
verbal self-reflection. Identify what went wrong, why, and what strategy the
actor should use next time. Be specific and actionable.
Output JSON with "reflection" (string) and "suggested_strategy" (string).
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Self-Reflection Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Reflexion"""
    def __init__(self):
        self.episodic_memory = Store_episodic_memory()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_task(state):
    """
    Receive Task
    Accept the task and success criteria, initialize trial state
    """
    print(f"  → Receive Task")

    # Logic from spec
    state.data["trial"] = 0
    state.data["max_trials"] = 5
    state.data["reflections"] = []
    state.data["episodes"] = []
    state.data["succeeded"] = False
    print(f"    Task: {state.data.get('task', '')[:100]}")
    print(f"    Max trials: {state.data['max_trials']}")
    if state.data.get("_done"):
        return state

    return state


def process_attempt_task(state):
    """
    Attempt Task
    Invoke the actor agent with the task and accumulated reflections
    """
    print(f"  → Attempt Task")

    # Read: Read prior episodes
    episodic_memory_data = state.episodic_memory.read()
    state.data["episodic_memory"] = episodic_memory_data

    # Logic from spec
    trial = state.data.get("trial", 0) + 1
    state.data["trial"] = trial
    reflections = state.data.get("reflections", [])
    print(f"    Trial {trial}/{state.data.get('max_trials', 5)} with {len(reflections)} prior reflections")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Execute task attempt
    actor_agent_input = build_input(state, "ActorInput")
    actor_agent_msg = json.dumps(actor_agent_input, default=str)
    actor_agent_raw = invoke_actor_agent(actor_agent_msg, output_schema="ActorOutput")
    actor_agent_result = parse_response(actor_agent_raw, "ActorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(actor_agent_result, "ActorOutput"))
    state.data.update(actor_agent_result)
    state.data["actor_output"] = actor_agent_result
    state.data["attempt_task_result"] = actor_agent_result
    print(f"    ← Actor Agent: {actor_agent_result}")

    return state


def process_evaluate_attempt(state):
    """
    Evaluate Attempt
    Invoke the evaluator agent to judge whether the attempt succeeded
    """
    print(f"  → Evaluate Attempt")

    # Logic from spec
    answer = state.data.get("answer", "")
    print(f"    Evaluating answer ({len(answer)} chars)...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Judge attempt
    evaluator_agent_input = build_input(state, "EvaluatorInput")
    evaluator_agent_msg = json.dumps(evaluator_agent_input, default=str)
    evaluator_agent_raw = invoke_evaluator_agent(evaluator_agent_msg, output_schema="EvaluatorOutput")
    evaluator_agent_result = parse_response(evaluator_agent_raw, "EvaluatorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(evaluator_agent_result, "EvaluatorOutput"))
    state.data.update(evaluator_agent_result)
    state.data["evaluator_output"] = evaluator_agent_result
    state.data["evaluate_attempt_result"] = evaluator_agent_result
    print(f"    ← Evaluator Agent: {evaluator_agent_result}")

    return state


def process_check_success(state):
    """
    Succeeded?
    """
    print(f"  → Succeeded?")

    # Gate: success == True
    # Branch: success is True → finalize_success
    # Branch: success is False → check_trials_remaining

    if state.data.get("success") == True:
        print(f"    → success is True")
        return "finalize_success"
    else:
        print(f"    → success is False")
        return "check_trials_remaining"


def process_check_trials_remaining(state):
    """
    Trials remaining?
    """
    print(f"  → Trials remaining?")

    # Gate: trial < max_trials
    # Branch: trials remaining → generate_reflection
    # Branch: trials exhausted → finalize_failure

    if (state.data.get("trial", 0)) < (state.data.get("max_trials", 0)):
        print(f"    → trials remaining")
        return "generate_reflection"
    else:
        print(f"    → trials exhausted")
        return "finalize_failure"


def process_generate_reflection(state):
    """
    Generate Self-Reflection
    Invoke the self-reflection agent to produce verbal reflection on the failure
    """
    print(f"  → Generate Self-Reflection")

    # Logic from spec
    trial = state.data.get("trial", 0)
    reason = state.data.get("failure_reason", "unknown")
    print(f"    Reflecting on trial {trial} failure: {reason[:80]}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Generate self-reflection
    self_reflection_agent_input = build_input(state, "ReflectionInput")
    self_reflection_agent_msg = json.dumps(self_reflection_agent_input, default=str)
    self_reflection_agent_raw = invoke_self_reflection_agent(self_reflection_agent_msg, output_schema="ReflectionOutput")
    self_reflection_agent_result = parse_response(self_reflection_agent_raw, "ReflectionOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(self_reflection_agent_result, "ReflectionOutput"))
    state.data.update(self_reflection_agent_result)
    state.data["reflection_output"] = self_reflection_agent_result
    state.data["generate_reflection_result"] = self_reflection_agent_result
    print(f"    ← Self-Reflection Agent: {self_reflection_agent_result}")

    return state


def process_store_reflection(state):
    """
    Store Reflection
    Append the reflection to episodic memory and prepare for retry
    """
    print(f"  → Store Reflection")

    # Logic from spec
    reflection = state.data.get("reflection", "")
    strategy = state.data.get("suggested_strategy", "")
    trial = state.data.get("trial", 0)
    # Accumulate reflections
    reflections = state.data.get("reflections", [])
    reflections.append(reflection)
    state.data["reflections"] = reflections
    # Build episode entry
    episode = {
        "trial_number": trial,
        "answer": state.data.get("answer", ""),
        "score": state.data.get("score", 0.0),
        "failure_reason": state.data.get("failure_reason", ""),
        "reflection": reflection,
        "suggested_strategy": strategy,
    }
    episodes = state.data.get("episodes", [])
    episodes.append(episode)
    state.data["episodes"] = episodes
    print(f"    Stored reflection #{len(reflections)}: {reflection[:80]}")
    if state.data.get("_done"):
        return state

    # Write: Persist episode
    episodic_memory_write = build_input(state, "EpisodeEntry")
    state.episodic_memory.write(episodic_memory_write)

    return state


def process_actor_error_handler(state):
    """
    Actor Error Handler
    """
    print(f"  → Actor Error Handler")



def process_handle_actor_error(state):
    """
    Handle Actor Error
    Process errors from the actor agent invocation
    """
    print(f"  → Handle Actor Error")

    # Logic from spec
    error = state.data.get("error_message", "Unknown error")
    state.data["failure_reason"] = f"Actor execution error: {error}"
    state.data["success"] = False
    state.data["score"] = 0.0
    state.data["answer"] = ""
    state.data["trajectory"] = []
    print(f"    Actor error caught: {error[:100]}")
    if state.data.get("_done"):
        return state

    return state


def process_finalize_success(state):
    """
    Finalize (Success)
    Return the successful answer with trial metadata
    """
    print(f"  → Finalize (Success)")

    # Logic from spec
    trial = state.data.get("trial", 0)
    score = state.data.get("score", 0.0)
    print(f"    Success on trial {trial} with score {score}")
    state.data["final_answer"] = state.data.get("answer", "")
    state.data["total_trials"] = trial
    state.data["final_score"] = score
    state.data["outcome"] = "success"
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


def process_finalize_failure(state):
    """
    Finalize (Failure)
    Return the best attempt after exhausting all trials
    """
    print(f"  → Finalize (Failure)")

    # Logic from spec
    trial = state.data.get("trial", 0)
    score = state.data.get("score", 0.0)
    reflections = state.data.get("reflections", [])
    print(f"    All {trial} trials exhausted. Best score: {score}")
    print(f"    Total reflections generated: {len(reflections)}")
    state.data["final_answer"] = state.data.get("answer", "")
    state.data["total_trials"] = trial
    state.data["final_score"] = score
    state.data["outcome"] = "failure_max_trials"
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_task": process_receive_task,
    "attempt_task": process_attempt_task,
    "evaluate_attempt": process_evaluate_attempt,
    "check_success": process_check_success,
    "check_trials_remaining": process_check_trials_remaining,
    "generate_reflection": process_generate_reflection,
    "store_reflection": process_store_reflection,
    "actor_error_handler": process_actor_error_handler,
    "handle_actor_error": process_handle_actor_error,
    "finalize_success": process_finalize_success,
    "finalize_failure": process_finalize_failure,
}

TRANSITIONS = {
    "receive_task": "attempt_task",
    "attempt_task": "evaluate_attempt",
    "evaluate_attempt": "check_success",
    # "check_success": determined by gate logic
    # "check_trials_remaining": determined by gate logic
    "generate_reflection": "store_reflection",
    "store_reflection": "attempt_task",  # loop: Reflexion retry loop
    "actor_error_handler": "handle_actor_error",
    "handle_actor_error": "check_trials_remaining",
    "finalize_success": None,  # terminal
    "finalize_failure": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Reflexion — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_task"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Reflexion")
    print(f"  Reflexion agent that attempts tasks, self-evaluates, generates verbal reflections on failure, and retries with accumulated episodic memory until success or max trials.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_success', 'check_trials_remaining']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    initial = {}
    initial["task"] = input("Enter task: ")
    initial["success_criteria"] = input("Enter success_criteria: ")
    run(initial)