#!/usr/bin/env python3
"""
BabyAGI — Generated by OpenClaw Instantiation Engine
Spec: Task-driven autonomous agent that creates, prioritizes, and executes tasks in a loop
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "Task": {
        "description": """A single task in the queue""",
        "fields": [{"name": "id", "type": "integer"}, {"name": "description", "type": "string"}, {"name": "status", "type": "enum[pending, complete]"}, {"name": "priority", "type": "integer"}],
    },
    "TaskList": {
        "description": """Ordered list of tasks""",
        "fields": [{"name": "tasks", "type": "list<Task>"}, {"name": "objective", "type": "string"}],
    },
    "ExecutionInput": {
        "description": """Input to the execution agent""",
        "fields": [{"name": "task", "type": "Task"}, {"name": "objective", "type": "string"}, {"name": "context", "type": "list<string>"}],
    },
    "ExecutionOutput": {
        "description": """Result of task execution""",
        "fields": [{"name": "result", "type": "string"}, {"name": "task_id", "type": "integer"}],
    },
    "ContextQuery": {
        "description": """Query to the context agent""",
        "fields": [{"name": "query", "type": "string"}, {"name": "top_k", "type": "integer"}, {"name": "stored_results", "type": "list<string>"}],
    },
    "ContextResult": {
        "description": """Retrieved context""",
        "fields": [{"name": "context", "type": "list<string>"}, {"name": "sources", "type": "list<string>"}],
    },
    "EnrichedResult": {
        "description": """Execution result enriched with context""",
        "fields": [{"name": "result", "type": "string"}, {"name": "context", "type": "list<string>"}, {"name": "task_id", "type": "integer"}],
    },
    "EmbeddedResult": {
        "description": """Result stored in vector DB""",
        "fields": [{"name": "text", "type": "string"}, {"name": "embedding", "type": "list<float>"}, {"name": "metadata", "type": "object"}],
    },
    "TaskCreationInput": {
        "description": """Input to the task creation agent""",
        "fields": [{"name": "objective", "type": "string"}, {"name": "result", "type": "string"}, {"name": "existing_tasks", "type": "list<Task>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

# ── ChromaDB Vector Store (falls back to in-memory list) ──
try:
    import chromadb
    _chroma_client = chromadb.Client()
    _USE_CHROMA = True
except ImportError:
    _USE_CHROMA = False


class _ChromaVectorStore:
    """Vector store backed by ChromaDB with in-memory fallback."""
    def __init__(self, name):
        self._fallback = []
        self._collection = None
        if _USE_CHROMA:
            self._collection = _chroma_client.get_or_create_collection(name)
            print(f"    [ChromaDB] collection '{name}' ready")
        else:
            print(f"    [VectorStore] using in-memory fallback (pip install chromadb for semantic search)")

    def read(self, query=None, top_k=5):
        """Read all entries, or query for similar ones."""
        if self._collection is not None:
            if query:
                results = self._collection.query(query_texts=[query], n_results=min(top_k, max(self._collection.count(), 1)))
                docs = results.get("documents", [[]])[0]
                metas = results.get("metadatas", [[]])[0]
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
            else:
                if self._collection.count() == 0:
                    return []
                all_data = self._collection.get()
                docs = all_data.get("documents", [])
                metas = all_data.get("metadatas", [])
                return [{"text": d, "metadata": m} for d, m in zip(docs, metas)]
        return self._fallback

    def write(self, value, key=None):
        """Write an entry. value should have "text" and optionally "metadata"."""
        text = value.get("text", str(value)) if isinstance(value, dict) else str(value)
        metadata = value.get("metadata", {}) if isinstance(value, dict) else {}
        # ChromaDB metadata values must be str, int, float, or bool
        clean_meta = {}
        for k, v in (metadata or {}).items():
            if isinstance(v, (str, int, float, bool)):
                clean_meta[k] = v
            elif v is not None:
                clean_meta[k] = str(v)
        if self._collection is not None:
            doc_id = key or f"doc_{self._collection.count()}"
            self._collection.add(documents=[text], metadatas=[clean_meta], ids=[doc_id])
        else:
            self._fallback.append(value if isinstance(value, dict) else {"text": text, "metadata": clean_meta})



class Store_vector_db:
    """Vector DB (vector)"""
    def __init__(self):
        self._store = _ChromaVectorStore("vector_db")

    def read(self, key=None):
        return self._store.read(query=key)

    def write(self, value, key=None):
        self._store.write(value, key=key)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_execution_agent(user_message, output_schema=None):
    """Execution Agent"""
    system = """You are an AI who performs one task. You will receive a JSON object with 'task' (the task to perform), 'objective' (the overall goal), and 'context' (relevant prior results). Perform the task and return your result.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Execution Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_context_agent(user_message, output_schema=None):
    """Context Agent"""
    system = """You retrieve relevant context for a given query from stored results. You will receive a query, top_k count, and stored_results (a list of text strings from previous task executions). Select and return the top_k most relevant stored_results as the 'context' array. For 'sources', use short labels like 'task_result_1'. If stored_results is empty, return empty arrays for both context and sources.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Context Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_task_creation_agent(user_message, output_schema=None):
    """Task Creation Agent"""
    system = """You create new tasks based on the result of an execution agent. You will receive the objective, the latest execution result, and a list of existing incomplete tasks. Return a TaskList containing ALL existing incomplete tasks PLUS any new tasks needed to achieve the objective. Assign incrementing integer IDs to new tasks (starting after the highest existing ID). Each task needs: id (integer), description (string), status ('pending'), priority (integer, 1=highest). Do NOT drop any existing tasks from the list.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Task Creation Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_prioritization_agent(user_message, output_schema=None):
    """Prioritization Agent"""
    system = """You reprioritize a task list based on the objective."""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Prioritization Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for BabyAGI"""
    def __init__(self):
        self.vector_db = Store_vector_db()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_pull_task(state):
    """
    Step 1: Pull task
    Dequeue the highest-priority incomplete task from the task list
    """
    print(f"  → Step 1: Pull task")

    # Read: Read prior results
    vector_db_data = state.vector_db.read()
    state.data["vector_db"] = vector_db_data

    # Logic from spec
    tasks = state.data.get("tasks", [])
    if not tasks:
        print("    No tasks remaining!")
        state.data["_done"] = True
        return state
    completed = state.data.get("completed_count", 0)
    max_tasks = state.data.get("max_tasks", 5)
    if completed >= max_tasks:
        print(f"    Completed {completed} tasks (max {max_tasks}). Stopping.")
        state.data["_done"] = True
        return state
    task = tasks.pop(0)
    state.data["task"] = task
    state.data["tasks"] = tasks
    state.data["completed_count"] = completed + 1
    # Prepare context from vector_db stored results (top 5)
    stored_results = [e.get("text", "") for e in state.data.get("vector_db", []) if e.get("text")]
    state.data["context"] = stored_results[-5:]
    print(f"    Pulled task {completed + 1}/{max_tasks}: {task.get('description', task)}")
    print(f"    Context from prior results: {len(state.data['context'])} items")
    if state.data.get("_done"):
        return state

    return state


def process_execute_task(state):
    """
    Step 2: Execute task
    Send task, objective, and context to Execution Agent
    """
    print(f"  → Step 2: Execute task")

    # Invoke: Execute task
    execution_agent_input = build_input(state, "ExecutionInput")
    execution_agent_msg = json.dumps(execution_agent_input, default=str)
    execution_agent_raw = invoke_execution_agent(execution_agent_msg, output_schema="ExecutionOutput")
    execution_agent_result = parse_response(execution_agent_raw, "ExecutionOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(execution_agent_result, "ExecutionOutput"))
    state.data.update(execution_agent_result)
    state.data["execution_output"] = execution_agent_result
    state.data["execute_task_result"] = execution_agent_result
    print(f"    ← Execution Agent: {execution_agent_result}")

    return state


def process_enrich_and_store(state):
    """
    Step 3: Enrich and store
    Take execution result, retrieve relevant context, enrich the result, and persist to vector store
    """
    print(f"  → Step 3: Enrich and store")

    # Read: Read stored results
    vector_db_data = state.vector_db.read()
    state.data["vector_db"] = vector_db_data

    # Logic from spec
    state.data["query"] = state.data.get("result", "")
    state.data["top_k"] = 5
    stored_results = [e.get("text", "") for e in state.data.get("vector_db", []) if e.get("text")]
    state.data["stored_results"] = stored_results
    state.data["text"] = state.data.get("result", "")
    state.data["embedding"] = []  # Placeholder for embedding vector
    state.data["metadata"] = {"task_id": state.data.get("task_id"), "objective": state.data.get("objective", "")}
    if state.data.get("_done"):
        return state

    # Write: Store result in Vector DB
    vector_db_write = build_input(state, "EmbeddedResult")
    state.vector_db.write(vector_db_write)

    return state


def process_retrieve_context(state):
    """
    Step 4: Retrieve context
    Retrieve relevant context from vector DB using Context Agent
    """
    print(f"  → Step 4: Retrieve context")

    # Read: Read context
    vector_db_data = state.vector_db.read()
    state.data["vector_db"] = vector_db_data

    # Logic from spec
    state.data["query"] = state.data.get("result", "")
    state.data["top_k"] = 5
    stored_results = [e.get("text", "") for e in state.data.get("vector_db", []) if e.get("text")]
    state.data["stored_results"] = stored_results
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Retrieve context
    context_agent_input = build_input(state, "ContextQuery")
    context_agent_msg = json.dumps(context_agent_input, default=str)
    context_agent_raw = invoke_context_agent(context_agent_msg, output_schema="ContextResult")
    context_agent_result = parse_response(context_agent_raw, "ContextResult")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(context_agent_result, "ContextResult"))
    state.data.update(context_agent_result)
    state.data["context_result"] = context_agent_result
    state.data["retrieve_context_result"] = context_agent_result
    print(f"    ← Context Agent: {context_agent_result}")

    return state


def process_create_and_reprioritize(state):
    """
    Step 5 & 6: Create and reprioritize
    Based on the enriched result and context, generate new tasks and reprioritize the full task list
    """
    print(f"  → Step 5 & 6: Create and reprioritize")

    # Logic from spec
    # Prepare input for task creation agent
    state.data["existing_tasks"] = state.data.get("tasks", [])
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Create new tasks
    task_creation_agent_input = build_input(state, "TaskCreationInput")
    task_creation_agent_msg = json.dumps(task_creation_agent_input, default=str)
    task_creation_agent_raw = invoke_task_creation_agent(task_creation_agent_msg, output_schema="TaskList")
    task_creation_agent_result = parse_response(task_creation_agent_raw, "TaskList")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(task_creation_agent_result, "TaskList"))
    state.data.update(task_creation_agent_result)
    state.data["task_list"] = task_creation_agent_result
    state.data["create_and_reprioritize_result"] = task_creation_agent_result
    print(f"    ← Task Creation Agent: {task_creation_agent_result}")

    # Invoke: Reprioritize task list
    prioritization_agent_input = build_input(state, "TaskList")
    prioritization_agent_msg = json.dumps(prioritization_agent_input, default=str)
    prioritization_agent_raw = invoke_prioritization_agent(prioritization_agent_msg, output_schema="TaskList")
    prioritization_agent_result = parse_response(prioritization_agent_raw, "TaskList")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(prioritization_agent_result, "TaskList"))
    state.data.update(prioritization_agent_result)
    state.data["task_list"] = prioritization_agent_result
    state.data["create_and_reprioritize_result"] = prioritization_agent_result
    print(f"    ← Prioritization Agent: {prioritization_agent_result}")

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "pull_task": process_pull_task,
    "execute_task": process_execute_task,
    "enrich_and_store": process_enrich_and_store,
    "retrieve_context": process_retrieve_context,
    "create_and_reprioritize": process_create_and_reprioritize,
}

TRANSITIONS = {
    "pull_task": "execute_task",
    "execute_task": "enrich_and_store",
    "enrich_and_store": "retrieve_context",
    "retrieve_context": "create_and_reprioritize",
    "create_and_reprioritize": "pull_task",  # loop: Loop
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """BabyAGI — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "pull_task"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  BabyAGI")
    print(f"  Task-driven autonomous agent that creates, prioritizes, and executes tasks in a loop")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()