#!/usr/bin/env python3
"""
Self Improver — Generated by Agent Ontology Instantiation Engine
Spec: Reads an agent spec YAML, analyzes it for structural weaknesses (lint warnings, missing patterns, complexity issues), proposes mutations, validates the result, and outputs an improved spec.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set AGENT_ONTOLOGY_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "SpecInput": {
        "description": """The spec to improve""",
        "fields": [{"name": "spec_yaml", "type": "string"}, {"name": "max_rounds", "type": "integer"}],
    },
    "AnalysisResult": {
        "description": """Results of analyzing a spec""",
        "fields": [{"name": "lint_warnings", "type": "list<object>"}, {"name": "verify_issues", "type": "list<object>"}, {"name": "detected_patterns", "type": "list<string>"}, {"name": "entity_count", "type": "integer"}, {"name": "process_count", "type": "integer"}, {"name": "edge_count", "type": "integer"}, {"name": "schema_count", "type": "integer"}],
    },
    "AnalystInput": {
        "description": """Input to the architecture analyst""",
        "fields": [{"name": "spec_yaml", "type": "string"}, {"name": "spec_name", "type": "string"}, {"name": "lint_warnings", "type": "list<object>"}, {"name": "verify_issues", "type": "list<object>"}, {"name": "detected_patterns", "type": "list<string>"}, {"name": "entity_count", "type": "integer"}, {"name": "process_count", "type": "integer"}, {"name": "improvements_made", "type": "list<object>"}],
    },
    "AnalystOutput": {
        "description": """Diagnosis and proposed improvement""",
        "fields": [{"name": "diagnosis", "type": "string"}, {"name": "improvement_type", "type": "string"}, {"name": "improvement_details", "type": "string"}, {"name": "expected_benefit", "type": "string"}, {"name": "risk", "type": "string"}],
    },
    "MutatorInput": {
        "description": """Input to the spec mutator""",
        "fields": [{"name": "current_spec_yaml", "type": "string"}, {"name": "improvement_type", "type": "string"}, {"name": "improvement_details", "type": "string"}, {"name": "diagnosis", "type": "string"}],
    },
    "MutatorOutput": {
        "description": """The mutated spec""",
        "fields": [{"name": "mutated_spec_yaml", "type": "string"}, {"name": "changes_summary", "type": "string"}, {"name": "processes_added", "type": "list<string>"}, {"name": "processes_removed", "type": "list<string>"}],
    },
    "ValidationResult": {
        "description": """Validation results for the mutated spec""",
        "fields": [{"name": "mutation_valid", "type": "boolean"}, {"name": "validation_errors", "type": "list<string>"}],
    },
    "EvaluatorInput": {
        "description": """Input to the improvement evaluator""",
        "fields": [{"name": "diagnosis", "type": "string"}, {"name": "improvement_type", "type": "string"}, {"name": "changes_summary", "type": "string"}, {"name": "lint_warnings", "type": "list<object>"}, {"name": "new_lint_warnings", "type": "list<object>"}, {"name": "verify_issues", "type": "list<object>"}, {"name": "new_verify_issues", "type": "list<object>"}],
    },
    "EvaluatorOutput": {
        "description": """Evaluation of the improvement""",
        "fields": [{"name": "improved", "type": "boolean"}, {"name": "score_delta", "type": "integer"}, {"name": "warnings_fixed", "type": "integer"}, {"name": "warnings_introduced", "type": "integer"}, {"name": "reasoning", "type": "string"}],
    },
    "FinalResult": {
        "description": """The improved spec with summary""",
        "fields": [{"name": "final_spec_yaml", "type": "string"}, {"name": "total_improvements", "type": "integer"}, {"name": "improvements_made", "type": "list<object>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_analyst(user_message, output_schema=None):
    """Architecture Analyst"""
    system = """You are an expert AI agent architect. Given a YAML agent spec and analysis results
(lint warnings, verification issues, detected patterns, complexity metrics), identify
the most impactful improvement to make. Consider:
- Fixing lint warnings (dead stores, schema mismatches, unbounded loops)
- Adding missing patterns (e.g., adding a critique step, error handling)
- Simplifying over-complex structures
- Improving data flow (adding missing read/write edges)

Output JSON with:
- "diagnosis" (string): What is the primary weakness
- "improvement_type" (string): One of: add_review_step, add_store, change_gate_condition,
  modify_prompt, swap_pattern, insert_pattern, remove_process, change_model
- "improvement_details" (string): Specific description of the change
- "expected_benefit" (string): Why this improves the agent
- "risk" (string): What could go wrong
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Architecture Analyst", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_mutator(user_message, output_schema=None):
    """Spec Mutator"""
    system = """You are a precise YAML spec editor. Given an agent spec (as YAML text), an improvement
type, and improvement details, apply the mutation to produce a modified spec.

Rules:
- Preserve the YAML structure (entities, processes, edges, schemas sections)
- Every new entity/process must be connected via edges
- Every schema referenced must be defined
- Gate conditions must reference fields that exist in upstream schemas
- Do NOT remove the entry_point process
- Do NOT break existing flow chains — add to them

Output JSON with:
- "mutated_spec_yaml" (string): The complete modified YAML spec
- "changes_summary" (string): What was changed and why
- "processes_added" (list of strings): IDs of any new processes
- "processes_removed" (list of strings): IDs of any removed processes
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Spec Mutator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_evaluator(user_message, output_schema=None):
    """Improvement Evaluator"""
    system = """You are a quality evaluator for agent architecture changes. Compare the original
spec analysis with the mutated spec analysis and determine if the mutation improved
the architecture. Consider:
- Were lint warnings reduced?
- Were verification issues fixed?
- Did complexity change appropriately?
- Were any new issues introduced?

Output JSON with:
- "improved" (boolean): Is the mutation an improvement overall?
- "score_delta" (integer): Change in quality score (-10 to +10)
- "warnings_fixed" (integer): Number of lint warnings eliminated
- "warnings_introduced" (integer): Number of new warnings
- "reasoning" (string): Explanation of your assessment
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Improvement Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Self Improver"""
    def __init__(self):
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_receive_spec(state):
    """
    Receive Spec
    Load the target spec and initialize improvement state
    """
    print(f"  → Receive Spec")

    # Logic from spec
    state.data["improvement_round"] = 0
    state.data["max_rounds"] = state.data.get("max_rounds", 2)
    state.data["improvements_made"] = []
    state.data["original_spec_yaml"] = state.data.get("spec_yaml", "")
    state.data["current_spec_yaml"] = state.data["original_spec_yaml"]
    print(f"    Loaded spec ({len(state.data['current_spec_yaml'])} chars)")
    if state.data.get("_done"):
        return state

    return state


def process_analyze_spec(state):
    """
    Analyze Spec
    Run lint, verify, and pattern detection on the current spec
    """
    print(f"  → Analyze Spec")

    # Logic from spec
    import yaml as _yaml
    import tempfile, os
    spec_yaml = state.data.get("current_spec_yaml", "")
    # Write spec to temp file for analysis tools
    tmp = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)
    tmp.write(spec_yaml)
    tmp.close()
    tmp_path = tmp.name
    try:
        # Parse the spec
        spec = _yaml.safe_load(spec_yaml)
        state.data["spec_name"] = spec.get("name", "unknown")
        # Run lint
        try:
            from agent_ontology.lint import lint_spec
            lint_results = lint_spec(spec)
            state.data["lint_warnings"] = [{"rule": r.code, "severity": r.severity, "message": r.message} for r in lint_results]
            print(f"    Lint: {len(lint_results)} warnings")
        except Exception as e:
            state.data["lint_warnings"] = []
            print(f"    Lint failed: {e}")
        # Run verify
        try:
            from agent_ontology.verify import verify_spec
            passes, failures = verify_spec(spec, spec_path=tmp_path)
            state.data["verify_issues"] = [{"check": f, "status": "fail"} for f in failures]
            print(f"    Verify: {len(failures)} issues, {len(passes)} passed")
        except Exception as e:
            state.data["verify_issues"] = []
            print(f"    Verify failed: {e}")
        # Detect patterns
        try:
            from agent_ontology.patterns import detect_patterns
            patterns = detect_patterns(spec)
            state.data["detected_patterns"] = list(patterns.keys()) if isinstance(patterns, dict) else patterns
            print(f"    Patterns: {state.data['detected_patterns']}")
        except Exception as e:
            state.data["detected_patterns"] = []
            print(f"    Pattern detection failed: {e}")
        # Count entities/processes/edges
        state.data["entity_count"] = len(spec.get("entities", []))
        state.data["process_count"] = len(spec.get("processes", []))
        state.data["edge_count"] = len(spec.get("edges", []))
        state.data["schema_count"] = len(spec.get("schemas", []))
    finally:
        os.unlink(tmp_path)
    if state.data.get("_done"):
        return state

    return state


def process_diagnose(state):
    """
    Diagnose Weaknesses
    Use the analyst agent to identify the most impactful improvement
    """
    print(f"  → Diagnose Weaknesses")

    # Logic from spec
    round_num = state.data.get("improvement_round", 0)
    print(f"    Round {round_num + 1}: Diagnosing weaknesses...")
    lint_count = len(state.data.get("lint_warnings", []))
    verify_issues = len([v for v in state.data.get("verify_issues", []) if v.get("status") != "pass"])
    print(f"    {lint_count} lint warnings, {verify_issues} verify issues")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Diagnose weaknesses
    analyst_input = build_input(state, "AnalystInput")
    analyst_msg = json.dumps(analyst_input, default=str)
    analyst_raw = invoke_analyst(analyst_msg, output_schema="AnalystOutput")
    analyst_result = parse_response(analyst_raw, "AnalystOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(analyst_result, "AnalystOutput"))
    state.data.update(analyst_result)
    state.data["analyst_output"] = analyst_result
    state.data["diagnose_result"] = analyst_result
    print(f"    ← Architecture Analyst: {analyst_result}")

    return state


def process_check_needs_improvement(state):
    """
    Needs improvement?
    """
    print(f"  → Needs improvement?")

    # Gate: improvement_type is not empty
    # Branch: has improvement to make → mutate_spec
    # Branch: no improvement needed → finalize

    if bool(state.data.get("improvement_type")):
        print(f"    → has improvement to make")
        return "mutate_spec"
    else:
        print(f"    → no improvement needed")
        return "finalize"


def process_mutate_spec(state):
    """
    Mutate Spec
    Apply the proposed mutation to the spec
    """
    print(f"  → Mutate Spec")

    # Logic from spec
    improvement = state.data.get("improvement_type", "unknown")
    details = state.data.get("improvement_details", "")
    print(f"    Applying mutation: {improvement}")
    print(f"    Details: {details[:100]}")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Apply mutation
    mutator_input = build_input(state, "MutatorInput")
    mutator_msg = json.dumps(mutator_input, default=str)
    mutator_raw = invoke_mutator(mutator_msg, output_schema="MutatorOutput")
    mutator_result = parse_response(mutator_raw, "MutatorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(mutator_result, "MutatorOutput"))
    state.data.update(mutator_result)
    state.data["mutator_output"] = mutator_result
    state.data["mutate_spec_result"] = mutator_result
    print(f"    ← Spec Mutator: {mutator_result}")

    return state


def process_extract_mutated_yaml(state):
    """
    Extract Mutated YAML
    Extract the mutated YAML from the mutator response, handling JSON parsing edge cases
    """
    print(f"  → Extract Mutated YAML")

    # Logic from spec
    import re as _re, json as _json
    mutated = state.data.get("mutated_spec_yaml", "")
    if not mutated.strip():
        raw = state.data.get("raw", "")
        if raw:
            # Try full JSON parse of the raw response
            try:
                parsed = _json.loads(raw)
                if isinstance(parsed, dict) and parsed.get("mutated_spec_yaml"):
                    mutated = parsed["mutated_spec_yaml"]
                    state.data["mutated_spec_yaml"] = mutated
                    print(f"    Parsed mutated YAML from JSON ({len(mutated)} chars)")
            except _json.JSONDecodeError:
                pass
        if not mutated.strip() and raw:
            # Try regex extraction from JSON-escaped string
            m = _re.search(r'"mutated_spec_yaml"\s*:\s*"', raw)
            if m:
                start = m.end()
                # Walk forward to find unescaped closing quote
                i = start
                while i < len(raw):
                    if raw[i] == '\\' and i + 1 < len(raw):
                        i += 2
                        continue
                    if raw[i] == '"':
                        break
                    i += 1
                yaml_escaped = raw[start:i]
                mutated = yaml_escaped.replace('\\n', '\n').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                state.data["mutated_spec_yaml"] = mutated
                print(f"    Extracted YAML via regex ({len(mutated)} chars)")
        if not mutated.strip():
            print(f"    Could not extract mutated YAML (raw len={len(state.data.get('raw', ''))})")
    else:
        print(f"    Mutated YAML already present ({len(mutated)} chars)")
    if state.data.get("_done"):
        return state

    return state


def process_validate_mutation(state):
    """
    Validate Mutation
    Check if the mutated spec is valid YAML and passes validation
    """
    print(f"  → Validate Mutation")

    # Logic from spec
    import yaml as _yaml
    import tempfile, os
    mutated_yaml = state.data.get("mutated_spec_yaml", "")
    state.data["mutation_valid"] = False
    state.data["validation_errors"] = []
    if not mutated_yaml.strip():
        state.data["validation_errors"] = ["Empty spec"]
        print("    Mutation produced empty spec")
    else:
        try:
            spec = _yaml.safe_load(mutated_yaml)
            if not isinstance(spec, dict):
                state.data["validation_errors"] = ["Not a valid YAML dict"]
                print("    Invalid YAML structure")
            elif "entities" not in spec or "processes" not in spec:
                state.data["validation_errors"] = ["Missing entities or processes"]
                print("    Missing required sections")
            else:
                # Write to temp and validate
                tmp = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)
                tmp.write(mutated_yaml)
                tmp.close()
                try:
                    from agent_ontology.validate import validate_spec, load_yaml
                    ontology_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'agent_ontology', 'ONTOLOGY.yaml')
                    if not os.path.exists(ontology_path):
                        import agent_ontology
                        ontology_path = os.path.join(os.path.dirname(agent_ontology.__file__), 'ONTOLOGY.yaml')
                    ontology = load_yaml(ontology_path)
                    errors, warnings = validate_spec(spec, ontology, tmp.name)
                    state.data["validation_errors"] = errors
                    if errors:
                        print(f"    Validation: {len(errors)} errors")
                    else:
                        state.data["mutation_valid"] = True
                        print("    Validation: PASS")
                except Exception as e:
                    state.data["validation_errors"] = [str(e)]
                    print(f"    Validation failed: {e}")
                finally:
                    os.unlink(tmp.name)
        except _yaml.YAMLError as e:
            state.data["validation_errors"] = [f"YAML parse error: {e}"]
            print(f"    YAML parse error: {e}")
    if state.data.get("_done"):
        return state

    return state


def process_check_valid(state):
    """
    Mutation valid?
    """
    print(f"  → Mutation valid?")

    # Gate: mutation_valid == True
    # Branch: valid mutation → reanalyze
    # Branch: invalid mutation → handle_invalid

    if state.data.get("mutation_valid") == True:
        print(f"    → valid mutation")
        return "reanalyze"
    else:
        print(f"    → invalid mutation")
        return "handle_invalid"


def process_handle_invalid(state):
    """
    Handle Invalid Mutation
    Record the failure and decide whether to retry or give up
    """
    print(f"  → Handle Invalid Mutation")

    # Logic from spec
    errors = state.data.get("validation_errors", [])
    print(f"    Mutation invalid: {errors[:2]}")
    # Skip this improvement, move to finalize
    state.data["improvement_round"] = state.data.get("max_rounds", 2)
    state.data["_skip_to_finalize"] = True
    if state.data.get("_done"):
        return state

    return state


def process_reanalyze(state):
    """
    Re-analyze Mutated Spec
    Run analysis on the mutated spec to compare with original
    """
    print(f"  → Re-analyze Mutated Spec")

    # Logic from spec
    import yaml as _yaml
    import tempfile, os
    mutated_yaml = state.data.get("mutated_spec_yaml", "")
    tmp = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)
    tmp.write(mutated_yaml)
    tmp.close()
    try:
        spec = _yaml.safe_load(mutated_yaml)
        # Run lint on mutated spec
        try:
            from agent_ontology.lint import lint_spec
            new_lint = lint_spec(spec)
            state.data["new_lint_warnings"] = [{"rule": r.code, "severity": r.severity, "message": r.message} for r in new_lint]
            print(f"    New lint: {len(new_lint)} warnings (was {len(state.data.get('lint_warnings', []))})")
        except Exception:
            state.data["new_lint_warnings"] = []
        # Run verify on mutated spec
        try:
            from agent_ontology.verify import verify_spec
            passes, failures = verify_spec(spec, spec_path=tmp.name)
            state.data["new_verify_issues"] = [{"check": f, "status": "fail"} for f in failures]
        except Exception:
            state.data["new_verify_issues"] = []
        state.data["new_entity_count"] = len(spec.get("entities", []))
        state.data["new_process_count"] = len(spec.get("processes", []))
        state.data["new_edge_count"] = len(spec.get("edges", []))
    finally:
        os.unlink(tmp.name)
    if state.data.get("_done"):
        return state

    return state


def process_evaluate(state):
    """
    Evaluate Improvement
    Compare original and mutated analyses to judge improvement
    """
    print(f"  → Evaluate Improvement")

    # Logic from spec
    print(f"    Evaluating improvement quality...")
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: Evaluate improvement
    evaluator_input = build_input(state, "EvaluatorInput")
    evaluator_msg = json.dumps(evaluator_input, default=str)
    evaluator_raw = invoke_evaluator(evaluator_msg, output_schema="EvaluatorOutput")
    evaluator_result = parse_response(evaluator_raw, "EvaluatorOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(evaluator_result, "EvaluatorOutput"))
    state.data.update(evaluator_result)
    state.data["evaluator_output"] = evaluator_result
    state.data["evaluate_result"] = evaluator_result
    print(f"    ← Improvement Evaluator: {evaluator_result}")

    return state


def process_check_improved(state):
    """
    Improvement accepted?
    """
    print(f"  → Improvement accepted?")

    # Gate: improved == True
    # Branch: improvement accepted → accept_improvement
    # Branch: improvement rejected → reject_improvement

    if state.data.get("improved") == True:
        print(f"    → improvement accepted")
        return "accept_improvement"
    else:
        print(f"    → improvement rejected")
        return "reject_improvement"


def process_accept_improvement(state):
    """
    Accept Improvement
    Apply the accepted mutation as the new current spec
    """
    print(f"  → Accept Improvement")

    # Logic from spec
    state.data["current_spec_yaml"] = state.data.get("mutated_spec_yaml", state.data.get("current_spec_yaml", ""))
    improvement = {
        "round": state.data.get("improvement_round", 0) + 1,
        "type": state.data.get("improvement_type", ""),
        "details": state.data.get("improvement_details", ""),
        "score_delta": state.data.get("score_delta", 0),
    }
    improvements = state.data.get("improvements_made", [])
    improvements.append(improvement)
    state.data["improvements_made"] = improvements
    state.data["improvement_round"] = state.data.get("improvement_round", 0) + 1
    print(f"    Accepted: {improvement['type']} (delta: {improvement['score_delta']:+d})")
    if state.data.get("_done"):
        return state

    return state


def process_reject_improvement(state):
    """
    Reject Improvement
    Discard the mutation and increment the round counter
    """
    print(f"  → Reject Improvement")

    # Logic from spec
    state.data["improvement_round"] = state.data.get("improvement_round", 0) + 1
    print(f"    Rejected improvement (round {state.data['improvement_round']})")
    if state.data.get("_done"):
        return state

    return state


def process_check_rounds_remaining(state):
    """
    More rounds?
    """
    print(f"  → More rounds?")

    # Gate: improvement_round < max_rounds
    # Branch: rounds remaining → analyze_spec
    # Branch: max rounds reached → finalize

    if (state.data.get("improvement_round", 0)) < (state.data.get("max_rounds", 0)):
        print(f"    → rounds remaining")
        return "analyze_spec"
    else:
        print(f"    → max rounds reached")
        return "finalize"


def process_finalize(state):
    """
    Finalize
    Produce the final improved spec with improvement summary
    """
    print(f"  → Finalize")

    # Logic from spec
    improvements = state.data.get("improvements_made", [])
    original_len = len(state.data.get("original_spec_yaml", ""))
    current_len = len(state.data.get("current_spec_yaml", ""))
    print(f"    Finalized: {len(improvements)} improvements applied")
    print(f"    Spec size: {original_len} -> {current_len} chars")
    state.data["final_spec_yaml"] = state.data.get("current_spec_yaml", "")
    state.data["total_improvements"] = len(improvements)
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "receive_spec": process_receive_spec,
    "analyze_spec": process_analyze_spec,
    "diagnose": process_diagnose,
    "check_needs_improvement": process_check_needs_improvement,
    "mutate_spec": process_mutate_spec,
    "extract_mutated_yaml": process_extract_mutated_yaml,
    "validate_mutation": process_validate_mutation,
    "check_valid": process_check_valid,
    "handle_invalid": process_handle_invalid,
    "reanalyze": process_reanalyze,
    "evaluate": process_evaluate,
    "check_improved": process_check_improved,
    "accept_improvement": process_accept_improvement,
    "reject_improvement": process_reject_improvement,
    "check_rounds_remaining": process_check_rounds_remaining,
    "finalize": process_finalize,
}

TRANSITIONS = {
    "receive_spec": "analyze_spec",
    "analyze_spec": "diagnose",
    "diagnose": "check_needs_improvement",
    # "check_needs_improvement": determined by gate logic
    "mutate_spec": "extract_mutated_yaml",
    "extract_mutated_yaml": "validate_mutation",
    "validate_mutation": "check_valid",
    # "check_valid": determined by gate logic
    "handle_invalid": "finalize",
    "reanalyze": "evaluate",
    "evaluate": "check_improved",
    # "check_improved": determined by gate logic
    "accept_improvement": "check_rounds_remaining",
    "reject_improvement": "check_rounds_remaining",
    # "check_rounds_remaining": determined by gate logic
    "finalize": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Self Improver — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "receive_spec"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Self Improver")
    print(f"  Reads an agent spec YAML, analyzes it for structural weaknesses (lint warnings, missing patterns, complexity issues), proposes mutations, validates the result, and outputs an improved spec.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_needs_improvement', 'check_valid', 'check_improved', 'check_rounds_remaining']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()