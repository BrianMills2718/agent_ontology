#!/usr/bin/env python3
"""
Debate Agent — Generated by OpenClaw Instantiation Engine
Spec: A multi-agent debate system with pro/con agents moderated by a judge agent over multiple rounds.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set OPENCLAW_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("OPENCLAW_MODEL", "")


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TopicInput": {
        "description": """User-provided debate topic""",
        "fields": [{"name": "topic", "type": "string"}],
    },
    "DebateSetup": {
        "description": """Moderator's framing of proposition and assigned positions""",
        "fields": [{"name": "proposition", "type": "string"}, {"name": "pro_position", "type": "string"}, {"name": "con_position", "type": "string"}],
    },
    "DebateTurn": {
        "description": """A single turn in the debate history""",
        "fields": [{"name": "round", "type": "integer"}, {"name": "side", "type": "enum[pro, con]"}, {"name": "argument", "type": "string"}, {"name": "position", "type": "string"}, {"name": "timestamp", "type": "string"}],
    },
    "DebateTurnInput": {
        "description": """Input to pro/con agents containing debate history, position, and proposition""",
        "fields": [{"name": "proposition", "type": "string"}, {"name": "debate_history", "type": "list<DebateTurn>"}, {"name": "position", "type": "string"}, {"name": "round", "type": "integer"}],
    },
    "ArgumentOutput": {
        "description": """Agent's argument or rebuttal output""",
        "fields": [{"name": "argument", "type": "string"}],
    },
    "DebateHistoryInput": {
        "description": """Input to judge agent with full debate history""",
        "fields": [{"name": "debate_history", "type": "list<DebateTurn>"}, {"name": "proposition", "type": "string"}, {"name": "pro_position", "type": "string"}, {"name": "con_position", "type": "string"}, {"name": "rounds_completed", "type": "integer"}],
    },
    "JudgmentOutput": {
        "description": """Judge's evaluation of the debate""",
        "fields": [{"name": "pro_score", "type": "integer"}, {"name": "con_score", "type": "integer"}, {"name": "rebuttal_quality", "type": "integer"}, {"name": "winner", "type": "enum[pro, con, tie]"}, {"name": "summary", "type": "string"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_debate_history_store:
    """Debate History (queue)"""
    def __init__(self):
        self.queue = []

    def read(self, key=None):
        return self.queue[0] if self.queue else None

    def write(self, value, key=None):
        self.queue.append(value)



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_moderator_agent(user_message, output_schema=None):
    """Moderator Agent"""
    system = """You are the Moderator Agent. Given a user-provided topic, you will frame it as a clear proposition and assign pro and con positions to two agents. Output a DebateSetup object with the proposition and assigned positions.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Moderator Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_pro_agent(user_message, output_schema=None):
    """Pro Agent"""
    system = """You are the Pro Agent arguing in favor of the proposition. You will receive the debate history and your position. Provide your argument or rebuttal for the current round.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Pro Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_con_agent(user_message, output_schema=None):
    """Con Agent"""
    system = """You are the Con Agent arguing against the proposition. You will receive the debate history and your position. Provide your argument or rebuttal for the current round.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Con Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_judge_agent(user_message, output_schema=None):
    """Judge Agent"""
    system = """You are the Judge Agent. After the debate rounds, evaluate the strength of arguments (1-10 per side), quality of rebuttals, determine the overall winner, and summarize key points from each side. If the combined score is less than 12, request one more round.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Judge Agent", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Debate Agent"""
    def __init__(self):
        self.debate_history_store = Store_debate_history_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_topic_setup(state):
    """
    Topic Setup
    User provides a topic; moderator frames proposition and assigns positions
    """
    print(f"  → Topic Setup")

    # Logic from spec
    # Expect state.data['topic'] from user input
    if "topic" not in state.data or not state.data["topic"]:
        print("    Waiting for user to provide a debate topic.")
        return state
    print(f"    Received topic: {state.data['topic']}")
    if state.data.get("_done"):
        return state

    return state


def process_moderate_topic(state):
    """
    Moderate Topic
    Invoke moderator agent to frame proposition and assign positions
    """
    print(f"  → Moderate Topic")

    # Invoke: Frame proposition and assign positions
    moderator_agent_input = build_input(state, "TopicInput")
    moderator_agent_msg = json.dumps(moderator_agent_input, default=str)
    moderator_agent_raw = invoke_moderator_agent(moderator_agent_msg, output_schema="DebateSetup")
    moderator_agent_result = parse_response(moderator_agent_raw, "DebateSetup")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(moderator_agent_result, "DebateSetup"))
    state.data.update(moderator_agent_result)
    state.data["debate_setup"] = moderator_agent_result
    state.data["moderate_topic_result"] = moderator_agent_result
    print(f"    ← Moderator Agent: {moderator_agent_result}")

    return state


def process_initialize_debate(state):
    """
    Initialize Debate
    Initialize debate state, set round count to 1, clear history
    """
    print(f"  → Initialize Debate")

    # Logic from spec
    state.data["round"] = 1
    state.data["max_rounds"] = 3
    state.data["debate_history"] = []
    state.data["pro_position"] = state.data.get("pro_position", "Pro")
    state.data["con_position"] = state.data.get("con_position", "Con")
    print(f"    Debate initialized with proposition: {state.data.get('proposition', '')}")
    if state.data.get("_done"):
        return state

    return state


def process_pro_argument(state):
    """
    Pro Agent Argument
    Invoke Pro Agent to present argument
    """
    print(f"  → Pro Agent Argument")

    # Logic from spec
    state.data["position"] = state.data.get("pro_position", "Pro")
    if state.data.get("_done"):
        return state

    # Invoke: Pro presents argument
    pro_agent_input = build_input(state, "DebateTurnInput")
    pro_agent_msg = json.dumps(pro_agent_input, default=str)
    pro_agent_raw = invoke_pro_agent(pro_agent_msg, output_schema="ArgumentOutput")
    pro_agent_result = parse_response(pro_agent_raw, "ArgumentOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(pro_agent_result, "ArgumentOutput"))
    state.data.update(pro_agent_result)
    state.data["argument_output"] = pro_agent_result
    state.data["pro_argument_result"] = pro_agent_result
    print(f"    ← Pro Agent: {pro_agent_result}")

    return state


def process_con_argument(state):
    """
    Con Agent Argument
    Invoke Con Agent to present argument
    """
    print(f"  → Con Agent Argument")

    # Logic from spec
    state.data["position"] = state.data.get("con_position", "Con")
    if state.data.get("_done"):
        return state

    # Invoke: Con presents argument
    con_agent_input = build_input(state, "DebateTurnInput")
    con_agent_msg = json.dumps(con_agent_input, default=str)
    con_agent_raw = invoke_con_agent(con_agent_msg, output_schema="ArgumentOutput")
    con_agent_result = parse_response(con_agent_raw, "ArgumentOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(con_agent_result, "ArgumentOutput"))
    state.data.update(con_agent_result)
    state.data["argument_output"] = con_agent_result
    state.data["con_argument_result"] = con_agent_result
    print(f"    ← Con Agent: {con_agent_result}")

    return state


def process_record_pro_argument(state):
    """
    Record Pro Argument
    Append Pro Agent's argument to debate history
    """
    print(f"  → Record Pro Argument")

    # Logic from spec
    turn = {
        "round": state.data.get("round", 1),
        "side": "pro",
        "argument": state.data.get("argument", ""),
        "position": state.data.get("pro_position", "Pro"),
        "timestamp": None
    }
    history = state.data.get("debate_history", [])
    history.append(turn)
    state.data["debate_history"] = history
    print(f"    Recorded Pro argument for round {turn['round']}")
    if state.data.get("_done"):
        return state

    return state


def process_record_con_argument(state):
    """
    Record Con Argument
    Append Con Agent's argument to debate history
    """
    print(f"  → Record Con Argument")

    # Logic from spec
    turn = {
        "round": state.data.get("round", 1),
        "side": "con",
        "argument": state.data.get("argument", ""),
        "position": state.data.get("con_position", "Con"),
        "timestamp": None
    }
    history = state.data.get("debate_history", [])
    history.append(turn)
    state.data["debate_history"] = history
    print(f"    Recorded Con argument for round {turn['round']}")
    if state.data.get("_done"):
        return state

    return state


def process_check_rounds(state):
    """
    Check Rounds
    """
    print(f"  → Check Rounds")

    # Gate: round > max_rounds
    # Branch: round <= max_rounds → pro_argument
    # Branch: round > max_rounds → judge_evaluation

    if (state.data.get("round", 0)) > (state.data.get("max_rounds", 0)):
        print(f"    → round > max_rounds")
        return "judge_evaluation"
    else:
        print(f"    → round <= max_rounds")
        return "pro_argument"


def process_judge_evaluation(state):
    """
    Judge Evaluation
    Invoke Judge Agent to evaluate debate
    """
    print(f"  → Judge Evaluation")

    # Invoke: Judge evaluates debate
    judge_agent_input = build_input(state, "DebateHistoryInput")
    judge_agent_msg = json.dumps(judge_agent_input, default=str)
    judge_agent_raw = invoke_judge_agent(judge_agent_msg, output_schema="JudgmentOutput")
    judge_agent_result = parse_response(judge_agent_raw, "JudgmentOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(judge_agent_result, "JudgmentOutput"))
    state.data.update(judge_agent_result)
    state.data["judgment_output"] = judge_agent_result
    state.data["judge_evaluation_result"] = judge_agent_result
    print(f"    ← Judge Agent: {judge_agent_result}")

    return state


def process_evaluate_judgment(state):
    """
    Evaluate Judgment
    Check judge scores and decide if another round is needed
    """
    print(f"  → Evaluate Judgment")

    # Logic from spec
    combined_score = state.data.get("pro_score", 0) + state.data.get("con_score", 0)
    print(f"    Combined judge score: {combined_score}")
    if combined_score < 12:
        print("    Debate quality low, adding one more round.")
        state.data["_continue_debate"] = True
    else:
        state.data["_done"] = True
        state.data["_continue_debate"] = False
    if state.data.get("_done"):
        return state

    return state


def process_continue_or_end(state):
    """
    Continue or End Debate?
    """
    print(f"  → Continue or End Debate?")

    # Gate: _continue_debate == True
    # Branch: continue debate → pro_argument
    # Branch: end debate → end_debate

    if state.data.get("_continue_debate") == True:
        print(f"    → continue debate")
        return "pro_argument"
    else:
        print(f"    → end debate")
        return "end_debate"


def process_end_debate(state):
    """
    End Debate
    Finalize debate and output judgment summary
    """
    print(f"  → End Debate")

    # Logic from spec
    print("    Debate ended. Final judgment summary:")
    print(state.data.get("summary", "No summary available."))
    if state.data.get("_done"):
        return state

    return state


def process_increment_round(state):
    """
    Increment Round
    Increment the debate round counter
    """
    print(f"  → Increment Round")

    # Logic from spec
    state.data["round"] = state.data.get("round", 1) + 1
    print(f"    Moving to round {state.data['round']}")
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "topic_setup": process_topic_setup,
    "moderate_topic": process_moderate_topic,
    "initialize_debate": process_initialize_debate,
    "pro_argument": process_pro_argument,
    "con_argument": process_con_argument,
    "record_pro_argument": process_record_pro_argument,
    "record_con_argument": process_record_con_argument,
    "check_rounds": process_check_rounds,
    "judge_evaluation": process_judge_evaluation,
    "evaluate_judgment": process_evaluate_judgment,
    "continue_or_end": process_continue_or_end,
    "end_debate": process_end_debate,
    "increment_round": process_increment_round,
}

TRANSITIONS = {
    "topic_setup": "moderate_topic",
    "moderate_topic": "initialize_debate",
    "initialize_debate": "check_rounds",
    "pro_argument": "record_pro_argument",
    "con_argument": "record_con_argument",
    "record_pro_argument": "con_argument",
    "record_con_argument": "increment_round",
    # "check_rounds": determined by gate logic
    "judge_evaluation": "evaluate_judgment",
    "evaluate_judgment": "continue_or_end",
    # "continue_or_end": determined by gate logic
    "end_debate": None,  # terminal
    "increment_round": "check_rounds",
}


MAX_ITERATIONS = int(os.environ.get("OPENCLAW_MAX_ITER", "100"))


def run(initial_data=None):
    """Debate Agent — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "topic_setup"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Debate Agent")
    print(f"  A multi-agent debate system with pro/con agents moderated by a judge agent over multiple rounds.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_rounds', 'continue_or_end']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            current = TRANSITIONS.get(_targets[-1])

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    initial = {}
    initial["topic"] = input("Enter topic: ")
    run(initial)