#!/usr/bin/env python3
"""
Socratic Tutoring Agent — Generated by Agent Ontology Instantiation Engine
Spec: A tutoring system that guides students through topics using probing questions, adaptive hints, and remediation based on understanding levels.
"""

import json
import os
import sys
import time
from datetime import datetime

# ═══════════════════════════════════════════════════════════
# Trace Log
# ═══════════════════════════════════════════════════════════

TRACE = []

def trace_call(agent_label, model, system_prompt, user_message, response, duration_ms):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "agent": agent_label,
        "model": model,
        "system_prompt": system_prompt,
        "user_message": user_message,
        "response": response,
        "duration_ms": duration_ms,
    }
    TRACE.append(entry)
    print(f"    [{agent_label}] ({model}, {duration_ms}ms)")
    print(f"      IN:  {user_message[:300]}")
    print(f"      OUT: {response[:300]}")


def dump_trace(path="trace.json", iterations=0, clean_exit=True):
    # Compute metrics
    total_ms = sum(e["duration_ms"] for e in TRACE)
    agents_used = list(set(e["agent"] for e in TRACE))
    schema_ok = 0
    for e in TRACE:
        try:
            r = e["response"].strip()
            if r.startswith("```"): r = "\n".join(r.split("\n")[1:-1])
            json.loads(r if r.startswith("{") else r[r.find("{"):r.rfind("}")+1])
            schema_ok += 1
        except (json.JSONDecodeError, ValueError):
            pass
    est_input_tokens = sum(len(e.get("user_message",""))//4 for e in TRACE)
    est_output_tokens = sum(len(e.get("response",""))//4 for e in TRACE)
    metrics = {
        "total_llm_calls": len(TRACE),
        "total_duration_ms": total_ms,
        "avg_call_ms": total_ms // max(len(TRACE), 1),
        "iterations": iterations,
        "clean_exit": clean_exit,
        "agents_used": agents_used,
        "schema_compliance": f"{schema_ok}/{len(TRACE)}",
        "est_input_tokens": est_input_tokens,
        "est_output_tokens": est_output_tokens,
    }
    output = {"metrics": metrics, "trace": TRACE}
    with open(path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nTrace written to {path} ({len(TRACE)} calls, {total_ms}ms total)")
    print(f"  Schema compliance: {schema_ok}/{len(TRACE)}, est tokens: ~{est_input_tokens}in/~{est_output_tokens}out")
    return metrics

# ═══════════════════════════════════════════════════════════
# LLM Call Infrastructure
# ═══════════════════════════════════════════════════════════

# Model override: set AGENT_ONTOLOGY_MODEL env var to override all agent models at runtime
_MODEL_OVERRIDE = os.environ.get("AGENT_ONTOLOGY_MODEL", "")
_OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")


def _openrouter_model_id(model):
    """Map spec model names to OpenRouter model IDs (provider/model format)."""
    if "/" in model:
        return model  # already in provider/model format
    if model.startswith("gemini"):
        return f"google/{model}"
    if model.startswith("claude") or model.startswith("anthropic"):
        return f"anthropic/{model}"
    if model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
        return f"openai/{model}"
    return model  # pass through as-is


def _call_openrouter(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=_OPENROUTER_API_KEY,
    )
    response = client.chat.completions.create(
        model=_openrouter_model_id(model),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def call_llm(model, system_prompt, user_message, temperature=0.7, max_tokens=4096, retries=3):
    if _MODEL_OVERRIDE:
        model = _MODEL_OVERRIDE
    for attempt in range(retries):
        try:
            if _OPENROUTER_API_KEY:
                return _call_openrouter(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("claude") or model.startswith("anthropic"):
                return _call_anthropic(model, system_prompt, user_message, temperature, max_tokens)
            elif model.startswith("gemini"):
                return _call_gemini(model, system_prompt, user_message, temperature, max_tokens)
            else:
                return _call_openai(model, system_prompt, user_message, temperature, max_tokens)
        except Exception as e:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"    [RETRY] {type(e).__name__}: {e} — retrying in {wait}s (attempt {attempt+1}/{retries})")
                import time as _time; _time.sleep(wait)
            else:
                print(f"    [FAIL] {type(e).__name__}: {e} after {retries} attempts")
                return json.dumps({"stub": True, "model": model, "error": str(e)})


def _call_openai(model, system_prompt, user_message, temperature, max_tokens):
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def _call_anthropic(model, system_prompt, user_message, temperature, max_tokens):
    import anthropic
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}],
    )
    return response.content[0].text


def _call_gemini(model, system_prompt, user_message, temperature, max_tokens):
    from google import genai
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY", ""))
    response = client.models.generate_content(
        model=model,
        contents=f"{system_prompt}\n\n{user_message}",
        config={"temperature": temperature, "max_output_tokens": max_tokens},
    )
    return response.text

# ═══════════════════════════════════════════════════════════
# Schema Registry
# ═══════════════════════════════════════════════════════════

SCHEMAS = {
    "TutoringInput": {
        "description": """""",
        "fields": [{"name": "topic", "type": "string"}, {"name": "learning_objectives", "type": "list<string>"}, {"name": "difficulty", "type": "enum[beginner, intermediate, advanced]"}],
    },
    "LessonPlanOutput": {
        "description": """""",
        "fields": [{"name": "concepts", "type": "list<Concept>"}, {"name": "total_concepts", "type": "integer"}],
    },
    "Concept": {
        "description": """""",
        "fields": [{"name": "concept_name", "type": "string"}, {"name": "prerequisites", "type": "list<string>"}, {"name": "suggested_question_types", "type": "list<string>"}],
    },
    "QuestionGeneratorInput": {
        "description": """""",
        "fields": [{"name": "concept", "type": "Concept"}, {"name": "history", "type": "list<Exchange>"}, {"name": "attempt_count", "type": "integer"}],
    },
    "TutorQuestionOutput": {
        "description": """""",
        "fields": [{"name": "question_text", "type": "string"}, {"name": "expected_key_points", "type": "list<string>"}, {"name": "difficulty_level", "type": "string"}, {"name": "hints", "type": "list<string>"}],
    },
    "StudentResponse": {
        "description": """""",
        "fields": [{"name": "answer_text", "type": "string"}],
    },
    "EvaluationInput": {
        "description": """""",
        "fields": [{"name": "student_answer", "type": "string"}, {"name": "key_points", "type": "list<string>"}, {"name": "concept", "type": "Concept"}],
    },
    "EvaluationOutput": {
        "description": """""",
        "fields": [{"name": "correctness_score", "type": "float"}, {"name": "misconceptions", "type": "list<string>"}, {"name": "mastered_concepts", "type": "list<string>"}, {"name": "feedback_text", "type": "string"}],
    },
    "RemediationOutput": {
        "description": """""",
        "fields": [{"name": "explanation", "type": "string"}, {"name": "prerequisite_review", "type": "string"}],
    },
    "SummaryInput": {
        "description": """""",
        "fields": [{"name": "mastered", "type": "list<string>"}, {"name": "review", "type": "list<string>"}, {"name": "transcript", "type": "list<Exchange>"}],
    },
    "SessionSummaryOutput": {
        "description": """""",
        "fields": [{"name": "concepts_mastered", "type": "list<string>"}, {"name": "concepts_needing_review", "type": "list<string>"}, {"name": "progress_percentage", "type": "float"}, {"name": "recommended_next_topics", "type": "list<string>"}, {"name": "full_transcript", "type": "list<Exchange>"}],
    },
    "Exchange": {
        "description": """""",
        "fields": [{"name": "role", "type": "string"}, {"name": "content", "type": "string"}],
    },
    "SessionState": {
        "description": """""",
        "fields": [{"name": "transcript", "type": "list<Exchange>"}, {"name": "current_concept_index", "type": "integer"}, {"name": "mastered_concepts", "type": "list<string>"}],
    },
}


def validate_output(data, schema_name):
    """Validate parsed LLM output against schema. Returns list of issues."""
    schema = SCHEMAS.get(schema_name)
    if not schema or "raw" in data:
        return []
    issues = []
    for field in schema["fields"]:
        fname = field["name"]
        ftype = field["type"]
        if fname not in data:
            issues.append(f"Missing field '{fname}' ({ftype})")
            continue
        val = data[fname]
        if ftype == "string" and not isinstance(val, str):
            issues.append(f"Field '{fname}' expected string, got {type(val).__name__}")
        elif ftype == "integer" and not isinstance(val, (int, float)):
            issues.append(f"Field '{fname}' expected integer, got {type(val).__name__}")
        elif ftype.startswith("list") and not isinstance(val, list):
            issues.append(f"Field '{fname}' expected list, got {type(val).__name__}")
        elif ftype.startswith("enum["):
            allowed = [v.strip() for v in ftype[5:-1].split(",")]
            if val not in allowed:
                issues.append(f"Field '{fname}' value '{val}' not in {allowed}")
    if issues:
        print(f"    [SCHEMA] {schema_name}: {len(issues)} issue(s): {issues[0]}")
    return issues


def build_input(state, schema_name):
    """Build an input dict for an agent call using schema field names.
    Pulls matching keys from state.data, checking both flat keys and
    values nested inside dict entries (e.g. state.data["xxx_input"]["field"])."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return state.data
    result = {}
    for field in schema["fields"]:
        fname = field["name"]
        if fname in state.data:
            result[fname] = state.data[fname]
        else:
            # Search nested dicts in state.data for the field
            for _k, _v in state.data.items():
                if isinstance(_v, dict) and fname in _v:
                    result[fname] = _v[fname]
                    break
    return result


def output_instruction(schema_name):
    """Generate a JSON output instruction string for the LLM."""
    schema = SCHEMAS.get(schema_name)
    if not schema:
        return ""
    fields = ", ".join(f'"{f["name"]}": <{f["type"]}>' for f in schema["fields"])
    return f"\n\nRespond with ONLY valid JSON matching this schema: {{{fields}}}"


def parse_response(response, schema_name):
    """Parse an LLM response according to an output schema.
    Returns a dict of field values, or {"raw": response} if parsing fails."""
    import re as _re
    text = response.strip()
    # Handle markdown code blocks
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]  # skip ```json
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    # Attempt 1: direct parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
        return {"value": parsed}
    except json.JSONDecodeError:
        pass
    # Attempt 2: extract JSON object from prose
    start = text.find("{")
    end = text.rfind("}") + 1
    if start >= 0 and end > start:
        candidate = text[start:end]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # Attempt 3: fix trailing commas
        fixed = _re.sub(r",\s*}", "}", candidate)
        fixed = _re.sub(r",\s*]", "]", fixed)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # Attempt 4: find matching braces (handle nested)
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{": depth += 1
            elif ch == "}": depth -= 1
            if depth == 0:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    break
    return {"raw": response}


# ═══════════════════════════════════════════════════════════
# Stores
# ═══════════════════════════════════════════════════════════

class Store_session_store:
    """Session Store (kv)"""
    def __init__(self):
        self.data = {}

    def read(self, key=None):
        return self.data if key is None else self.data.get(key)

    def write(self, value, key=None):
        if key is not None:
            self.data[key] = value
        else:
            self.data = value



# ═══════════════════════════════════════════════════════════
# Agent Wrappers
# ═══════════════════════════════════════════════════════════

def invoke_curriculum_planner(user_message, output_schema=None):
    """Curriculum Planner"""
    system = """You are an expert educational designer. Given a topic and learning objectives, create a structured lesson plan.
Break the topic into a sequence of concepts. For each concept, specify prerequisite concepts and suggested question types.
Output a LessonPlanOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Curriculum Planner", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_question_generator(user_message, output_schema=None):
    """Question Generator"""
    system = """You are a Socratic tutor. Your goal is to ask probing questions that lead the student to discover answers themselves.
Receive the current concept, student history, and attempt count.
Generate a question, expected key points, and a list of 3 increasingly specific hints.
Output a TutorQuestionOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Question Generator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_evaluator(user_message, output_schema=None):
    """Response Evaluator"""
    system = """Compare the student's response against the expected key points. 
Provide a correctness score (0.0 to 1.0), identify misconceptions, and provide feedback that encourages further thought without giving away the answer.
Output an EvaluationOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Response Evaluator", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_remediation_agent(user_message, output_schema=None):
    """Remediation Specialist"""
    system = """The student is struggling significantly with a concept. Provide a simpler explanation or a review of prerequisites to bridge the gap.
Do not solve the original question, but prepare them to try again.
Output a RemediationOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Remediation Specialist", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result


def invoke_summary_agent(user_message, output_schema=None):
    """Session Summarizer"""
    system = """Analyze the full tutoring transcript and progress. Summarize mastered concepts, areas for review, and overall progress.
Output a SessionSummaryOutput.
"""
    if output_schema:
        system += output_instruction(output_schema)
    t0 = time.time()
    result = call_llm(
        model="gemini-3-flash-preview",
        system_prompt=system,
        user_message=user_message,
        temperature=0.7,
        max_tokens=4096,
    )
    trace_call("Session Summarizer", "gemini-3-flash-preview", system, user_message, result, int((time.time()-t0)*1000))
    return result



# ═══════════════════════════════════════════════════════════
# Agent State
# ═══════════════════════════════════════════════════════════

class AgentState:
    """Runtime state for Socratic Tutoring Agent"""
    def __init__(self):
        self.session_store = Store_session_store()
        self.data = {}  # current data flowing through the pipeline
        self.iteration = 0
        self.schema_violations = 0


# ═══════════════════════════════════════════════════════════
# Process Functions
# ═══════════════════════════════════════════════════════════

def process_start_tutoring(state):
    """
    Initialize Session
    Accept tutoring parameters and initialize state
    """
    print(f"  → Initialize Session")

    # Logic from spec
    state.data["transcript"] = []
    state.data["current_concept_index"] = 0
    state.data["attempt_count"] = 0
    state.data["mastered_concepts"] = []
    state.data["needs_review"] = []
    if state.data.get("_done"):
        return state

    return state


def process_plan_curriculum(state):
    """
    Plan Curriculum
    Invoke planner to create the lesson sequence
    """
    print(f"  → Plan Curriculum")

    # Invoke: curriculum_planner
    curriculum_planner_input = build_input(state, "TutoringInput")
    curriculum_planner_msg = json.dumps(curriculum_planner_input, default=str)
    curriculum_planner_raw = invoke_curriculum_planner(curriculum_planner_msg, output_schema="LessonPlanOutput")
    curriculum_planner_result = parse_response(curriculum_planner_raw, "LessonPlanOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(curriculum_planner_result, "LessonPlanOutput"))
    state.data.update(curriculum_planner_result)
    state.data["lesson_plan_output"] = curriculum_planner_result
    state.data["plan_curriculum_result"] = curriculum_planner_result
    print(f"    ← Curriculum Planner: {curriculum_planner_result}")

    return state


def process_generate_question(state):
    """
    Generate Question
    Prepare context for the question generator
    """
    print(f"  → Generate Question")

    # Logic from spec
    # Retrieve current concept from the plan
    plan = state.data.get("lesson_plan_output", {})
    concepts = plan.get("concepts", [])
    idx = state.data.get("current_concept_index", 0)
    
    if idx < len(concepts):
        state.data["current_concept"] = concepts[idx]
    
    # Prepare input for agent
    state.data["gen_input"] = {
        "concept": state.data.get("current_concept"),
        "history": state.data.get("transcript", []),
        "attempt_count": state.data.get("attempt_count", 0)
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: question_generator
    question_generator_input = build_input(state, "QuestionGeneratorInput")
    question_generator_msg = json.dumps(question_generator_input, default=str)
    question_generator_raw = invoke_question_generator(question_generator_msg, output_schema="TutorQuestionOutput")
    question_generator_result = parse_response(question_generator_raw, "TutorQuestionOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(question_generator_result, "TutorQuestionOutput"))
    state.data.update(question_generator_result)
    state.data["tutor_question_output"] = question_generator_result
    state.data["generate_question_result"] = question_generator_result
    print(f"    ← Question Generator: {question_generator_result}")

    return state


def process_get_student_response(state):
    """
    Wait for Student
    """
    print(f"  → Wait for Student")

    response = input("Please answer the tutor's question. [Submit Answer/End Session]: ").strip().lower()
    state.data["checkpoint_response"] = response
    return state


def process_evaluate_response(state):
    """
    Evaluate Response
    Prepare student response and key points for evaluation
    """
    print(f"  → Evaluate Response")

    # Logic from spec
    # Capture response into transcript
    response_text = state.data.get("answer_text", "")
    state.data["transcript"].append({"role": "student", "content": response_text})
    
    # Prepare evaluation input
    state.data["eval_input"] = {
        "student_answer": response_text,
        "key_points": state.data.get("tutor_question_output", {}).get("expected_key_points", []),
        "concept": state.data.get("current_concept")
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: evaluator
    evaluator_input = build_input(state, "EvaluationInput")
    evaluator_msg = json.dumps(evaluator_input, default=str)
    evaluator_raw = invoke_evaluator(evaluator_msg, output_schema="EvaluationOutput")
    evaluator_result = parse_response(evaluator_raw, "EvaluationOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(evaluator_result, "EvaluationOutput"))
    state.data.update(evaluator_result)
    state.data["evaluation_output"] = evaluator_result
    state.data["evaluate_response_result"] = evaluator_result
    print(f"    ← Response Evaluator: {evaluator_result}")

    # Write: 
    session_store_write = build_input(state, "SessionState")
    state.session_store.write(session_store_write)

    return state


def process_check_progress(state):
    """
    Check Understanding
    """
    print(f"  → Check Understanding")

    # Gate: correctness_score
    # Branch: correctness_score > 0.8 → advance_concept
    # Branch: correctness_score >= 0.4 → provide_hint
    # Branch: correctness_score < 0.4 → remediate

    if (state.data.get("correctness_score", 0)) > 0.8:
        return "advance_concept"
    elif (state.data.get("correctness_score", 0)) >= 0.4:
        return "provide_hint"
    else:
        return "remediate"


def process_advance_concept(state):
    """
    Advance Concept
    Mark concept as mastered and move to next
    """
    print(f"  → Advance Concept")

    # Logic from spec
    concept_name = state.data.get("current_concept", {}).get("concept_name", "Unknown")
    state.data["mastered_concepts"].append(concept_name)
    state.data["current_concept_index"] += 1
    state.data["attempt_count"] = 0
    print(f"Concept {concept_name} mastered.")
    if state.data.get("_done"):
        return state

    return state


def process_provide_hint(state):
    """
    Provide Hint
    Select next hint and increment attempt counter
    """
    print(f"  → Provide Hint")

    # Logic from spec
    state.data["attempt_count"] += 1
    hints = state.data.get("tutor_question_output", {}).get("hints", [])
    # Select hint based on attempt count (clamped to list size)
    hint_idx = min(state.data["attempt_count"] - 1, len(hints) - 1)
    current_hint = hints[hint_idx] if hints else "Think about the core principle again."
    
    state.data["transcript"].append({"role": "tutor", "content": f"Hint: {current_hint}"})
    
    # Termination check for loop: if attempts > 3, force advance but mark for review
    if state.data["attempt_count"] >= 3:
        state.data["needs_review"].append(state.data.get("current_concept", {}).get("concept_name"))
        state.data["current_concept_index"] += 1
        state.data["attempt_count"] = 0
    if state.data.get("_done"):
        return state

    return state


def process_remediate(state):
    """
    Remediate
    Invoke remediation agent for struggling student
    """
    print(f"  → Remediate")

    # Logic from spec
    state.data["attempt_count"] += 1
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: remediation_agent
    remediation_agent_input = build_input(state, "EvaluationOutput")
    remediation_agent_msg = json.dumps(remediation_agent_input, default=str)
    remediation_agent_raw = invoke_remediation_agent(remediation_agent_msg, output_schema="RemediationOutput")
    remediation_agent_result = parse_response(remediation_agent_raw, "RemediationOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(remediation_agent_result, "RemediationOutput"))
    state.data.update(remediation_agent_result)
    state.data["remediation_output"] = remediation_agent_result
    state.data["remediate_result"] = remediation_agent_result
    print(f"    ← Remediation Specialist: {remediation_agent_result}")

    return state


def process_check_completion(state):
    """
    All Concepts Covered?
    """
    print(f"  → All Concepts Covered?")

    # Gate: current_concept_index >= total_concepts
    # Branch: done → summarize_session
    # Branch: more → generate_question

    if (state.data.get("current_concept_index", 0)) >= (state.data.get("total_concepts", 0)):
        print(f"    → done")
        return "summarize_session"
    else:
        print(f"    → more")
        return "generate_question"


def process_summarize_session(state):
    """
    Summarize Session
    Generate final report
    """
    print(f"  → Summarize Session")

    # Logic from spec
    state.data["summary_input"] = {
        "mastered": state.data.get("mastered_concepts", []),
        "review": state.data.get("needs_review", []),
        "transcript": state.data.get("transcript", [])
    }
    if state.data.get("_done"):
        return state

    # Flatten nested dicts: promote schema fields to top-level state.data
    for _nested_val in list(state.data.values()):
        if isinstance(_nested_val, dict):
            for _nk, _nv in _nested_val.items():
                if _nk not in state.data:
                    state.data[_nk] = _nv

    # Invoke: summary_agent
    summary_agent_input = build_input(state, "SummaryInput")
    summary_agent_msg = json.dumps(summary_agent_input, default=str)
    summary_agent_raw = invoke_summary_agent(summary_agent_msg, output_schema="SessionSummaryOutput")
    summary_agent_result = parse_response(summary_agent_raw, "SessionSummaryOutput")
    # Validate and merge output fields into state.data
    state.schema_violations += len(validate_output(summary_agent_result, "SessionSummaryOutput"))
    state.data.update(summary_agent_result)
    state.data["session_summary_output"] = summary_agent_result
    state.data["summarize_session_result"] = summary_agent_result
    print(f"    ← Session Summarizer: {summary_agent_result}")

    return state


def process_end_session(state):
    """
    End Session
    """
    print(f"  → End Session")

    # Logic from spec
    state.data["_done"] = True
    if state.data.get("_done"):
        return state

    return state


# ═══════════════════════════════════════════════════════════
# State Machine Executor
# ═══════════════════════════════════════════════════════════

PROCESSES = {
    "start_tutoring": process_start_tutoring,
    "plan_curriculum": process_plan_curriculum,
    "generate_question": process_generate_question,
    "get_student_response": process_get_student_response,
    "evaluate_response": process_evaluate_response,
    "check_progress": process_check_progress,
    "advance_concept": process_advance_concept,
    "provide_hint": process_provide_hint,
    "remediate": process_remediate,
    "check_completion": process_check_completion,
    "summarize_session": process_summarize_session,
    "end_session": process_end_session,
}

TRANSITIONS = {
    "start_tutoring": "plan_curriculum",
    "plan_curriculum": "generate_question",
    "generate_question": "get_student_response",
    "get_student_response": "evaluate_response",
    "evaluate_response": "check_progress",
    # "check_progress": determined by gate logic
    "advance_concept": "check_completion",
    "provide_hint": "generate_question",
    "remediate": "generate_question",
    # "check_completion": determined by gate logic
    "summarize_session": "end_session",
    "end_session": None,  # terminal
}


MAX_ITERATIONS = int(os.environ.get("AGENT_ONTOLOGY_MAX_ITER", "100"))


def run(initial_data=None):
    """Socratic Tutoring Agent — main execution loop"""
    state = AgentState()
    if initial_data:
        state.data.update(initial_data)

    current = "start_tutoring"
    print(f"\n════════════════════════════════════════════════════════════")
    print(f"  Socratic Tutoring Agent")
    print(f"  A tutoring system that guides students through topics using probing questions, adaptive hints, and remediation based on understanding levels.")
    print(f"════════════════════════════════════════════════════════════\n")

    while current and state.iteration < MAX_ITERATIONS:
        state.iteration += 1
        print(f"\n[Iteration {state.iteration}] State: {current}")

        process_fn = PROCESSES.get(current)
        if not process_fn:
            print(f"  Unknown process: {current}")
            break

        result = process_fn(state)

        if current in ['check_progress', 'check_completion']:
            current = result
        else:
            current = TRANSITIONS.get(current)

        # Fan-out: if transition is a list, run all branches sequentially
        while isinstance(current, list):
            _targets = current
            for _ft in _targets:
                state.iteration += 1
                print(f"\n[Iteration {state.iteration}] State: {_ft} (fan-out)")
                _fn = PROCESSES.get(_ft)
                if _fn:
                    _fn(state)
            # Collect unique next-hops from all fan-out targets
            _next_set = []
            for _ft in _targets:
                _nt = TRANSITIONS.get(_ft)
                if _nt is not None and _nt not in _next_set:
                    _next_set.append(_nt)
            current = _next_set[0] if len(_next_set) == 1 else (_next_set if _next_set else None)

        if current is None or state.data.get("_done"):
            print("\n  [DONE] Reached terminal state.")
            break

    _clean_exit = state.iteration < MAX_ITERATIONS
    if state.iteration >= MAX_ITERATIONS:
        print(f"\n  [STOPPED] Max iterations ({MAX_ITERATIONS}) reached.")
        _clean_exit = False

    if state.schema_violations > 0:
        print(f"\n  [SCHEMA] {state.schema_violations} total schema violation(s) during execution")
    dump_trace(iterations=state.iteration, clean_exit=_clean_exit)
    print(f"\nFinal state.data keys: {list(state.data.keys())}")
    return state


if __name__ == "__main__":
    run()